<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Ivan Hanigan</title>
 <link href="http://ivanhanigan.github.com/atom.xml" rel="self"/>
 <link href="http://ivanhanigan.github.com/"/>
 <updated>2016-03-04T23:07:44+11:00</updated>
 <id>http://ivanhanigan.github.com/</id>
 <author>
   <name>Ivan Hanigan</name>
   <email>ivan.hanigan@gmail.com</email>
 </author>

 
 <entry>
   <title>R base graphics are fine except barplot</title>
   <link href="http://ivanhanigan.github.com/2016/02/r-base-graphics-are-fine-except-barplot/"/>
   <updated>2016-02-19T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/02/r-base-graphics-are-fine-except-barplot</id>
   <content type="html">&lt;p&gt;I concur with Jeff Leek that once spent time learning base graphics in R there is less incentive to learn ggplot2 &lt;a href=&quot;http://simplystatistics.org/2016/02/11/why-i-dont-use-ggplot2/&quot;&gt;http://simplystatistics.org/2016/02/11/why-i-dont-use-ggplot2/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However I always hate the way &lt;code&gt;barplot&lt;/code&gt; works.  Here is an example:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;qc &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;textConnection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;id,  OnlinePaper, Q, freq, totals,       prop&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;1,      Online,         ,1768,   9950, 0.17768844&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;2,      Online,      No ,4022,   9950, 0.40422111&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;3,      Online,     Yes ,4160,   9950, 0.41809045&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;4,       Paper,         , 256,   3355, 0.07630402&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;5,       Paper,      No , 979,   3355, 0.29180328&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;6,       Paper,     Yes ,2120,   3355, 0.63189270&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

qc1 &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; cast&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;qc&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; OnlinePaper &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; Q74 &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; value &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;prop&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
qc1
barplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;as.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;qc1&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; beside &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; legend.text &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; qc1&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; ylim &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src=&quot;/images/barplot_base.png&quot; alt=&quot;/images/barplot_base.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ggplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;qc&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; aes&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Q&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;prop&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; fill&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;OnlinePaper&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
    geom_bar&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;stat&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; position&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;position_dodge&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src=&quot;/images/barplot_gg.png&quot; alt=&quot;/images/barplot_gg.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Going to extremes&lt;/h4&gt;

&lt;p&gt;I should say though that I have found barplot can produce very customised graphs that serve a specific purpose such as that below (I have de-identified the content as this is unpublished research)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/barplot-gonuts.png&quot; alt=&quot;/images/barplot-gonuts.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This made heavy use of the following approach&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# original by Joseph Guillaume 2009&lt;/span&gt;
SideBySideBarPlot2 &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aggAllData&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  par&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mar&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  bp&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;barplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aggAllData&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              horiz&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;gray.colors&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aggAllData&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
              las&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; axisnames &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  labels &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;aggAllData&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  text&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;bp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; par&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;usr&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; labels &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; srt &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
       adj &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; xpd &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; cex&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;bp&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# with width = xvar (proportions)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>r-syntax-highlights-for-my-jekyll-powered-blog.md</title>
   <link href="http://ivanhanigan.github.com/2016/02/r-syntax-highlights-for-my-jekyll-powered-blog/"/>
   <updated>2016-02-14T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/02/r-syntax-highlights-for-my-jekyll-powered-blog</id>
   <content type="html">&lt;h2&gt;Syntax Highlights&lt;/h2&gt;

&lt;p&gt;Until today I had no idea how to make code pretty in my blog posts which go to github after being first rendered locally so I can get the categories and tags.&lt;/p&gt;

&lt;p&gt;Because github disables any plugins when it processes your blog I took Charlie Park's advice. &lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;http://charliepark.org/jekyll-with-plugins/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This blog post solved it for me &lt;a href=&quot;http://tuxette.nathalievilla.org/?p=1574&quot;&gt;http://tuxette.nathalievilla.org/?p=1574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The trick is to write &lt;code&gt;highlighter:      pygments&lt;/code&gt; into the &lt;code&gt;_config.yml&lt;/code&gt; and then:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% highlight r % # with curly braces
data(&quot;iris&quot;)
plot(iris$Sepal.Length ~ iris$Sepal.Width)
dat &amp;lt;- rnorm(1000,1,2)
% endhighlight % # with curly braces
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Will render as:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;data&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;iris&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;iris&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Sepal.Length &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; iris&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;Sepal.Width&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
dat &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; rnorm&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;But I also pushed this to another site that I do use gh-pages to build and it sent me an email complaining:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;You are attempting to use the 'pygments' highlighter, 
which is currently unsupported on GitHub Pages. 
Your site will use 'rouge' for highlighting instead. 
To suppress this warning, change the 'highlighter' value to 
'rouge' in your '_config.yml'. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;So there.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Climate grids thredds European data</title>
   <link href="http://ivanhanigan.github.com/2016/02/climate-grids-thredds-european-data/"/>
   <updated>2016-02-02T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/02/climate-grids-thredds-european-data</id>
   <content type="html">&lt;p&gt;I've got netCDF data regarding temperature calculated from the daily E-OBS gridded dataset which is based on observational data with a spatial resolution of 0.22° on a
rotated pole grid, with the north pole at 39.25N, 162W.  &lt;a href=&quot;http://www.ecad.eu&quot;&gt;http://www.ecad.eu&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The rotated grid is the same as used in many ENSEMBLES Regional Climate Models.  But I've never worked with the rotated grid projection before and so here is what I learnt.&lt;/p&gt;

&lt;h2&gt;This is the actual geographic points, it shows a kind of fan of locations&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_actual_xy.png&quot; alt=&quot;/images/qc_actual_xy.png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;{r}&quot;&gt;#library(ncdf4)
library(ncdf)
library(raster)
projdir &amp;lt;- &quot;~/projects/weather_european_eobs/eobs_temperature_tg_dataset&quot;
outdir &amp;lt;- &quot;data_provided&quot;
outdir &amp;lt;- file.path(projdir, outdir)
#dir.create(outdir, recursive = T)
setwd(projdir)
dir()
# The URL below will get you the data, but the ECAD group do request you register your email address 
# they probably use this information to report some usage stats to their funders, so 
# Please do consider going to this webpage (http://www.ecad.eu/download/ensembles/ensembles.php)
# and register yourself.  Thanks!
webroot &amp;lt;- &quot;http://www.ecad.eu/download/ensembles/data/Grid_0.22deg_rot&quot;
# Create a list of netcdf files to download, we can loop over it
file_list &amp;lt;- c(&quot;tg_0.22deg_rot_1950-1964_v12.0.nc.gz&quot;,&quot;tg_0.22deg_rot_1965-1979_v12.0.nc.gz&quot;)

# only download if not already done
setwd(outdir)
for(i in 1:length(file_list))
    {
    dl &amp;lt;- file.path(webroot, file_list[i])
    dl
    infile &amp;lt;- basename(dl)
    exists  &amp;lt;- dir(pattern= infile)
    if(
      !exists(&quot;exists&quot;)
       )
    {
    download.file(dl, destfile = infile, mode = &quot;wb&quot;)
    system(sprintf(&quot;gunzip %s&quot;, infile))
    }
    &quot; (250MB)&quot;
    }
setwd(projdir)

# now we can go through all days, I set this up as a loop over ncdf files then days, 
# but I'll probably try to set this up to directly use the netCDF aggregation functions available and avoid looping 
# for(j in 1:length(file_list))
  #{
    j = 1
    infile &amp;lt;- file.path(outdir, gsub(&quot;.gz&quot;, &quot;&quot;, file_list[j]))
    nc &amp;lt;- open.ncdf(infile)
    #str(nc)
    print(nc)
    #?get.var.ncdf, following the example in the help file
    print(paste(&quot;The file has&quot;,nc$nvars,&quot;variables&quot;))
    var_i      &amp;lt;- nc$var[[4]]
    #var_i
    varsize &amp;lt;- var_i$varsize
    ndims   &amp;lt;- var_i$ndims
    nt      &amp;lt;- varsize[ndims]  # Remember timelike dim is always the LAST dimension!
    #nt 
    # we will set up to do a loop over days, I want to
    # a) read in the day grid
    # b) make a spatial points file with the appropriate projection
    # c) convert to geotiff and save to disk
    #for(i in 1:nt)
    #  {
        i = 1
       # Initialize start and count to read one timestep of the variable.
       start &amp;lt;- rep(1,ndims)   # begin with start=(1,1,1,...,1)
       start[ndims] &amp;lt;- i       # change to start=(1,1,1,...,i) to read timestep i
       count &amp;lt;- varsize        # begin w/count=(nx,ny,nz,...,nt), reads entire var
       count[ndims] &amp;lt;- 1       # change to count=(nx,ny,nz,...,1) to read 1 tstep
       data3 &amp;lt;- get.var.ncdf( nc, var_i, start=start, count=count )

       # Now read in the value of the timelike dimension
       timeval &amp;lt;- get.var.ncdf( nc, var_i$dim[[ndims]]$name, start=i, count=1 )

       print(paste(&quot;Data for variable&quot;,var_i$name,&quot;at timestep&quot;,i,
               &quot; (time value=&quot;,timeval,var_i$dim[[ndims]]$units,&quot;):&quot;))
       # [1] &quot;Data for variable tg at timestep 1  (time value= 0 days since 1950-01-01 00:00 ):&quot;      
       #print(data3)
       #image(data3)
       dat1 &amp;lt;- list()
       dat1$x &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_longitude&quot;)
       #dat1$x &amp;lt;- dat1$x[,1]       
       dat1$y &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_latitude&quot;)
       #dat1$y &amp;lt;- dat1$y[1,]              
       dat1$z &amp;lt;- get.var.ncdf(nc, var_i, start=start, count=count )
       str(dat1$z)
       #image(dat1$z)
       #map(&quot;world&quot;, xlim = c(xmin, xmax), ylim = c(ymin, ymax))
       #with(dat1, points(x, y, cex = .1, pch = 16))
       dat2 &amp;lt;- data.frame(
         x = as.vector(dat1$x),
         y = as.vector(dat1$y),
         z = as.vector(dat1$z)
         )
       #str(dat2)  
       #getwd()
       # I had the idea to save each day out to a file for looping over again and extracting spatially located data
       # say for a pixel, but I decided to try out the netCDF aggregation tools at a next step
       # save(dat2, file = sprintf(&quot;data_derived/eobs_tg_%s.RData&quot;,i))           
       #load(sprintf(&quot;eobs_tg_%s.RData&quot;,i))
       #This seemed like a good idea, but RData is more compressed
       # write.csv(dat2, sprintf(&quot;eobs_tg.csv&quot;,i), row.names = F, na = &quot;&quot;)       
       #}
#}

# load the data for a specific day as example
dir(&quot;data_derived&quot;)
infile  &amp;lt;- &quot;eobs_tg_1.RData&quot;
load(file.path(&quot;data_derived/&quot;, infile))
ls()
# Now this is able to be mapped, but have make sure of the projection
library(maptools)
library(scales) 
library(RColorBrewer)
library(rgdal)
# the following code was adapted from

# Bedia, J. (2012). R practice using data from the ENSEMBLES
# Project. Retrieved from
# http://www.value-cost.eu/sites/default/files/VALUE_TS1_D02_RIntro.pdf

data(wrld_simpl)
# loads the world map dataset 
wrl  &amp;lt;- as(wrld_simpl,&quot;SpatialLines&quot;) 
l1 &amp;lt;- list(&quot;sp.lines&quot;,wrl)
#x &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_longitude&quot;)
str(x)
x &amp;lt;- as.vector(x)
#y &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_latitude&quot;)
str(y)
y  &amp;lt;- as.vector(y)


coords &amp;lt;- cbind(x, y)
str(coords)
head(coords)
# so the actual lat lons are not regular grid in degrees
png(&quot;figures_and_tables/qc_actual_xy.png&quot;)
plot(coords, asp=1, cex=.4, col=&quot;grey&quot;, 
  pch=&quot;+&quot;, main=(&quot;actual lon-lat grid&quot;))
lines(wrl)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;This is the first graphic shown above, it shows a kind of fan of locations&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;{r}&quot;&gt;# so what does the rotated grid look like in its own universe?
str(dat1$z)
png(&quot;figures_and_tables/qc_rotated.png&quot;)
image(dat1$z)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;This is how it looks on rotated grid&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_rotated.png&quot; alt=&quot;images/qc_rotated.png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;{r}&quot;&gt;# so now add in the temperatures, use the wgs latlongs
t &amp;lt;- as.vector(dat1$z)
dat3 &amp;lt;- cbind.data.frame(coords, t)
summary(dat3)
coordinates(dat3) &amp;lt;- c(1,2)
#names(dat3@data)  &amp;lt;- c(&quot;z&quot;)
str(dat3)
summary(dat3@data)
color.palette &amp;lt;- rev(brewer.pal(11,&quot;Spectral&quot;))
getwd()
dir()
png(&quot;figures_and_tables/qc_tempmap_wgs.png&quot;)
spplot(dat3, scales=list(draw=TRUE), sp.layout=list(l1), 
       col.regions=alpha(color.palette,.2), cuts=10, 
       main=&quot;tg&quot;)
dev.off()

# write out the data to a shapefile
# we first have to make sure that the proj4 string is ok
str(dat3)
proj4string(dat3) &amp;lt;- &quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0&quot;
setwd(file.path(projdir, &quot;data_derived&quot;))
writeOGR(dat3, &quot;out2.shp&quot;, &quot;out2&quot;, &quot;ESRI Shapefile&quot;)
setwd(projdir)

# to look at the rotated project do the following
# need to look at the website to get this info
# http://opendap.knmi.nl/knmi/thredds/dodsC/e-obs_0.22rotated/tg_0.22deg_rot_v12.0.nc.html
print(nc)
# it doesn't seem like there is this info in the ncdf file?
get.var.ncdf(nc, &quot;projection&quot;)
rcm.lonlat.grid &amp;lt;- SpatialPoints(coords)
# now set this to wgs84
proj4string(rcm.lonlat.grid) &amp;lt;-&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0&quot;

# this is the proj from website
# +proj=ob_tran +o_proj=longlat +lon_0=18 +o_lat_p=39.25 +a=6367470 +e=0
rcm.lambert.proj4 &amp;lt;- CRS(&quot;+proj=ob_tran +o_proj=longlat +lon_0=18 +o_lat_p=39.25 +a=6367470 +e=0&quot;)
# do the transform
spTransform(rcm.lonlat.grid, rcm.lambert.proj4) -&amp;gt; rcm.lambert.grid
summary(rcm.lambert.grid)

world.trans &amp;lt;- spTransform(wrl, rcm.lambert.proj4)

png(&quot;figures_and_tables/qc_rotated_grid_and_countries.png&quot;)
plot(rcm.lambert.grid@coords, cex=.2, pch=3, asp=1, col=&quot;grey&quot;, 
  main=&quot;Projected RCM Grid - Lambert Conical Conformal&quot;)
lines(world.trans, col=&quot;red&quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;This shows the rotated world countries&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_rotated_grid_and_countries.png&quot; alt=&quot;images/qc_rotated_grid_and_countries.png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;{r&quot;&gt;pr.df &amp;lt;- cbind.data.frame(coordinates(rcm.lambert.grid), dat3@data)
coordinates(pr.df) &amp;lt;- c(1,2) 
l1  &amp;lt;- list(&quot;sp.lines&quot;, world.trans)
color.palette &amp;lt;- colorRampPalette(c(&quot;yellow&quot;,&quot;cyan&quot;, &quot;blue&quot;,&quot;purple&quot;))

# This graph shows this with temp
png(&quot;figures_and_tables/qc_rotated_grid_and_countries2.png&quot;)
spplot(pr.df, sp.layout=list(l1), cuts=7, cex=1.5, 
  col.regions=alpha(color.palette(7),.15), pch=rep(15,7),
  main=&quot;Temp&quot;)
dev.off()
# not much point writing this to shapefile?
#proj4string(pr.df)  &amp;lt;- rcm.lambert.proj4
#str(pr.df)
#setwd(file.path(projdir, &quot;data_derived&quot;))
#dir()
# writeOGR(pr.df, &quot;out.shp&quot;, &quot;out&quot;, &quot;ESRI Shapefile&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;this one has temperatures too&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_rotated_grid_and_countries2.png&quot; alt=&quot;images/qc_rotated_grid_and_countries2.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;acknowledgement and citations.&lt;/h2&gt;

&lt;p&gt;E-OBS temperature and precipitation:&lt;/p&gt;

&lt;p&gt;We acknowledge the E-OBS dataset from the EU-FP6 project ENSEMBLES
(http://ensembles-eu.metoffice.com) and the data providers in the
ECA&amp;amp;D project (http://www.ecad.eu)&lt;/p&gt;

&lt;p&gt;Haylock, M.R., N. Hofstra, A.M.G. Klein Tank, E.J. Klok, P.D. Jones,
M. New. 2008: A European daily high-resolution gridded dataset of
surface temperature and precipitation. J. Geophys. Res (Atmospheres),
113, D20119, doi:10.1029/2008JD10201&quot;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The perfect is the enemy of the good</title>
   <link href="http://ivanhanigan.github.com/2016/01/the-perfect-is-the-enemy-of-the-good/"/>
   <updated>2016-01-31T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/the-perfect-is-the-enemy-of-the-good</id>
   <content type="html">&lt;p&gt;According to Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good&quot;&gt;https://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good&lt;/a&gt;
this phrase was popularised by Voltaire, and Shakespeare via King Lear: &quot;striving to better, oft we mar what's well&quot;&lt;/p&gt;

&lt;p&gt;This phrase is a favourite of several people I respect and admire.  On the other hand it has always vaguely troubled me.
I think there may be two dimensions of this phrase that are worth pondering.&lt;/p&gt;

&lt;h2&gt;One: &quot;the good&quot; is a quality of the work&lt;/h2&gt;

&lt;p&gt;The first dimension is better expressed in King Lear, and on the wiki page where it is suggested the meaning is &quot;that we might never complete a task if we have decided not to stop until it is perfect.&quot;
This is correct, however not an absolutely faithful interpretation of the phrase &quot;striving to better&quot;.  To my mind this does not necessarily lead to the position &quot;will not stop until perfect&quot;.
If one strives for perfection but admits that this is beyond the scope of one's ambition then one will eventually stop when &quot;it&quot; is &quot;good enough&quot;, and that state will be better if one were striving for perfection than if one where aiming for &quot;satisfactory&quot;.&lt;/p&gt;

&lt;h2&gt;Two: &quot;the good&quot; is the impact the work may have on the world around us&lt;/h2&gt;

&lt;p&gt;The second dimension resonates more clearly with me and that is that &quot;the good&quot; is a public good, an impact that we feel would make the world a better place.
To aim for a satisfactory action or intervention on the world around is would then be the goal, and we should try to achieve this as soon as possible, without delaying our work with minutia or trivia that might be that extra 1 percent that we would hope elevates our work further than satisfactory.  One extra rung up the ladder toward perfection.&lt;/p&gt;

&lt;h2&gt;Stirzaker's &quot;simplicity cycle&quot;&lt;/h2&gt;

&lt;p&gt;On page 175 of Stirzaker, R. (2010). Out of the scientists garden: A story of water and food. Canberra, Australia: CSIRO.
I copied the image and show it below, but I have flipped it around so the axis are reversed from his original.
This shows a couple of trajectories we can head along in terms of striving for enhancements, and the effect this has on how &quot;good&quot; the results are.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/stirzacker.jpg&quot; alt=&quot;/images/stirzacker.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Seriousness aside&lt;/h2&gt;

&lt;p&gt;I used this image in my last project as a rallying call to the team to focus on both striving for the perfection while also balancing our aims to impact on the good.
My colleagues pointed out that the flipped over version is easily turned into a stickman, a la XKCD!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/stirzacker2.jpg&quot; alt=&quot;/images/stirzacker2.jpg&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>UPDATE to post 'GIS Issues when R is Used for Transforming Coordinate Systems'</title>
   <link href="http://ivanhanigan.github.com/2016/01/gis-issues-when-r-used-transforming-coordinate-systems/"/>
   <updated>2016-01-29T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/gis-issues-when-r-used-transforming-coordinate-systems</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;UPDATE to post &lt;a href=&quot;http://ivanhanigan.github.com//2015/10/gis-issues-when-r-used-transforming-coordinate-systems&quot;&gt;http://ivanhanigan.github.com//2015/10/gis-issues-when-r-used-transforming-coordinate-systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recall that RGDAL transforms and write out spatial data in GDA94 with a problem in that ArcGIS does not like the prj file and complains&lt;/li&gt;
&lt;li&gt;This seems an ok workaround&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;infile &amp;lt;- &quot;western_sydney_passive_samplers_site_details.csv&quot;
indir &amp;lt;- &quot;data_derived&quot;

outdir &amp;lt;- indir
outfile &amp;lt;- file.path(outdir, gsub(&quot;.csv&quot;,&quot;.shp&quot;,infile))

dat &amp;lt;- read.csv(file.path(indir, infile), as.is=T)
str(dat)

dat2 &amp;lt;- SpatialPointsDataFrame(data.frame(x=dat$long, y=dat$lat), dat,
                                proj4string=CRS(epsg$prj4[epsg$code %in% '4283'])
                                )

writeOGR(dat2, outfile,
         outdir, driver='ESRI Shapefile'
         )
# fix the prj file 
download.file(&quot;http://spatialreference.org/ref/epsg/4283/prj/&quot;,
              gsub(&quot;.shp&quot;, &quot;.prj&quot;, outfile), mode = &quot;wb&quot;
              )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/samplers_sydney.png&quot; alt=&quot;/images/samplers_sydney.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Release checklist references</title>
   <link href="http://ivanhanigan.github.com/2016/01/release-checklist-references/"/>
   <updated>2016-01-28T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/release-checklist-references</id>
   <content type="html">&lt;h2&gt;Some good reading and best-practice advice&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Driessen, V. (2010). A successful Git branching model. Nvie.Com. Retrieved January 28, 2016, from &lt;a href=&quot;http://nvie.com/posts/a-successful-git-branching-model/&quot;&gt;http://nvie.com/posts/a-successful-git-branching-model/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;object data=&quot;/images/nvie2010-2.pdf&quot; type=&quot;application/pdf&quot; width=&quot;675&quot; height=&quot;800&quot;&gt;
  &lt;p&gt;Alternative text - include a link &lt;a href=&quot;images/nvie2010-2.pdf.pdf&quot;&gt;to the PDF!&lt;/a&gt;&lt;/p&gt;
&lt;/object&gt;


&lt;ul&gt;
&lt;li&gt;Huff, K. D. (n.d.). A Software Release Checklist. Retrieved January 28, 2016, from &lt;a href=&quot;http://bids.berkeley.edu/news/software-release-checklist&quot;&gt;http://bids.berkeley.edu/news/software-release-checklist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/Huff2016.png&quot; alt=&quot;/images/Huff2016.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stanisic, L., Legrand, A., &amp;amp; Danjean, V. (2015). An Effective Git And Org-Mode Based Workflow For Reproducible Research. ACM SIGOPS Operating Systems  …. Retrieved from &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2723881&quot;&gt;http://dl.acm.org/citation.cfm?id=2723881&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/Stanisic2015.png&quot; alt=&quot;/images/Stanisic2015.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Repeatability vs replication - Definitional variations</title>
   <link href="http://ivanhanigan.github.com/2016/01/repeatability-vs-replication-definitional-variations/"/>
   <updated>2016-01-25T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/repeatability-vs-replication-definitional-variations</id>
   <content type="html">&lt;p&gt;In my previous post I bemoaned the variability in definitions of reproducibility and replication.&lt;br/&gt;
The definition of reproducibility in particular concerns me as I finalise my thesis on 'Reproducible Research Pipelines'.  However I also have noticed a confusion of Repeatability and Replication that is worth noting.  In various author's definitions.  In CASSEY, P., &amp;amp; BLACKBURN, T. M. (2006). Reproducibility and Repeatability in Ecology. BioScience, 56(12), 958. &lt;a href=&quot;http://bioscience.oxfordjournals.org/content/56/12/958.full&quot;&gt;http://bioscience.oxfordjournals.org/content/56/12/958.full&lt;/a&gt; they agree with the definition of Peng 2011 that Reproducibility is to re-calculate the same result from the same data and they use Repeatability as a synonym for Peng's Replication.&lt;/p&gt;

&lt;p&gt;According to Cassey and Blackburn (2006), reproducibility is the case that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from the information presented in the study, a third party could
replicate (sic) the reported results identically.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This definition distinguishes reproducibility from repeatability which is when&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a third party must be able to perform a study using identical methodological protocols 
and analyze the resulting data in an identical manner
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Which is what Peng terms 'replicability'.  This leads me to conclude that:&lt;/p&gt;

&lt;h3&gt;Cassey and Blackburn conflate Repeatability with Replicability.&lt;/h3&gt;

&lt;p&gt;However another author gives a very confused and overlapping view Ellison, A. (2010). Repeatability and Transparency in Ecological Research. Ecology.  &lt;a href=&quot;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&quot;&gt;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&lt;/a&gt; Accessed 12 Jan 2016.&lt;/p&gt;

&lt;p&gt;To add further confusion I have dug up the latest dictionary of epidemiology and find that whilst this agrees with Reproducibility and Replication, it treates Repeatability as a synonym of Reproducibility!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Porta, Miquel S. Dictionary of Epidemiology (6th Edition). New York, NY, USA: Oxford University Press, USA, 2014. ProQuest ebrary. Web. 24 January 2016.
Copyright © 2014. Oxford University Press, USA. All rights reserved.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;
Repeatability (Syn: reproducibility): The value below which the
absolute difference between two single test results may be expected to
lie with a probability of 95%, when the results are obtained by the
same method and equipment from identical test material in the same
setting by the same operator within short intervals of time. A test or
measurement is repeatable if the results are identical or closely
similar each time it is conducted. 1-3,5-9,91 See also measurement,
terminology of; reliability.

Replication: The execution of an observational or experimental study
more than once so as to confirm the findings, increase precision, and
obtain a closer estimation of sampling error. Exact replication should
be distinguished from consistency of results on replication. Exact
replication is often possible in the physical sciences, but in the
health, life, and social sciences consistency of results on
replication is often the best that can be
attained. 1,2,6,25,39,42,91,206-208,270,273,533 Consistency of results
on replication is perhaps the most important consideration in
judgements of CAUSALITY.

Reproducibility: see REPEATABILITY.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Whatever happens, never cite Drummond 2009!&lt;/h2&gt;

&lt;p&gt;It is unclear whether Drummond's self-published conference paper was peer reviewed (or reviewed by the conference committee) but the following quote is unsupported assertion and should be ignored.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reproducibility requires changes; replicability avoids them.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Drummond, C. (2009). Replicability is not Reproducibility: Nor is it Good Science. Proceedings of the Evaluation Methods for Machine Learning Workshop 26th International Conference for Machine Learning. Retrieved January 25, 2016, from &lt;a href=&quot;http://cogprints.org/7691/7/icmlws09.pdf&quot;&gt;http://cogprints.org/7691/7/icmlws09.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Indeed in 2012 Drummond (in another self-published, working paper without evidence of peer review) did a backflip and said:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reproducibility requires that the experiment originally carried out be duplicated 
as far as is reasonably possible. The aim is to minimize the difference from 
the first experiment including its flaws, to produce
independent verification of the result as reported
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Drummond, C. (2012). Reproducible research: A dissenting opinion. Unpublished draft. Retrieved October 9, 2015, from &lt;a href=&quot;http://cogprints.org/8675/&quot;&gt;http://cogprints.org/8675/&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reproducibility vs replication - Definitional variations</title>
   <link href="http://ivanhanigan.github.com/2016/01/reproducibility-vs-replication-definitional-variations/"/>
   <updated>2016-01-22T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/reproducibility-vs-replication-definitional-variations</id>
   <content type="html">&lt;p&gt;There is confusion between the definitions of reproducibility, repeatability and replicability.
I strongly feel we need to tackle that head on and come to an agreed definition.
I prefer Peng 2011:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reproduciblity is using the same data and getting the exact same result.&lt;/li&gt;
&lt;li&gt;Replication is getting a new sample and doing the analysis again and getting a similar result.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;Peng, R. D. (2011). Reproducible research in computational
science. Science, 334(6060), 1226–1227. doi:10.1126/science.1213847
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;There are many people using these interchangeably or around the opposite way. For example Drummond got it round the wrong way in 'Drummond, C., 2009. Replicability is not reproducibility: nor is it good science' &lt;a href=&quot;http://cogprints.org/7691/7/icmlws09.pdf&quot;&gt;http://cogprints.org/7691/7/icmlws09.pdf&lt;/a&gt;
and then reverted it in 'Reproducible Research: a Dissenting Opinion'. &lt;a href=&quot;http://cogprints.org/8675/1/ReproducibleResearch.pdf&quot;&gt;http://cogprints.org/8675/1/ReproducibleResearch.pdf&lt;/a&gt;
(Check out Peng's reaction: &lt;a href=&quot;http://simplystatistics.org/2012/11/15/reproducible-research-with-us-or-against-us-3/&quot;&gt;http://simplystatistics.org/2012/11/15/reproducible-research-with-us-or-against-us-3/&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;And this blog but also gets the definitions around the wrong way &lt;a href=&quot;http://jermdemo.blogspot.com.au/2012/12/the-reproducible-research-guilt-trip.html&quot;&gt;http://jermdemo.blogspot.com.au/2012/12/the-reproducible-research-guilt-trip.html&lt;/a&gt; (even tho being quite entertaining to read and had this great picture.... not sure what the picture means???)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/thinker2.jpg&quot; alt=&quot;/images/thinker2.jpg&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Sure, OK, it is fine that people define things differently to one another but:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The single biggest problem in communication 
is the illusion that it has taken place.
George Bernard Shaw quotes from BrainyQuote.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;We rely on a common defintion to ensure we are talking about the same thing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/communication-bnewell.png&quot; alt=&quot;/images/communication-bnewell.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Source: Newell, B. (2012). Simple models, powerful ideas: Towards effective integrative practice. Global Environmental Change, 22(3), 776–783. &lt;a href=&quot;http://dx.doi.org/10.1016/j.gloenvcha.2012.03.006&quot;&gt;http://dx.doi.org/10.1016/j.gloenvcha.2012.03.006&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is regrettable that in Ecology (my favourite discipline) there seems to be quite a wide gap between various author's definitions.  In CASSEY, P., &amp;amp; BLACKBURN, T. M. (2006). Reproducibility and
Repeatability in Ecology. BioScience, 56(12), 958. &lt;a href=&quot;http://bioscience.oxfordjournals.org/content/56/12/958.full&quot;&gt;http://bioscience.oxfordjournals.org/content/56/12/958.full&lt;/a&gt; they agree with the definition of Peng 2011. However another author gives a very confused and overlapping view:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
because that context changes through time and space, it is virtually
impossible to reproduce precisely or quantitatively any single
experimental or observational field study in ecology. Yet many
ecological studies can be repeated. In particular, ecological
synthesis – the assembly of derived datasets and their subsequent
analysis, re-analysis, and meta-analysis – should be easy to repeat
and reproduce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Ellison, A. (2010). Repeatability and Transparency in Ecological Research.
Ecology.  &lt;a href=&quot;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&quot;&gt;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&lt;/a&gt; Accessed 12 Jan 16&lt;/p&gt;

&lt;p&gt;In another interesting approach Freedman, L. P., Cockburn, I. M., &amp;amp; Simcoe, T. S. (2015). The Economics of Reproducibility in Preclinical Research. PLOS Biology, 13(6), e1002165. &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pbio.1002165&quot;&gt;http://dx.doi.org/10.1371/journal.pbio.1002165&lt;/a&gt; chose instead to define &lt;em&gt;irreproducibility&lt;/em&gt; such that it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;that encompasses the existence and propagation of one or more errors,
flaws, inadequacies, or omissions (collectively referred to as errors) 
that prevent replication of results
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Leaving us to assume that the opposite of this is therefore &lt;em&gt;reproducibility&lt;/em&gt;, although avoiding defining this themselves.  Looking back at the two heads in the picture above... it is interesting to ponder how some people would receive the signal of Freedman et al, having defined the opposite of the thing that is the object of their discussion, rather than the thing itself!&lt;/p&gt;

&lt;p&gt;Let's all agree with Peng and Cassey/Blackburn and move on already!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Validity of measurement</title>
   <link href="http://ivanhanigan.github.com/2016/01/validity-of-measurement/"/>
   <updated>2016-01-18T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/validity-of-measurement</id>
   <content type="html">&lt;p&gt;I have needed to describe validity recently and found it useful to paraphrase some of this statistics blog post: &lt;a href=&quot;&quot;&gt;http://andrewgelman.com/2015/04/28/whats-important-thing-statistics-thats-not-textbooks/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I've been working a lot on air pollution modelling recently, where 'validation' is used to assess how well the modelled pollution predicted values represent the actual pollution observed. I tend to think of validity as formalised in statistical terms, i.e. as correlations between different measurements of the same thing, or between measurement and 'truth', and statistics are used for assessing and calibrating measurements.&lt;/p&gt;

&lt;p&gt;I am guessing that when applied to the validity behind a research proposal, the issue might be whether the measurement is suitable for addressing the issue the researcher (and research question) is interested in, and therefore supporting the researchers to make valid inferences from the outcome of statistical methods. I have heard lots of anecdotes from statisticians in which when they are asked to help with analysing data, their advice was essentially that in order for a valid analysis that addresses the research question one would rather need to have collected different measurements.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>We have a statistically rigorous and scientifically meaningful definition of replication. Let's use it</title>
   <link href="http://ivanhanigan.github.com/2016/01/we-have-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication/"/>
   <updated>2016-01-12T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/we-have-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication</id>
   <content type="html">&lt;p&gt;Researchers writing about the 'Reproducibility Crisis' often conflate
the terms reproducibility, repeatability and replicability, but it is
quite important to distinguish these.  There is a great discussion of
the distinction in the SimplyStatistics blog post titled: &lt;a href=&quot;http://simplystatistics.org/2015/10/20/we-need-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication&quot;&gt;We need a statistically rigorous and
scientifically meaningful definition of
replication&lt;/a&gt;.
But I actually now think &lt;em&gt;we DO have that definition!&lt;/em&gt; It is the
that repeatability is the same
as replication and involves a new sample with new measurement errors
while reproducibility uses the same data to recalculate the result.&lt;/p&gt;

&lt;p&gt;I think it is vital that we labour the point so that the
distinction between repeatability and reproducibility is made clear.&lt;/p&gt;

&lt;p&gt;I follow the definition that 'reproducible' is exact re-computation
whereas repeatable/replicable is a new analysis, of a new sample,
yielding a new result (plus or minus some variance from measurement
error), and if the original study is replicated then the same
conclusions are reached from the new analysis.&lt;/p&gt;

&lt;p&gt;I used this paragraph and recent reference in my thesis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reproducibility is defined as ‘the ability to recompute data analytic
results given an observed dataset and knowledge of the data analysis
pipeline’ (Leek &amp;amp; Peng 2015). This definition distinguishes
reproducibility from replicability which is ‘the chance that an
independent experiment targeting the same scientific question will
produce a consistent result’ (Leek &amp;amp; Peng 2015).  

Leek, J.T. &amp;amp; Peng, R.D. (2015). Opinion: Reproducible research can
still be wrong: Adopting a prevention approach. Proceedings of the
National Academy of Sciences of the United States of America, 112(6),
1645–1646.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;But I am also a big fan of the the definitions in Peng 2011 and Cassey 2006.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Peng 2011:&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;With replication, independent investigators address a scientific
hypothesis and build up evidence for or against it...

Reproducibility calls for the data and computer code used to analyze
the data be made available to others. This standard falls short of
full replication because the same data are analysed again, rather than
analysing independently collected data.

Peng, R. D. (2011). Reproducible research in computational
science. Science, 334(6060), 1226–1227. doi:10.1126/science.1213847
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Cassey 2006:&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;[For a repeatable study] a third party must be able to perform a study
using identical methodological proto- cols and analyze the resulting
data in an identical manner... [Further] a published result
must be presented in a manner that allows for a quantitative
comparison in a later study...

We consider a study reproducible if, from the information presented in
the study, a third party could replicate the re- ported results
identically. 

CASSEY, P., &amp;amp; BLACKBURN, T. M. (2006). Reproducibility and
Repeatability in Ecology. BioScience,
56(12), 958. doi:10.1641/0006-3568
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;To fully endorse any scientific claims, the experimental findings should be completely repeatable by many independent investigators who ‘address areplicated hypothesis and build up evidence for or against it’ (Peng, 2011). It is important to note that the exact results need not be computed in a repeatable study. This is because experimentation involves probability, and if performed again, with a different sample and new set of measurement errors some variance between experiments is to be expected.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Exemplars of distributing data and code - rOpenSci</title>
   <link href="http://ivanhanigan.github.com/2016/01/exemplars-of-distributing-data-and-code-ropensci/"/>
   <updated>2016-01-03T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2016/01/exemplars-of-distributing-data-and-code-ropensci</id>
   <content type="html">&lt;p&gt;The practice of distributing data and code has a wide variety of possible approaches.  There are many resources available to be used to post data and code to the internet for dissemination, and it is very easy to access these resources.  It is more difficult to find exemplars of how data and code are easily and effectively distributed.  I am conducting a review of some of the resources that describe procedures for this and present exemplars in this and following notes.&lt;/p&gt;

&lt;p&gt;A paper that describes the rOpenSci project's approach is &lt;a href=&quot;http://dx.doi.org/10.5334/jors.bu&quot;&gt;http://dx.doi.org/10.5334/jors.bu&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;Boettiger, C., Chamberlain, S., Hart, E., &amp;amp; Ram,
K. (2015). Building Software, Building Community: Lessons from the
rOpenSci Project. Journal of Open Research Software,
3(1). 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This paper is focused on the way the community development and capacity building part of the project was conducted.  The diagram shown below is introduced as an example of the style that rOpenSci recommend a data analysis workflow be constructed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/workflow-ropensci.png&quot; alt=&quot;/images/workflow-ropensci.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The focus on publishing data to a public repository so early in the project (prior to final analysis and manuscript) seems premature to me. But then, I do feel that I am somewhat more concerned with vexatious activity by climate skeptics than the rOpenSci team.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>My framework of scientific workflow and integration software for holistic data analysis</title>
   <link href="http://ivanhanigan.github.com/2015/12/my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis/"/>
   <updated>2015-12-22T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/12/my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis</id>
   <content type="html">&lt;p&gt;Scientific workflow and integration software for holistic data analysis (SWISH) is a
title I have given to describe the area of my research that focuses on the tools and techniques
of reproducible data analysis.&lt;/p&gt;

&lt;p&gt;Reproducibility is the ability to recompute the results of a data
analysis with the original data.  It is possible to have analyses that
are reproducible with varying degrees of difficulty. A data
analysis might be reproducible but require thousands of hours of work to
piece together the datasets, transformations, manipulations, calculations and interpretations of computational results.
A primary challenge to reproducible data analysis is to make analyses
that are &lt;em&gt;easy&lt;/em&gt; to reproduce.&lt;/p&gt;

&lt;p&gt;To achieve this, a guiding principle is that analysts should
effectively implement 'pipelines' of method steps and tools.  Data
analysts should employ standardised and evidence-based methods based
on conventions developed from many data analysts approaching the
problems in a similar way, rather than each analyst configuring
pipelines to suit particular individual or domain-specific
preferences.&lt;/p&gt;

&lt;h2&gt;Planning and implementing a pipeline&lt;/h2&gt;

&lt;p&gt;It can be much easier to conceptualise a complicated data analysis
method than to implement this as a reproducible research pipeline. The
most effective way to implement a pipeline is by methodically tracking
each of the steps taken, the data inputs needed and all the outputs of
the step.  If done in a disciplined way then the analyst or some other
person could 'audit' the procedure easily and access the details of
the pipeline they need to scrutinise.&lt;/p&gt;

&lt;h3&gt;Toward a standardised data analysis pipeline framework&lt;/h3&gt;

&lt;p&gt;In my own work I have tried a diverse variety of configurations based on
things I have read and discussions I have had.  Coming to the end of
my PhD project I have reflected on the framework that I have arrived at and
present this below as a schematic overview.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*   /home/
**    /overview.org 
           - summary data_inventory
           - DMP
**    /worklog.org    
           - YYYY-MM-DD
*   /projects/
**    /project1_data_analysis_project_health_research
***       /dataset1_merged_health_outcomes_and_exposures
             - index.org
             - git (local private, gitignore all subfolders)
             - workplan
             - worklog
             - workflow
             - main.Rmd
****         /data1_provided
****         /data2_derived
*****            - workflow script
****         /code
****         /results/  (this has all the pathways explored)
*****           - README.md
                 - git (public Github)
                 /YYYY-MM-DD-shortname (i.e. EDA, prelim, model-selection, sensitivity)
                     /main.Rmd
                     /code/
                     /data/
****         /report/
                   /manuscript.Rmd
                     - main results recomputed in production/publication quality
                     - supporting_information (but also can refer to github/results)
                 /figures_and_tables/
                     - png
                     - csv
*****           /journal_submission/
                     - cover letter
                     - approval signatures
                     - submitted manuscript
*****           /journal_revision/
                     - response.org
**    /project2_data_analysis_project_exposure_assessment
           - index.org
           - git
***       /dataset2.1_monitored_data
              - workplan
              - worklog
              - workflow
****         /data1_provided
****         /data2_derived 
                 - stored here or
                 - web2py crud or
                 - geoserver
              /data1_and_data2_backups
              /reports/
                 - manuscript.Rmd -&amp;gt; publish with the data somehow
              /tools (R package)
                 - git/master -&amp;gt; Github
****      /dataset2.2_GIS_layers 
**    /methods_or_literature_review_project
*  /tools/
         /web2py
             /applications
                 /data_inventory
                     - holdings
                     - prospective
                 /database_crud
          /disentangle (R package)
          /pipeline_templates
**   /data/
         /postgis_hanigan
         /postgis_anu_gislibrary
         /geoserver_anu_gislibrary
**   /references/
         - mendeley
         - bib
         - PDFs annotated
**   /KeplerData/workflows/MyWorkflows/
***      /data_analysis_workflow_using_kepler (implemented as an R package)
****         /inst/doc/A01_load.R
***      /data_analysis_workflow_using_kepler (implemented as an R LCFD workflow)
             - main.Rmd (raw R version)
             - main.xml (this is kepler)
****         /data/
                 - file1.csv
                 - file2.csv
****         /code/
                 - load.R
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>This is an open notebook but selected content delayed</title>
   <link href="http://ivanhanigan.github.com/2015/12/this-is-an-open-notebook-but-selected-content-delayed/"/>
   <updated>2015-12-20T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/12/this-is-an-open-notebook-but-selected-content-delayed</id>
   <content type="html">&lt;h4&gt;Open Notebook Science, Selected Content, Delayed (ONS-SCD)&lt;/h4&gt;

&lt;p&gt;I am trying to juggle my work in a dual Open-And-Closed way.&lt;/p&gt;

&lt;p&gt;To explain: I try to keep an electronic ‘Open Notebook’ that aligns
with the principles of the Open Notebook Science (ONS) movement’s
‘Selected Content – Delayed’ category (ONS-SCD).  Back in 2012 when I started my notebook I looked
around for models of what style of publication I wanted.  I knew that some of my work was owned
by the university I work at, and I am not allowed to publish this openly.
Then there is other stuff I owned as part of my PhD, but that I might
not want to release all the details of my work.  So I settled on a 'Selected Content - Delayed' category and got the logo shown here from the (now-defunct) website &lt;a href=&quot;http://onsclaims.wikispaces.com/&quot;&gt;http://onsclaims.wikispaces.com/&lt;/a&gt;.  The ONS movement
is still described on Wikipedia though &lt;a href=&quot;https://en.wikipedia.org/wiki/Open_notebook_science&quot;&gt;https://en.wikipedia.org/wiki/Open_notebook_science&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ONS-SCD.png&quot; alt=&quot;/images/ONS-SCD.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this publication model I
make publicly available the content of my research notebook (like a
blog), in which I write reports of the details of the data, code and
documents related to my research. I selectively make material open on
github, and I sometimes delay publication of the material that I keep
in my private research notebook.  That work is kept private either
because it includes unpublished work that I wish to keep embargoed
until after publication, or because it is all the gory details of
process of writing code to create or analyse data that is not
appropriate for open publication.&lt;/p&gt;

&lt;p&gt;In previous work I have either paid for additional private repos on
github, and made the repo open once the paper is published, or
alternately used bitbucket with unlimited free private repos for
university students and then just put together a public repo for
sharing 'polished' outputs.&lt;/p&gt;

&lt;p&gt;The upshot is that I use this part open / part closed approach during the data exploration, cleaning, analysis and writing. In my opinion as long as the final workflow is clearly and openly documented and reproducible, that's  the most important thing.&lt;/p&gt;

&lt;h4&gt;The motivation stems back to the Climategate scandal and infamous 'Harry Readme' file&lt;/h4&gt;

&lt;p&gt;My supervisors over the years have all been really supportive of working in an open way and I have flirted with the idea of being completely open.  However, I got a little worried about the implications of working too openly when malicious people might dig though my work for vexatious reasons, such as looking for errors or embarrassing comments I might inadvertently make that, when taken out of context, might make me sound foolish.&lt;/p&gt;

&lt;p&gt;This sounds far fetched, but as an example of this, a few years ago there was a fair amount of heat generated by a lot of
emails and other documents from the University of East Anglia Climate
Research University.  I was particularly interested because I was
struggling to make sense of a lot of weird and wonderful databases and
I felt a lot of sympathy for 'Harry', someone who as far as I could
tell was doing a pretty good job of exploring, cleaning and
documenting their work.&lt;/p&gt;

&lt;p&gt;Here is one journalists summary of this issue &lt;a href=&quot;http://blogs.telegraph.co.uk/technology/iandouglas/100004334/harry_read_me-txt-the-climategate-gun-that-does-not-smoke/&quot;&gt;http://blogs.telegraph.co.uk/technology/iandouglas/100004334/harry_read_me-txt-the-climategate-gun-that-does-not-smoke/&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;
the contents of the harry_read_me.txt file, apparently leaked from the
University of East Anglia and now becoming a totem for climate change
sceptics to gather around as though it were a piece of the true cross.

This file – thousands of lines of annotations kept on the process of
re-developing a computer model of the climate form figures submitted
by weather stations around the world and other historical data sets –
holds a personal commentary written by an un-named developer (let's
call him Harry), frustrated and often tied up in knots, working late
into the night and the weekend trying to squeeze differently-formatted
numbers into a consistent narrative.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Using git and Github in an ONS-SCD model&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Recall Noble's framework?  The results folder is what I want to publish&lt;/li&gt;
&lt;li&gt;Noble recommended the following folder and file structures &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1000424.g001&quot;&gt;http://dx.doi.org/10.1371/journal.pcbi.1000424.g001&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I revised his conceptual diagram, and I blogged about this at &lt;a href=&quot;/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/&quot;&gt;/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;/projectname (eg msms)/
    /doc/
        /ms-analysis.html 
        /paper/
            /msms.tex
            /msms.pdf
    /data/
        /YYYY-MM-DD/
            /yeast/
                /README
                /yeast.sqt
            /worm/
                /README
                /worm.sqt
    /src/
        /ms-analysis.c
    /bin/
        /parse-sqt.py
    /results/
        /notebook.html 
        /YYYY-MM-DD-1/
            /runall
            /split1/
            /split2/
        /YYYY-MM-DD-2/
            /runall
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;I want to publish my results, rather than my process&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I had the realisation that &lt;a href=&quot;/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/&quot;&gt;/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;
    the 'Experiment Results' level is about work you might do on a 
       single day, or over a week

    Workflow scripts: At this level each 'experiment' is written up in
    chronological order, as entries to the Worklog at the meso level

    Noble recommends 'create either a README file, in which I store
    every command line that I used while performing the experi- ment,
    or a driver script (I usually call this runall) that carries out
    the entire experiment automatically'...

    and 'you should end up with a file that is parallel to the lab
    notebook entry. The lab notebook contains a prose description of
    the exper- iment, whereas the driver script contains all the gory
    details.'

    this is the level I usually think of managing the distribution
    side of things. I will want to pack up the results and email to my
    collaborators, or decide on the one set of tables and figures to
    write into the manuscript for submission to a journal. If this is
    accepted for publication, this is the one combined package of
    'analytical data and code' that I would consider putting up online
    (to github) as supporting information for the paper.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Public Github repo within a private local &lt;code&gt;overview&lt;/code&gt; git repo: My setup&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I mostly use one single Emacs orgmode file to run the whole project, using tangle to send chunks of code to scripts, after testing them out using the library of babel&lt;/li&gt;
&lt;li&gt;To keep this version controlled I created a git repo for this&lt;/li&gt;
&lt;li&gt;To test out  have created a fake-data-analysis-project and this includes a local git repository&lt;/li&gt;
&lt;li&gt;in the &lt;code&gt;.gitignore&lt;/code&gt; file I added the commend &lt;code&gt;*&lt;/code&gt; to ignore all subfolders and files&lt;/li&gt;
&lt;li&gt;If I want to add files to this I need to use &lt;code&gt;git add -f thefile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Then I create a public github repo in the results folder (I named the repo: &lt;code&gt;THE-PROJECT-NAME-results&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;
$ cd ~/projects/fake-data-analysis-project
$ mkdir results
$ cd results/
/results$ git init
Initialized empty Git repository in /home/ivan_hanigan/tools/ReproducibleResearchPipelineTemplate/results/.git/
/results$ mkdir 2015-12-20-eda
/results$ git remote add origin git@github.com:ivanhanigan/ReproducibleResearchPipelineTemplate-results.git
$ git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;The Result&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;An example of these results are now published at &lt;a href=&quot;https://github.com/ivanhanigan/ReproducibleResearchPipelineTemplate-results&quot;&gt;https://github.com/ivanhanigan/ReproducibleResearchPipelineTemplate-results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;But the rest of my work is privately held, and version controlled.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Using scholar rankings to provide weights in systematic literature reviews part 1</title>
   <link href="http://ivanhanigan.github.com/2015/12/using-scholar-rankings-to-provide-weights-in-systematic-literature-reviews-part-1/"/>
   <updated>2015-12-17T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/12/using-scholar-rankings-to-provide-weights-in-systematic-literature-reviews-part-1</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;I've been thinking alot recently about an approach used in this recent systematic review&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;Vins, H., Bell, J., Saha, S., &amp;amp; Hess, J. (2015). The mental health
outcomes of drought: A systematic review and causal process
diagram. International Journal of Environmental Research and Public
Health, 12(10), 13251–13275. doi:10.3390/ijerph121013251
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;They identify causal pathways from papers and ascribe the supporting evidentiary weight based on the number of papers published with findings that support this cause-effect pathway&lt;/li&gt;
&lt;li&gt;The raw number of papers is probably not a good metric, prone to bias so I was thinking of ways to ascribe weight based on quality of journal or authors&lt;/li&gt;
&lt;li&gt;This is not supposed to replace the need to actually read the papers, but purely as an additional source of information&lt;/li&gt;
&lt;li&gt;This recent post on scholar metrics provided some impetus via h-indices
&lt;a href=&quot;http://datascienceplus.com/hindex-gindex-pubmed-rismed/&quot;&gt;http://datascienceplus.com/hindex-gindex-pubmed-rismed/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I also think this approach of text mining the abstracts could be useful &lt;a href=&quot;http://tuxette.nathalievilla.org/?p=1682&quot;&gt;http://tuxette.nathalievilla.org/?p=1682&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Sanity check of the two options using myself as guinea pig&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;library(RISmed)
x &amp;lt;- &quot;hanigan+ic[Author]&quot;
res &amp;lt;- EUtilsSummary(x, type=&quot;esearch&quot;, db=&quot;pubmed&quot;, datetype='pdat', mindate=1900, 
  maxdate=2015, retmax=500)
str(res)
citations1 &amp;lt;- Cited(res)
citations &amp;lt;- as.data.frame(citations1)
citations &amp;lt;- citations[order(citations$citations,decreasing=TRUE),]
citations &amp;lt;- as.data.frame(citations)
str(citations)
citations &amp;lt;- cbind(id=rownames(citations),citations)
citations$id&amp;lt;- as.character(citations$id)
citations$id&amp;lt;- as.numeric(citations$id)
hindex &amp;lt;- max(which(citations$id&amp;lt;=citations$citations))

hindex
# 5

library(scholar)
myid &amp;lt;- &quot;cGN1P0wAAAAJ&quot;
y &amp;lt;- scholar::get_publications(myid)
str(y)
y[,c(&quot;author&quot;, &quot;cites&quot;)]
y$id &amp;lt;- as.numeric(row.names(y))
hindex2 &amp;lt;- max(which(y$id&amp;lt;=y$cites))
hindex2
# 15
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Clearly the pubmed and google scholar search engines makes a big difference to my score!&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>My project inventory</title>
   <link href="http://ivanhanigan.github.com/2015/12/my-project-inventory/"/>
   <updated>2015-12-07T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/12/my-project-inventory</id>
   <content type="html">&lt;h3&gt;Auditing and inventorising&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I just completed an audit of my project files and updated the list on my website &lt;a href=&quot;/projects.html&quot;&gt;http://ivanhanigan.github.com/projects.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This was enabled by the work I have been doing on a data inventory web2py app &lt;a href=&quot;https://github.com/ivanhanigan/data_inventory&quot;&gt;https://github.com/ivanhanigan/data_inventory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The list shows some of the project data collections I have amassed during my research over the last 15 years&lt;/li&gt;
&lt;li&gt;Some of these are data collections I have developed, others are derivatives of collections originated by others&lt;/li&gt;
&lt;li&gt;Many of these are areas of active research, but others are dormant&lt;/li&gt;
&lt;li&gt;This list will be updated as time permits.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Projects&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;1 Air pollution&lt;/li&gt;
&lt;li&gt;2 Australian health&lt;/li&gt;
&lt;li&gt;3 Australian population&lt;/li&gt;
&lt;li&gt;4 Biodiversity and environmental change&lt;/li&gt;
&lt;li&gt;5 Bioregions&lt;/li&gt;
&lt;li&gt;6 Cardio-respiratory disease, biomass smoke, dust and heatwaves&lt;/li&gt;
&lt;li&gt;7 Climate Change&lt;/li&gt;
&lt;li&gt;8 Eco-social observatories&lt;/li&gt;
&lt;li&gt;9 Extreme Weather Events&lt;/li&gt;
&lt;li&gt;10 GIS&lt;/li&gt;
&lt;li&gt;11 Infectious diseases and local habitat&lt;/li&gt;
&lt;li&gt;12 Mental health and drought&lt;/li&gt;
&lt;li&gt;13 Mortality and morbidity effects from weather&lt;/li&gt;
&lt;li&gt;14 Reproducible research pipelines&lt;/li&gt;
&lt;li&gt;15 Medical geography theory and tools&lt;/li&gt;
&lt;li&gt;16 Roads and places&lt;/li&gt;
&lt;li&gt;17 Transformational adaptation&lt;/li&gt;
&lt;li&gt;18 Ultraviolet radiation&lt;/li&gt;
&lt;li&gt;19 Water&lt;/li&gt;
&lt;li&gt;20 Weather&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger</title>
   <link href="http://ivanhanigan.github.com/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger/"/>
   <updated>2015-12-02T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;This is a revision of my post &lt;a href=&quot;/2015/10/show-missingness-in-large-dataframes-v2&quot;&gt;/2015/10/show-missingness-in-large-dataframes-v2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This guy posted &lt;a href=&quot;http://www.njtierney.com/r/missing%20data/rbloggers/2015/12/01/ggplot-missing-data/&quot;&gt;http://www.njtierney.com/r/missing%20data/rbloggers/2015/12/01/ggplot-missing-data/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Let's try it out!&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
# depends
install.packages(&quot;gbm&quot;)
install_github(&quot;tierneyn/neato&quot;)
library(neato)
# small eg
locs=c(&quot;Australia&quot;,&quot;India&quot;,&quot;New Zealand&quot;,&quot;Sri Lanka&quot;,&quot;Uruguay&quot;,&quot;Somalia&quot;)
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable[atable == FALSE] &amp;lt;- NA
atable
png(&quot;ggplotmissing.png&quot;)
ggplot_missing(atable)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/ggplotmissing.png&quot; alt=&quot;/images/ggplotmissing.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The one I had problems with because too large is:&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;# Cool but what about a big one?
dat &amp;lt;- read.csv(&quot;~/path/to/file.csv&quot;)
str(dat)
png(&quot;ggplotmissing2.png&quot;, height=1800, width = 3000, res = 200)
ggplot_missing(dat)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/ggplotmissing2.png&quot; alt=&quot;/images/ggplotmissing2.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes from Dr Climate Re data reference syntax models for file organisation and naming</title>
   <link href="http://ivanhanigan.github.com/2015/11/notes-from-dr-climate-re-data-reference-syntax-models-for-file-organisation-and-naming/"/>
   <updated>2015-11-29T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/11/notes-from-dr-climate-re-data-reference-syntax-models-for-file-organisation-and-naming</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;This is an excellent explanation of the Australian Integrated Marine Observing System (IMOS) Data Reference Syntax by Damien Irving on the Dr Climate blog  &lt;a href=&quot;https://drclimate.wordpress.com/2015/09/04/managing-your-data/&quot;&gt;https://drclimate.wordpress.com/2015/09/04/managing-your-data/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Data Reference Syntax (DRS) – a convention for naming your files&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;&amp;lt;computer&amp;gt;/&amp;lt;project&amp;gt;/&amp;lt;organisation&amp;gt;/&amp;lt;collection&amp;gt;/&amp;lt;facility&amp;gt;/&amp;lt;data-type&amp;gt;/&amp;lt;site-code&amp;gt;/&amp;lt;year&amp;gt;/

The data type has a sub-DRS of its own, which tells us that the data
represents the 1-hourly average surface current for a single month
(October 2012), and that it is archived on a regularly spaced spatial
grid and has not been quality controlled.

Just in case the file gets separated from this informative directory
structure, much of the information is repeated in the file name
itself, along with some more detailed information about the start and
end time of the data, and the last time the file was modified:

&amp;lt;project&amp;gt;_&amp;lt;facility&amp;gt;_V_&amp;lt;time-start&amp;gt;_&amp;lt;site-code&amp;gt;_FV00_&amp;lt;data-type&amp;gt;_&amp;lt;time-end&amp;gt;_&amp;lt;modified&amp;gt;.nc.gz

In the first instance this level of detail seems like a bit of
overkill... 

Since the data are so well labelled,
locating all monthly timescale ACORN data from the Turquoise Coast and
Rottnest Shelf sites (which represents hundreds of files) would be as
simple as typing the following at the command line:

$ ls */ACORN/monthly_*/{TURQ,ROT}/*/*.nc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Damien's personalised DRS&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;It is worthwhile thinking through these ideas and incorporating them in ones data management system as early as possible&lt;/li&gt;
&lt;li&gt;Damien has also helpfully openly shared his own DRS at &lt;a href=&quot;https://github.com/DamienIrving/climate-analysis/blob/master/data_reference_syntax.md&quot;&gt;https://github.com/DamienIrving/climate-analysis/blob/master/data_reference_syntax.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Here is a summary of some key items I'm going to implement versions of for my own work&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basic data files&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;var&amp;gt;_&amp;lt;dataset&amp;gt;_&amp;lt;level&amp;gt;_&amp;lt;time&amp;gt;_&amp;lt;spatial&amp;gt;.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Sub-categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;time&amp;gt;&lt;/code&gt;: &lt;code&gt;&amp;lt;tstep&amp;gt;-&amp;lt;aggregation&amp;gt;-&amp;lt;season&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;spatial&amp;gt;&lt;/code&gt;: &lt;code&gt;&amp;lt;grid&amp;gt;-&amp;lt;region&amp;gt;-&amp;lt;bounds&amp;gt;-&amp;lt;np&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;tstep&amp;gt;&lt;/code&gt;: &lt;code&gt;daily&lt;/code&gt;, &lt;code&gt;monthly&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;aggregation&amp;gt;&lt;/code&gt;: &lt;code&gt;030day-runmean&lt;/code&gt;, &lt;code&gt;anom-wrt-1979-2011&lt;/code&gt;, &lt;code&gt;anom-wrt-all&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;season&amp;gt;&lt;/code&gt;: &lt;code&gt;JJA&lt;/code&gt;, &lt;code&gt;MJJASO&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;grid&amp;gt;&lt;/code&gt;: &lt;code&gt;native&lt;/code&gt; or something like &lt;code&gt;y181x360&lt;/code&gt;, which describes the number of latitude (181) and longitude (360) points (in this case it is a 1 by 1 degree horizontal grid).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;region&amp;gt;&lt;/code&gt;: Region names are defined in &lt;code&gt;netcdf_io.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;bounds&amp;gt;&lt;/code&gt;: e.g. &lt;code&gt;lon225E335E-lat10S10N&lt;/code&gt; or &lt;code&gt;mermax&lt;/code&gt;, &lt;code&gt;zonal-anom&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;np&amp;gt;&lt;/code&gt;: North pole location, e.g. &lt;code&gt;np20N260E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Examples include:&lt;br/&gt;
&lt;code&gt;psl_Merra_surface_daily_y181x360.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;More complex file names&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;inside&amp;gt;_&amp;lt;filters&amp;gt;_&amp;lt;prev-var&amp;gt;_&amp;lt;dataset&amp;gt;_&amp;lt;level&amp;gt;_&amp;lt;time&amp;gt;_&amp;lt;spatial&amp;gt;.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Sub-categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;inside&amp;gt;&lt;/code&gt;: The variable inside the file. e.g. &lt;code&gt;tas-composite&lt;/code&gt;, &lt;code&gt;datelist&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;filters&amp;gt;&lt;/code&gt;: e.g. &lt;code&gt;samgt90pct&lt;/code&gt; (&lt;code&gt;gt&lt;/code&gt; and &lt;code&gt;lt&lt;/code&gt; and used for greater and less than, &lt;code&gt;pct&lt;/code&gt; for percentile)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;prev-var&amp;gt;&lt;/code&gt;: if it's not obvious what variable &lt;code&gt;&amp;lt;inside&amp;gt;&lt;/code&gt; was created from, include the previous variable/s&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Examples:&lt;br/&gt;
&lt;code&gt;tas-composite_pwigt90pct_ERAInterim_500hPa_030day-runmean-anom-wrt-all_native-sh.png&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Principles of Tidy Data&lt;/h3&gt;

&lt;p&gt;In the words of Hadley Wickham the order that data should be
arranged in follows some generic principles:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;'A good ordering makes it easier to scan the raw values. One way of
organizing variables is by their role in the analysis: are values
fixed by the design of the data collection, or are they measured
during the course of the experiment? Fixed variables describe the
experimental design and are known in advance. Computer scientists
often call fixed variables dimensions, and statisticians usually
denote them with subscripts on random variables. Measured variables
are what we actually measure in the study. Fixed variables should come
first, followed by measured variables, each ordered so that related
variables are contiguous. Rows can then be ordered by the first
variable, breaking ties with the second and subsequent (fixed)
variables.'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;An exemplar&lt;/h3&gt;

&lt;p&gt;In my last project the protocol we developed (for an ecology and biodiversity database) had a naming convention which relied heavily on a sequence of information being used to order the names of folders, subfolders and files.  This is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The project name (and optional sub-project name)&lt;/li&gt;
&lt;li&gt;Data type (such as experimental unit, observational unit, and/or measurement methods)&lt;/li&gt;
&lt;li&gt;Geographic location (locality name, State, Country)&lt;/li&gt;
&lt;li&gt;Temporal frequency and coverage (such as annual or seasonal tranches).&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;The concepts of slow moving dimensions and fast moving variables&lt;/h3&gt;

&lt;p&gt;The concept of dimensions and variables can be useful here, and especially for deciding on filenames.  Dimensions are fixed or change slowly while variables change more quickly.  By 'change', this  means that there are more of them. For example the project name is 'fixed', that is it does not change across the files, but the sub-project name does change, just more slowly (say there may be 2-3 different sub-projects within a project). Then there may be a set of data types, and these 'change' more quickly than the sub-project name.  Then the geographic and temporal variables might change quickest of all.&lt;/p&gt;

&lt;p&gt;So a general rule for the order of things can be stated. The fixed and slowly changing variables should come first (those things that don't change, or don't change much),
followed by the more fluid variables (or things that change more across the project).
List elements can then be ordered so that the groups of things that are similar will always be contiguous, and vary sequentially within clusters.&lt;/p&gt;

&lt;p&gt;So the only thing I disagree with Damien about is his decision to put space after time:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;var&amp;gt;_&amp;lt;dataset&amp;gt;_&amp;lt;level&amp;gt;_&amp;lt;time&amp;gt;_&amp;lt;spatial&amp;gt;.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This is  because I think that the geography is more stable than the time period for a data collection, and as most of my studies look at changes of variables measured at a location over time I generally want to compare the same spot at multiple times.  There are pros and cons of each approach such as if the analyst wants to make maps of a variable measured at several locations at a single point in time then having the data arranged by time first and then location may make that job simpler.&lt;/p&gt;

&lt;p&gt;I also notice however that the IMOS syntax puts the site spatial location before the year.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visualisation tools for communicating data management concepts</title>
   <link href="http://ivanhanigan.github.com/2015/11/visualisation-tools-for-communicating-data-management-concepts/"/>
   <updated>2015-11-26T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/11/visualisation-tools-for-communicating-data-management-concepts</id>
   <content type="html">&lt;h1&gt;Hyperlinked table of contents that looks like a filing system&lt;/h1&gt;

&lt;p&gt;This looks like it might be useful to display information about filing systems, with a clickable toc that looks like a filing system!&lt;/p&gt;

&lt;p&gt;Source:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://tex.stackexchange.com/a/36185&quot;&gt;http://tex.stackexchange.com/a/36185&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;object data=&quot;/images/tikz_hlink.pdf&quot; type=&quot;application/pdf&quot; width=&quot;600&quot; height=&quot;800&quot;&gt;
  &lt;p&gt;Alternative text - include a link &lt;a href=&quot;images/tikz_hlink.pdf&quot;&gt;to the PDF!&lt;/a&gt;&lt;/p&gt;
&lt;/object&gt;

</content>
 </entry>
 
 <entry>
   <title>A picture of the newnode function for workflow visualisation in R</title>
   <link href="http://ivanhanigan.github.com/2015/11/a-picture-of-the-newnode-function-for-workflow-visualisation-in-r/"/>
   <updated>2015-11-17T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/11/a-picture-of-the-newnode-function-for-workflow-visualisation-in-r</id>
   <content type="html">&lt;h2&gt;Newnode: An R function for visualising workflows&lt;/h2&gt;

&lt;p&gt;The scientific workflow concept is essentially a pipeline.  The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.&lt;/p&gt;

&lt;p&gt;A simple way to keep track of the steps, inputs and outputs is shown in Table below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CLUSTER ,  STEP       , INPUTS                   , OUTPUTS                   
A  ,       Step1      , &quot;Input 1, Input 2&quot;       , Output 1                 
A  ,       Step2      ,  Input 3                 , Output 2                   
B  ,       Step3      , &quot;Output 1, Output 2&quot;     , Output 3                  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps and data listed in this way can be visualised.
To achieve this an R function was written as part of my PhD project and is distributed in the R package available on Github &lt;a href=&quot;https://github.com/ivanhanigan/disentangle&quot;&gt;https://github.com/ivanhanigan/disentangle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is the &lt;code&gt;newnode&lt;/code&gt; function.  The function returns a string of text
written in the &lt;code&gt;dot&lt;/code&gt; language which can be rendered in R using the
&lt;code&gt;DiagrammeR&lt;/code&gt; package, or the standalone &lt;code&gt;graphviz&lt;/code&gt; package.   This creates the graph of this pipeline shown in Figure below.  Note that a new field was added for Descriptions as these are highly recommended.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/steps_basic_rstudio-test.png&quot; alt=&quot;/images/steps_basic_rstudio-test.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Putting bibtex key into my lit review database needs sanitized latex characters</title>
   <link href="http://ivanhanigan.github.com/2015/11/putting-bibtex-key-into-my-lit-review-database-needs-sanitized-latex-characters/"/>
   <updated>2015-11-15T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/11/putting-bibtex-key-into-my-lit-review-database-needs-sanitized-latex-characters</id>
   <content type="html">&lt;p&gt;As I compile my papers for the thesis I am keeping notes for my presentation to discuss the results and scope of each study.  The work on the 'evidence tables' I described in the last two posts has proved useful here.  I allowed my self a breif distraction to dig out some code I had developed for a database of case studies demonstrating eco-social tipping points from historical evidence, and show here how to include bibtex info.  The bibtex key (from Mendeley in my case) is added to the database, and then the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(xtable)
library(rpostgrestools) # my own work
if(!exists(&quot;ch&quot;)) ch &amp;lt;- connect2postgres2(&quot;data_inventory_hanigan_dev4&quot;)

dat &amp;lt;- dbGetQuery(ch,
&quot;SELECT id, dataset_id, bibtex_key, title,  key_results,background_to_study, 
       research_question, study_extent, outcomes, exposures,  covariates, method_protocol,
        general_comments
  FROM publication
  where key_results is not null and key_results != '';
&quot;)
names(dat) &amp;lt;- gsub(&quot;_&quot;, &quot; &quot;, names(dat))
tabcode &amp;lt;- xtable(dat[,1:8])
align(tabcode) &amp;lt;-  c( 'l', 'p{.7in}','p{.8in}','p{1.7in}', 'p{1.7in}', 'p{1.7in}','p{1.8in}','p{1.7in}', 'p{1.7in}')
print(tabcode,  include.rownames = F, table.placement = '!ht',
      floating.environment='sidewaystable',
      sanitize.text.function = function(x) x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Produces the below table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/bibtable.png&quot; alt=&quot;/images/bibtable.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Developing a lit review database</title>
   <link href="http://ivanhanigan.github.com/2015/11/developing-a-lit-review-database/"/>
   <updated>2015-11-14T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/11/developing-a-lit-review-database</id>
   <content type="html">&lt;p&gt;My work yesterday on implementing &lt;a href=&quot;/2015/11/judging-the-evidence-using-a-literature-review-database&quot;&gt;a lit review section of my data inventory database&lt;/a&gt;
went pretty well, and I tested this while collating information on the papers I compile into my thesis.&lt;/p&gt;

&lt;p&gt;However, as I worked through the information for each paper I realised that attaching this stuff at the EML/dataset level is not always going to work.
In particular I have projects in which several papers are part of the one dataset.  To deal with this I have invented a non-EML relationship module for publications.
So in my new schema the following is possible&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;eml/project/
           /dataset1/
                    /publication1
                    /publication2
           /dataset2/
                    /etc  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;In this scenario a single dataset (ie bushfire smoke, temperature and mortality) can be used to write one paper focused on smoke, controlling for temperature.  Another paper on heatwaves, controlling for smoke. Indeed we might use time-series Poisson models for one and case-crossover design for the other (this is similar to what I did with Johnston and Morgan).&lt;/p&gt;

&lt;p&gt;So the simple thing to do is input these 'evidence tables' fields at the publication level, rather than the dataset as I did yesterday.&lt;/p&gt;

&lt;p&gt;This also partially solves my problem about aggregating these non-EML tags inside the text of the abstract, and worrying about parsing that to extract the elements of information.&lt;/p&gt;

&lt;h2&gt;The fields&lt;/h2&gt;

&lt;!-- html table generated in R 3.2.2 by xtable 1.7-4 package --&gt;


&lt;!-- Sat Nov 14 10:12:49 2015 --&gt;


&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt; data_inventory_field &lt;/th&gt; &lt;th&gt; description &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Citation &lt;/td&gt; &lt;td&gt; At a minimum author-date-journal, perhaps DOI? &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Key results &lt;/td&gt; &lt;td&gt; Include both effect estimates and uncertainty &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Background to the study &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Research question &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Study extent &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Outcomes &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Exposures &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Covariates &lt;/td&gt; &lt;td&gt; Include covariates, effect modifiers, confounders and subgroups &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Method protocol &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; General Comments &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;


&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;An example&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/datinv_pub.png&quot; alt=&quot;/images/datinv_pub.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Judging the evidence using a literature review database</title>
   <link href="http://ivanhanigan.github.com/2015/11/judging-the-evidence-using-a-literature-review-database/"/>
   <updated>2015-11-13T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/11/judging-the-evidence-using-a-literature-review-database</id>
   <content type="html">&lt;p&gt;I recently read through a lecture slide deck called 'Judging the Evidence' by  Adrian Sleigh for a course PUBH7001 Introduction to Epidemiology, April 30, 2001.&lt;/p&gt;

&lt;p&gt;It had a lot of great material in it but I especially liked the section 'CRITIQUE OF AN EPIDEMIOLOGIC STUDY' and slide 11 'Quantity of data, duplication' which says:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Set clear criteria for admission of studies to your ‘judgement of evidence’
Devise ways to tabulate the information
‘Evidence tables’ show key features of design 
  (source, sample and study pop, N)
  exposures-outcomes measured
  observation methods
  confounding
  key results
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I thought this was a great idea, to build a database for keeping 'evidence tables' for each study I read.&lt;/p&gt;

&lt;p&gt;I then read through all the slides.  There is a lot of great information here, but it was spread out across the narrative.  I realised I wanted to collate these into a 'evidence table'. I also compared this with my understanding of the Ecological Metadata Language (EML) schema and the 'ANU Data Analysis Plan Template' and have put together a bit of a 'cross-walk' that lets me combine all this info and create a evidence table (database).&lt;/p&gt;

&lt;p&gt;I have started to use the database I built which uses EML concepts heavily and I include some these other ideas into my free &lt;code&gt;data_inventory&lt;/code&gt; application for a web2py database &lt;a href=&quot;https://github.com/ivanhanigan/data_inventory&quot;&gt;https://github.com/ivanhanigan/data_inventory&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is a webform style data entry interface, and I think good for these 'evidence tables'.
In the first instance I piggy back a lot of the elements into single EML tags, especially the abstract.
This may make it hard to parse.  The simple solution is to try to keep each element on a seperate line of the absract.&lt;/p&gt;

&lt;h2&gt;The key info for an evidence table entry per study&lt;/h2&gt;

&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt; EML &lt;/th&gt; &lt;th&gt; ANU &lt;/th&gt; &lt;th&gt; Adrian_Sleigh &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/title &lt;/td&gt; &lt;td&gt;  Study name &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/creator &lt;/td&gt; &lt;td&gt;  Person conducting analysis &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; project/personnel/[data_owner or orginator] &lt;/td&gt; &lt;td&gt;  Chief investigator &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/abstract &lt;/td&gt; &lt;td&gt;  Background to the study &lt;/td&gt; &lt;td&gt;  Purpose of Study &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;   Study research question  &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;   Specific hypothesis under study &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;  outcomes of interest/ Exposure variables /Covariates &lt;/td&gt; &lt;td&gt;  exposures-outcomes measured &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;td&gt;  key results &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/studyextent &lt;/td&gt; &lt;td&gt;  Study population &lt;/td&gt; &lt;td&gt;  Study Setting &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;                     &lt;/td&gt; &lt;td&gt;                   &lt;/td&gt; &lt;td&gt;  source / sample and study pop &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/temporalcoverage &lt;/td&gt; &lt;td&gt;  Duration of study &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/methods_protocol &lt;/td&gt; &lt;td&gt;  Study Type &lt;/td&gt; &lt;td&gt;  Type of study &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/sampling_desc &lt;/td&gt; &lt;td&gt;    &lt;/td&gt; &lt;td&gt;  Subject Selection &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/methods_steps &lt;/td&gt; &lt;td&gt;  analytical strategy &lt;/td&gt; &lt;td&gt;  Statistical procedures &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;                       &lt;/td&gt; &lt;td&gt;  exposures/ potential confounders or effect modifiers &lt;/td&gt; &lt;td&gt;  Confounding &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; entity/numberOfRecords &lt;/td&gt; &lt;td&gt;  Number study subjects &lt;/td&gt; &lt;td&gt;  N &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/distribution_methods &lt;/td&gt; &lt;td&gt;  dissemination strategy &lt;/td&gt; &lt;td&gt;    &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;




&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Here is a screen shot of my data inventory data entry form&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/datinv_entry.png&quot; alt=&quot;/images/datinv_entry.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Adopting a bullet point style</title>
   <link href="http://ivanhanigan.github.com/2015/11/adopting-a-bullet-point-style/"/>
   <updated>2015-11-12T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/11/adopting-a-bullet-point-style</id>
   <content type="html">&lt;p&gt;With respect to bullet points:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;there are a range of styles accepted&lt;/li&gt;
&lt;li&gt;you just have to be consistent&lt;/li&gt;
&lt;li&gt;I have decided I like the bullet point style from
&lt;a href=&quot;http://www.monash.edu/about/editorialstyle/editing/punctuation&quot;&gt;http://www.monash.edu/about/editorialstyle/editing/punctuation&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;End the introductory phrase preceding a list of bullet points in a
colon. If the individual bullet points are sentence fragments, don't
use a full stop, comma or semi-colon. Leave it bare until the last
bullet point, and then use a full stop. Don't use capitals. Use full
stops if each bullet point is a complete sentence.
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Decisions to make when modelling interactions</title>
   <link href="http://ivanhanigan.github.com/2015/10/decisions-to-make-when-modelling-interactions/"/>
   <updated>2015-10-31T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/decisions-to-make-when-modelling-interactions</id>
   <content type="html">&lt;p&gt;This post focuses on decision making during statistical
modelling.  The case of investigating effect modification is used as
an example.  The analyst has several options available to them when
constructing the model parametrisation for adjustment to explain
modification effectively.  Unlike confounding (in which the criterion
of substantial magnitude change of the effect estimate when
controlling for a third variable can be easily assessed), models
including effect modification can be hard to interpret.  Taking
account of effect modification becomes increasingly important when
modelling complex interactions.&lt;/p&gt;

&lt;p&gt;If an effect moderator is present then the relationship between
exposure and response varies between different levels of the
moderator. An example is provided by a paper we published, in which the
relationship between proximity to wetlands and Ross River virus
infection was found to be different for towns and rural areas, where the
'urban' variable is the 'effect moderator'.&lt;/p&gt;

&lt;p&gt;There are three common ways for data analysts to address this question
in statistical models:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;multiple models for each group,&lt;/li&gt;
&lt;li&gt;interaction terms or&lt;/li&gt;
&lt;li&gt;re-parametrisation to explicitly depict interactions.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The first way is
to split apart the dataset and conduct separate analyses of multiple
groups. For example, one could run the regression in the urban zone
and the rural zone seperately and see if there were different exposure
response functions in the two models.  This was the approach of our paper.
This approach has the strength that it is simple to do and yeilds
results that are easy to interpret.  A limitation of this method is
that by splitting the dataset one loses degrees of freedom and
therefore statistical power.&lt;/p&gt;

&lt;p&gt;The second approach (which removes that limitation of the first option) is to
fit interaction terms.  The example shown in Figure 1 is a multiplicative interaction model (Brambor 2006).
This  approach was not taken in the models reported in our paper, but can easily be implemented and shows that the function of distance was
estimated to have different 'slopes' in each of the dichotomous urban
groups.&lt;/p&gt;

&lt;p&gt;The statistical method can be easily implemented in
software by including a multiplicative term between two variables,
however in practice the resulting post estimation regression outputs
can be difficult to interpret.  For example, say one wants to calculate
the effect and standard error for exposure X on health outcome Y with
the interaction of the effect modifier Z.  The form of this model can
be written as:&lt;/p&gt;

&lt;p&gt;$$ Y ~ \beta&lt;em&gt;{1}X + \beta&lt;/em&gt;{2}Z + \beta_{3}XZ $$&lt;/p&gt;

&lt;p&gt;where B1, B2 and B3 are the regression coefficients estimated and  the term XZ is the interaction between exposure and effect modifier.&lt;/p&gt;

&lt;p&gt;The difficulty for interpretation comes when using this method for calculating the marginal effect of X on Y and the conditional standard error.  The specific method described in Brambor et al. is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calculate the coefficients and the variance-covariance matrix from the regression model&lt;/li&gt;
&lt;li&gt;The marginal effect is $\beta&lt;em&gt;{1} + \beta&lt;/em&gt;{3}XZ$ where Z is the level of the
modifying factor (0 or 1 in the dichotomous effect modifier case)&lt;/li&gt;
&lt;li&gt;The conditional standard error is:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;$$\sqrt{var(\beta&lt;em&gt;{1}) + Z&lt;sup&gt;2&lt;/sup&gt; var(\beta&lt;/em&gt;{3}) + 2Zcov(\beta&lt;em&gt;{1}\beta&lt;/em&gt;{3})}$$&lt;/p&gt;

&lt;p&gt;Therefore the strengths of this approach is that it does not reduce
degrees of freedom and is straightforward to specify the model in
standard statistical software packages. The limitations are related to
interpretation of the resulting coefficients for both the main effects and the marginal effects, and standard errors for these.&lt;/p&gt;

&lt;p&gt;The third approach available to analysts makes it easier to access the
resulting regression output.  This method was employed in Paper 1 in
the final modelling phase in which estimates were calculated for the
different drought exposure-response funcitons in each of the
sub-groups. In the terms of Figure \ref{fig:effectmod.png} it is simple to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calculate X1 = X * Z (i.e = exposure for condition is met, zero otherwise)&lt;/li&gt;
&lt;li&gt;Calculate X0 = X * (1-Z) (i.e. = exposure for NON-condition, zero for condition is TRUE)&lt;/li&gt;
&lt;li&gt;Instead of X, Z and XZ, fit X1, X0 and Z.  This model also contains three parameters and captures the same interactions as it is the same model with a different parametrisation.  The standard errors for the X1 and X0 are calculated directly from the regression.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This method is much easier to implement and interpret.  This is also
considerably more flexible than the other two approaches.  A
limitation remains for this method in that the pre-processing steps
required are more complicated, and there are inherently more
possibilities for the data analyst to make errors in writing their
code as they make these changes to the analytical data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Demonstrating this with the data from the paper (available in table 2)
## the following code shows the different parametrisations for the effect modification by urban
## we show that the coeffs and se are equivalent but that the psuedo-R
## squared will be better when including all our data in stratified analysis

# model 0 effect in eastern
d_eastern
fit &amp;lt;- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_eastern )
summa &amp;lt;- summary(fit)
summa
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept) -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer      -0.24702    0.07921  -3.119  0.00182 **

# model 1 effect in urban
fit1 &amp;lt;- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_urban )
summa &amp;lt;- summary(fit1)
summa
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept) -5.52363    0.33354 -16.561   &amp;lt;2e-16 ***
## buffer       0.03853    0.06352   0.607    0.544

# step 1, combine the urban and rural data
d_eastern$urban &amp;lt;- 0
d_urban$urban &amp;lt;- 1
dat2 &amp;lt;- rbind(d_eastern, d_urban)
str(dat2)
dat2

# model 2 is a multiplicative term
fit2 &amp;lt;- glm(cases ~ buffer * urban + offset(log(1+pops)), family = 'poisson', data = dat2)
summa &amp;lt;- summary(fit2)
summa
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer       -0.24702    0.07921  -3.119  0.00182 **
## urban        -0.69938    0.36350  -1.924  0.05435 .
## buffer:urban  0.28555    0.10153   2.812  0.00492 **

# the coeff on buffer is for urban = 0 is main effect
# the coeff on buffer:urban is for urban = 1 is the marginal effect
b1 &amp;lt;- summa$coeff[2,1]
b3 &amp;lt;- summa$coeff[4,1]
b1 + b3
# 0.0385268
# but what about that p-value?  and the se?
str(fit2)
fit2_vcov &amp;lt;- vcov(fit2)
fit2_vcov
# now calculate the conditional standard error for the marginal effect of buffer for the value of the modifying variable (Z, urban =1)
varb1&amp;lt;-fit2_vcov[2,2]
varb3&amp;lt;-fit2_vcov[4,4]
covarb1b3&amp;lt;-fit2_vcov[2,4]
Z&amp;lt;-1
conditional_se &amp;lt;- sqrt(varb1+varb3*(Z^2)+2*Z*covarb1b3)
conditional_se



# model 3 is the re-parametrisation
dat2$buffer_urban &amp;lt;- dat2$buffer * dat2$urban
dat2$buffer_eastern &amp;lt;- dat2$buffer * (1-dat2$urban)

fit3 &amp;lt;- glm(cases ~ buffer_urban + buffer_eastern + urban + offset(log(1+pops)), family = 'poisson', data = dat2)
summa &amp;lt;- summary(fit3)
summa

## Coefficients:
##                Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)    -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer_urban    0.03853    0.06352   0.607  0.54416
## buffer_eastern -0.24702    0.07921  -3.119  0.00182 **
## urban          -0.69938    0.36350  -1.924  0.05435 .
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Show missingness in large dataframes, version 2</title>
   <link href="http://ivanhanigan.github.com/2015/10/show-missingness-in-large-dataframes-v2/"/>
   <updated>2015-10-28T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/show-missingness-in-large-dataframes-v2</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;UPDATE: the other day I blogged this but I needed to tweak things, so this is a re-post with extra&lt;/li&gt;
&lt;li&gt;UPDATE 2: Today an R blogger has posted a new solution &lt;a href=&quot;/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger&quot;&gt;/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;The old post&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Sometime ago I saw this example of a method for assessing missing data in a large data frame &lt;a href=&quot;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&quot;&gt;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I asked my colleague Grant about doing this in R and he whipped up the following code to generate such an image:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/bankstown_traffic_counts_full_listing_june_2014.svg&quot; alt=&quot;/images/bankstown_traffic_counts_full_listing_june_2014.svg&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;misstable &amp;lt;- function(atable){
 op &amp;lt;- par(bg = &quot;white&quot;)
 plot(c(0, 400), c(0, 1000), type = &quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;,
     main = &quot;Missing Data Table&quot;)


 pmin=000
 pmax=400
 stre=pmax-pmin
 lnames=length(atable)
 cstep = (stre/lnames)
 for(titles in 1:lnames){
 text((titles-1) * cstep+pmin+cstep/2,1000,colnames(atable)[titles])
 }

 gmax=900
 gmin=0
 gstre=gmax-gmin
 rvec = as.vector(atable[ [ 1 ] ])
 dnames=length(rvec)
 step = gstre / dnames
 for(rows in 1:dnames){
 text(30,gmax - (rows-1)*step-step/2,rvec[rows])
 ymax=gmax - (rows-1)*step
 ymin=gmax - (rows)*step
 for(col in 2:lnames-1){
 if(atable[rows,col+1] == F){
 tcolor = &quot;red&quot;
 }
 if(atable[rows,col+1] == T){
 tcolor = &quot;white&quot;
 }
 rect((col) * (stre/lnames)+pmin, ymin, (col+1) * (stre/lnames)+pmin,
 ymax,col=tcolor,lty=&quot;blank&quot;)
 }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Now things to note are that the function expects the data to be TRUE if Not NA and  FALSE if is NA&lt;/li&gt;
&lt;li&gt;so might need to massage things a bit first&lt;/li&gt;
&lt;li&gt;here is the small test Grant supplied&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(grDevices)

# Make a quick dataframe with true/false representing data availability
locs=c(&quot;Australia&quot;,&quot;India&quot;,&quot;New Zealand&quot;,&quot;Sri Lanka&quot;,&quot;Uruguay&quot;,&quot;Somalia&quot;)
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable
#Draw the table.
misstable(atable)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;here is the one I worked on today&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# having defined the input dir and input file tried reading the excel sheet (without head 3 rows)
#dat &amp;lt;- readxl::read_excel(file.path(indir, infile), skip =3)
# got lots of warnings()
## 50: In read_xlsx_(path, sheet, col_names = col_names, col_types = col_types,  ... :
##   [1278, 4]: expecting date: got '[NULL]'
# I always worry about using excel connections so open in excel (in windows) 
# and save as to convert to CSV
dat &amp;lt;- read.csv(file.path(indir, gsub(&quot;.xlsx&quot;, &quot;.csv&quot;, infile)), skip =3, stringsAsFactor = F)
str(dat)
# 'data.frame':     1396 obs. of  167 variables:
# but most of the cols and a third of the rows are empty!
# check missings
dat2 &amp;lt;- data.frame(id = 1:nrow(dat), dat)
str(dat2)
# first if they are empty strings
dat2[dat2 == &quot;&quot;] &amp;lt;- NA
# now if NA
dat2[,2:ncol(dat2)] &amp;lt;- !is.na(dat2[,2:ncol(dat2)])

# Truncate the hundreds of empty cols
str(dat2[,1:18])
tail(dat2[,1:18])
svg(file.path(outdir, gsub(&quot;.csv&quot;, &quot;.svg&quot;, outfile))    )
misstable(dat2[,1:18])
dev.off()
browseURL(file.path(outdir, gsub(&quot;.csv&quot;, &quot;.svg&quot;, outfile))    )

# cool, that is an effective way to look at the data
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>show-missingness-in-large-dataframes</title>
   <link href="http://ivanhanigan.github.com/2015/10/show-missingness-in-large-dataframes/"/>
   <updated>2015-10-26T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/show-missingness-in-large-dataframes</id>
   <content type="html">&lt;p&gt;Sometime ago I saw this example of a method for assessing missing data in a large data frame &lt;a href=&quot;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&quot;&gt;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I asked my colleague Grant about doing this in R and he whipped up the following code to generate such an image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/misstable.png&quot; alt=&quot;/images/misstable.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;misstable &amp;lt;- function(atable){
 op &amp;lt;- par(bg = &quot;white&quot;)
 plot(c(0, 400), c(0, 1000), type = &quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;,
     main = &quot;Missing Data Table&quot;)


 pmin=000
 pmax=400
 stre=pmax-pmin
 lnames=length(atable)
 cstep = (stre/lnames)
 for(titles in 1:lnames){
 text((titles-1) * cstep+pmin+cstep/2,1000,colnames(atable)[titles])
 }

 gmax=900
 gmin=0
 gstre=gmax-gmin
 rvec = as.vector(atable[ [ 1 ] ])
 dnames=length(rvec)
 step = gstre / dnames
 for(rows in 1:dnames){
 text(30,gmax - (rows-1)*step-step/2,rvec[rows])
 ymax=gmax - (rows-1)*step
 ymin=gmax - (rows)*step
 for(col in 2:lnames-1){
 if(atable[rows,col+1] == F){
 tcolor = &quot;red&quot;
 }
 if(atable[rows,col+1] == T){
 tcolor = &quot;white&quot;
 }
 rect((col) * (stre/lnames)+pmin, ymin, (col+1) * (stre/lnames)+pmin,
 ymax,col=tcolor,lty=&quot;blank&quot;)
 }
 }
}

require(grDevices)

# Make a quick dataframe with true/false representing data availability
locs=c(&quot;Australia&quot;,&quot;India&quot;,&quot;New Zealand&quot;,&quot;Sri Lanka&quot;,&quot;Uruguay&quot;,&quot;Somalia&quot;)
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable
#Draw the table.
misstable(atable)
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Sanitize mendeley references in r markdown reporting</title>
   <link href="http://ivanhanigan.github.com/2015/10/sanitize-mendeley-references-in-r-markdown-reporting/"/>
   <updated>2015-10-24T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/sanitize-mendeley-references-in-r-markdown-reporting</id>
   <content type="html">&lt;p&gt;A key challenge for Reproducible Research Reports in Rmarkdown remains adequate scholarly citation management; the machinery of scholarly citations and references.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;knitcitations&lt;/code&gt; R package does a great job of working with bibtex bibliography files, however the bibtex manager that I use is Mendeley and it has implemented some rules on the way it handles special characters that forces the bibtex references into a state with some 'escaped' elements that breaks their presentation via &lt;code&gt;knitcitations&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is a post from my open notebook that shows the workaround I am using for sanitizing the Mendeley escaped underscores in URLS.&lt;/p&gt;

&lt;p&gt;Here is an example:&lt;/p&gt;

&lt;p&gt;There is considerable public health impact from the effects on mental of drought &lt;span class=&quot;citation&quot;&gt;(&lt;span&gt;Sarathi Biswas&lt;/span&gt; 2012)&lt;/span&gt;. It is proposed that the best method to disentangle the multifactorial nature of this causal mechanism is the ‘five-capitals’ framework, indeed this method may even enable understanding the human carrying capacity of ecosystems &lt;span class=&quot;citation&quot;&gt;(McMichael &amp;amp; Butler 2002)&lt;/span&gt;.&lt;/p&gt;




&lt;p&gt;McMichael, A.J. &amp;amp; Butler, C.D. (2002). Global health trends: Evidence for and against sustainable progress. &lt;em&gt;International Union for the Scientific Study of Population Committee on Emerging Health Threats&lt;/em&gt;. &lt;a href=&quot;http://www.demogr.mpg.de/papers/workshops/020619{\_}paper25.pdf&quot;&gt;http://www.demogr.mpg.de/papers/workshops/020619{\_}paper25.pdf&lt;/a&gt; [21 Sep. 2003]&lt;/p&gt;


&lt;p&gt;&lt;span&gt;Sarathi Biswas&lt;/span&gt;, P. (2012). Alcohol, drought lead to farmer’s suicide. &lt;em&gt;Daily News and Analysis&lt;/em&gt;. &lt;a href=&quot;http://www.dnaindia.com/pune/report{\_}alcohol-drought-lead-to-farmers-suicide{\_}1688976&quot;&gt;http://www.dnaindia.com/pune/report{\_}alcohol-drought-lead-to-farmers-suicide{\_}1688976&lt;/a&gt; [17 May 2012]&lt;/p&gt;




&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;See those pesky curly braces &lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt; around the underscores?&lt;/h2&gt;

&lt;h2&gt;The fix&lt;/h2&gt;

&lt;p&gt;The fix I am using is to sanitize each record where this is an issue as I build my document, so the mendeley version stays as-is, while the R version has been sanitized by removing the escape characters.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# read mendeley bibtex file
bib &amp;lt;- read.bibtex(&quot;~/references/library.bib&quot;)
# ad hoc fix
for(bibkey in c(&quot;SarathiBiswas2012&quot;, &quot;Mcmichael2002a&quot;)){
  bib[ [ bibkey ] ]$url &amp;lt;- gsub(&quot;\\{\\\\_\\}&quot;,&quot;_&quot;, bib[ [ bibkey ] ]$url)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the result:&lt;/p&gt;

&lt;p&gt;McMichael, A.J. &amp;amp; Butler, C.D. (2002). Global health trends: Evidence for and against sustainable progress. &lt;em&gt;International Union for the Scientific Study of Population Committee on Emerging Health Threats&lt;/em&gt;. &lt;a href=&quot;http://www.demogr.mpg.de/papers/workshops/020619_paper25.pdf&quot;&gt;http://www.demogr.mpg.de/papers/workshops/020619_paper25.pdf&lt;/a&gt; [21 Sep. 2003]&lt;/p&gt;


&lt;p&gt;&lt;span&gt;Sarathi Biswas&lt;/span&gt;, P. (2012). Alcohol, drought lead to farmer’s suicide. &lt;em&gt;Daily News and Analysis&lt;/em&gt;. &lt;a href=&quot;http://www.dnaindia.com/pune/report_alcohol-drought-lead-to-farmers-suicide_1688976&quot;&gt;http://www.dnaindia.com/pune/report_alcohol-drought-lead-to-farmers-suicide_1688976&lt;/a&gt; [17 May 2012]&lt;/p&gt;




&lt;p&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Keeping an electronic lab notebook for computational statistics projects</title>
   <link href="http://ivanhanigan.github.com/2015/10/keeping-an-electronic-lab-notebook-for-computational-statistics-projects/"/>
   <updated>2015-10-23T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/keeping-an-electronic-lab-notebook-for-computational-statistics-projects</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;In my previous post on this topic &lt;a href=&quot;http://ivanhanigan.github.io/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/&quot;&gt;http://ivanhanigan.github.io/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/&lt;/a&gt; I summarised some recommendations for electronic lab notebooks&lt;/li&gt;
&lt;li&gt;I've collated from a variety of sources for managing computational statistics projects in a reproducible research Pipelines&lt;/li&gt;
&lt;li&gt;One thing I found while reading the literature around this topic is that the concepts are difficult to really grasp until I see them being used&lt;/li&gt;
&lt;li&gt;The example worklog from Scott Long was a good insight into his method, that I really got more out of the figure below than from descriptions of the concept&lt;/li&gt;
&lt;li&gt;screen shot taken from Long, S. (2012). Principles of Workflow in Data Analysis. Retrieved from &lt;a href=&quot;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&quot;&gt;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&lt;/a&gt; (accessed 2015-10-23).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/worklog-long.png&quot; alt=&quot;/images/worklog-long.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;I added a red box around an important aspect of this example, the communication of gory details, that are often difficult to track if not using a notebook to log our work&lt;/li&gt;
&lt;li&gt;ARGH! indeed.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Replication from Noble's description&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;That looks good, but I also really liked the description in Noble's paper where scripts that do computations are linked to log entries&lt;/li&gt;
&lt;li&gt;This was something that I felt I wanted to see in action&lt;/li&gt;
&lt;li&gt;Without an example online, I had to have a go at creating one from the instructions&lt;/li&gt;
&lt;li&gt;I also had to make some modifications to the method because I want to have this set up work in a multidisciplinary team with some users on windows and others on linux, sharing the project on a network folder&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;My paraphrasing of Noble's description&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Worklog: this is the main file, like a 'lab notebook' for the analyst to track their work.  This  document resides in the root of the project directory and records your progress in detail. Entries in the notebook should be dated, and they should be relatively verbose, with links or embedded images or tables displaying the results of the experiments that you performed. In addition to describing precisely what you did, the notebook should record your observations, conclusions, and ideas for future work&lt;/li&gt;
&lt;li&gt;For group work, this can also contain a 'working' folder for each person to store their messy day-to-day files that we don't want to clutter up the main folder (eg 'working_ivan')&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Conventions I used for writing the worklog entries are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Names follow this structure [**] [date in ISO 8601 YYYY-MM-DD] [meeting/notes/results] [with/from UserName] [Re: topic shortname]&lt;/li&gt;
&lt;li&gt;'meetings' are for both agenda preparation and also notes of discussion&lt;/li&gt;
&lt;li&gt;'notes' are such things as emailed information or ad hoc Discovery&lt;/li&gt;
&lt;li&gt;'results' are entries related to a section of the 'results' folder. That is, this kind of entry is in parallel to the results entry (see below), however the log contains a prose description of the experiment, whereas the results folder contains scripts etc of all the gory details.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Tests&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I use Emacs on linux for most of my work but I need to share with windows users so tested out keeping the log in a MS word doc.  This got corrupted quickly I think because I edited in Libre office.&lt;/li&gt;
&lt;li&gt;I decided to try and just use a plain text format&lt;/li&gt;
&lt;li&gt;text files created in Ubuntu are so difficult to understand (read) when opened in Windows' Notepad. No matter how many lines have been used, all the lines appear in the same one line.
To set the buffer coding to DOS style issue:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;code&gt;M-x set-buffer-file-coding-system utf-8-dos&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;a couple of examples&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;As this is a plain text document opening it in emacs will not automatically render it in the Orgmode fashion&lt;/li&gt;
&lt;li&gt;To achieve this the command is &lt;code&gt;M-x org-mode&lt;/code&gt; and the file looks like below&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/worklog-ivan1.png&quot; alt=&quot;/images/worklog-ivan1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;From here I can keep adding new entries at the bottom, and have a section for URGENT ACTION a the top&lt;/li&gt;
&lt;li&gt;Orgmode can expand the entries by moving to that line and hitting TAB, or use the command &lt;code&gt;C-u C-u C-u TAB&lt;/code&gt; to expand all branches&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/worklog-ivan2.png&quot; alt=&quot;/images/worklog-ivan2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;the 'Experiment Results' level is about work you might do on a single day, or over a week or two&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Each results subfolder would have workflow scripts that does the work&lt;/li&gt;
&lt;li&gt;At this level each 'experiment' is written up in chronological order&lt;/li&gt;
&lt;li&gt;It is recommended to store every command used while performing the experiment preferably as an executable script that carries out the entire experiment automatically&lt;/li&gt;
&lt;li&gt;you should end up with a file that is parallel to the worklog entry&lt;/li&gt;
&lt;li&gt;The worklog contains a prose description of the experiment, whereas the driver script contains all the gory details&lt;/li&gt;
&lt;li&gt;this is the level I usually think of the distribution side of things&lt;/li&gt;
&lt;li&gt;You may want to pack up the results from one of these folders and email it to the collaborators, or decide on the one set of tables and figures to write into the manuscript for submission to a journal&lt;/li&gt;
&lt;li&gt;If this is accepted for publication, this is the one combined package of 'analytical data and code' that I would consider putting up online as supporting information for the paper.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Example&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I followed Noble's advice to create a driver script to set up the folder structure:&lt;/li&gt;
&lt;li&gt;it is in my Github R package &lt;code&gt;disentangle&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;&amp;gt; dir.create(&quot;exposures_blending&quot;)
&amp;gt; setwd(&quot;exposures_blending&quot;)
&amp;gt; disentangle::AdminTemplate()
[1] TRUE
&amp;gt; dir.create(&quot;results&quot;)
&amp;gt; setwd(&quot;results&quot;)
&amp;gt; makeProject::makeProject(&quot;2015-10-23-preliminary-modelling&quot;)
Creating Directories ...
Creating Code Files ...
Complete ...
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This populates the folders as shown below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/worklog-ivan3.png&quot; alt=&quot;/images/worklog-ivan3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Conclusions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I feel pretty happy with this as a replication of Noble's method&lt;/li&gt;
&lt;li&gt;My colleagues can look at my work and see a high level log that links to the gory details of day to day life in the trenches&lt;/li&gt;
&lt;li&gt;The only downside I can see at the moment is that my colleagues on windows will see a text file that is pretty dense, and will not be as easy to navigate as a word document (or emacs org file)&lt;/li&gt;
&lt;li&gt;Perhaps notepad++ can be used instead.  On my windows machine I did a quick experiment with NPP and found that under the Language menu &gt; Define your language there is a method to define code folding with ** as the opening.  Just need to define a closing tag.  I experimented with '---' which might be good, but ultimately I don't think my colleagues are going to want to do this on their machines.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Reproducible research pipelines improve description of method steps</title>
   <link href="http://ivanhanigan.github.com/2015/10/reproducible-research-pipelines-improve-description-of-method-steps/"/>
   <updated>2015-10-18T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/reproducible-research-pipelines-improve-description-of-method-steps</id>
   <content type="html">&lt;p&gt;Adequately documenting the methods and results of data analysis helps
safeguard against errors of execution and
interpretation. It is proposed that
reproducible research pipelines address the problem of
adequate documentation of data analysis.&lt;/p&gt;

&lt;p&gt;This is because they make it easy to
check the methods. Assumptions are easy to challenge and results
verified in new analyses. Reproducible research pipelines extend
traditional research.  They do this by encoding the steps in a
computer ‘scripting’ language and distributing the data and code with
publications.  Traditional research moves through the steps of
hypothesis and design, measured data, analytic data, computational
results (for figures, tables and numerical results), and reports (text
and formatted manuscript).&lt;/p&gt;

&lt;h2&gt;Fundamental components of a reproducible research pipeline&lt;/h2&gt;

&lt;p&gt;The basic components of a pipeline are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data Management Plan and Data Inventory&lt;/li&gt;
&lt;li&gt;Method steps&lt;/li&gt;
&lt;li&gt;Code&lt;/li&gt;
&lt;li&gt;Data storage&lt;/li&gt;
&lt;li&gt;Reports&lt;/li&gt;
&lt;li&gt;Distribution.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Method Steps&lt;/h3&gt;

&lt;p&gt;The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.&lt;/p&gt;

&lt;p&gt;A simple way to keep track of the steps, inputs and outputs is shown in the Table below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(stringr)
steps &amp;lt;- read.csv(textConnection('
CLUSTER ,  STEP  , INPUTS                   , OUTPUTS                   
A  ,  Step1      , &quot;Input 1, Input 2&quot;       , &quot;Output 1&quot;                 
A  ,  Step2      ,  Input 3                  , Output 2                   
B  ,  Step3      , &quot;Output 1, Output 2&quot;      , Output 3                  
'), stringsAsFactors = F, strip.white = T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;The steps and data listed in the Table above can be visualised.
To achieve this an R function was written as part of this PhD project and is distributed in my own R package available on Github &lt;a href=&quot;https://github.com/ivanhanigan/disentangle&quot;&gt;https://github.com/ivanhanigan/disentangle&lt;/a&gt;.
This is the &lt;code&gt;newnode&lt;/code&gt; function.  The function returns a string of text
written in the &lt;code&gt;dot&lt;/code&gt; language which can be rendered in R using the
&lt;code&gt;DiagrammeR&lt;/code&gt; package, or the standalone &lt;code&gt;graphviz&lt;/code&gt; package.   This creates the graph of this pipeline shown in Figure below.  Note that a new field was added for Descriptions as these are highly recommended.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(disentangle); library(stringr)
nodes &amp;lt;- newnode(indat = steps,   names_col = &quot;STEP&quot;, in_col = &quot;INPUTS&quot;,
  out_col = &quot;OUTPUTS&quot;, 
  nchar_to_snip = 40)
sink(&quot;fig-basic.dot&quot;);
cat(nodes);
sink()
# or DiagrammeR::grViz(nodes)
system(&quot;dot -Tpdf fig-basic.dot -o fig-basic.pdf&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/fig-basic.png&quot; alt=&quot;/images/fig-basic.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How To Effectively Implement Electronic Lab Notebooks In Epidemiology</title>
   <link href="http://ivanhanigan.github.com/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/"/>
   <updated>2015-10-17T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;It is often stated in the literature that an electronic lab notebook is a core component of reproducible research&lt;/li&gt;
&lt;li&gt;For example the following is from Buck, S. (2015). Solving reproducibility. Science, 348(6242), 1403–1403. &lt;a href=&quot;http://dx.doi.org/10.1126/science.aac8041&quot;&gt;http://dx.doi.org/10.1126/science.aac8041&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;one of the most effective ways to promote high-quality science 
is to create free open-source tools that give scientists
easier and cheaper ways to incorporate transparency
into their daily workflow: from open lab notebooks, to
software that tracks every version of a data set, to dynamic 
document generation.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;But I have struggled to operationalise the lab notebook in the epidemiology projects I work in&lt;/li&gt;
&lt;li&gt;Here are some notes based on my recent readings and attempts with a new team&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Modularised lab notebooks&lt;/h2&gt;

&lt;p&gt;There seem to be a small number of components to a lab notebook that can be defined as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data management plan&lt;/li&gt;
&lt;li&gt;Workplan&lt;/li&gt;
&lt;li&gt;Worklog&lt;/li&gt;
&lt;li&gt;Workflow&lt;/li&gt;
&lt;li&gt;Distribution&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;One thing I think is important is to have levels of organisation in a hierarchy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Macro level: The 'Research Programme' level is about the entire breadth of the projects in the group.

&lt;ul&gt;
&lt;li&gt;Data Management Plan: including managing the computers and a Data Inventory&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Personal Workplan and Worklog&lt;/em&gt;: this is an overview of things I do, plan to do or learn along the way (this is for the high level things like planning professional development, or a holiday)&lt;/li&gt;
&lt;li&gt;This is operationalised in the blog you are reading right now.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Meso level: the 'Research Project' level is about a single study, or a small group of studies based around a core dataset or Concept

&lt;ul&gt;
&lt;li&gt;This is the level that you might write up a manuscript for a journal, or report to a client&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Project workplan&lt;/em&gt;: at this level there may be high level information about the study design, hypotheses, resources and admin for managing relationships with a variety of collaborators&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Worklog&lt;/em&gt;: WS Noble &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1000424&quot;&gt;http://dx.doi.org/10.1371/journal.pcbi.1000424&lt;/a&gt; recommends that this be the main lab notebook for the analysts&lt;/li&gt;
&lt;li&gt;He says 'This is a document that resides in the root of the results directory and that records your progress in detail. Entries in the notebook should be dated, and they should be relatively verbose, with links or embedded images or tables displaying the results of the experiments that you performed. In addition to de- scribing precisely what you did, the notebook should record your observations, conclusions, and ideas for future work'&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Micro level: the 'Experiment Results' level is about work you might do on a single day, or over a week

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Workflow&lt;/em&gt; scripts: At this level each 'experiment' is written up in chronological order, as entries to the Worklog at the meso level&lt;/li&gt;
&lt;li&gt;Noble recommends 'create either a README file, in which I store every command line that I used while performing the experi- ment, or a driver script (I usually call this runall) that carries out the entire experiment automatically'...&lt;/li&gt;
&lt;li&gt;and 'you should end up with a file that is parallel to the lab notebook entry. The lab notebook contains a prose description of the exper- iment, whereas the driver script contains all the gory details.'&lt;/li&gt;
&lt;li&gt;this is the level I usually think of managing the distribution side of things.  I will want to pack up the results and email to my collaborators, or decide on the one set of tables and figures to write into the manuscript for submission to a journal.  If this is accepted for publication, this is the one combined package of 'analytical data and code' that I would consider putting up online (to github) as supporting information for the paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>GIS Issues when R is Used for Transforming Coordinate Systems</title>
   <link href="http://ivanhanigan.github.com/2015/10/gis-issues-when-r-used-transforming-coordinate-systems/"/>
   <updated>2015-10-16T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/gis-issues-when-r-used-transforming-coordinate-systems</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;I have been using RGDAL to transform and write out spatial data in GDA94&lt;/li&gt;
&lt;li&gt;It is a problem to know what I need to do to create the right prj file for ArcGIS to read without complaining&lt;/li&gt;
&lt;li&gt;I have an example of code below, that I used on a dataset I knew was in GDA94 when I read it in.  I want to do this mostly for when I have to convert from one to another, but I have done the manual hack a couple of times now and thought I better just check with an expert.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;infile &amp;lt;- &quot;ap_map&quot;
outfile &amp;lt;- gsub(&quot;map&quot;, &quot;map_GDA94.shp&quot;, infile)
outfile
setwd(indir)
shp &amp;lt;- readOGR(&quot;.&quot;, infile)
setwd(projdir)
plot(shp, add = T)
#str(shp)
shp@proj4string@projargs
#[1] &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;
# confirm this is GDA94
epsg &amp;lt;- make_EPSG()
str(epsg)
epsg[grep(4283, epsg$code),]
#     code    note                                                       prj4
# 212 4283 # GDA94   +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs
# Arcmap sees this as GRS 1980(IUGG, 1980) which I think is the same thing
# to be on the safe side I will force it to refer to GDA94
shp2 &amp;lt;- spTransform(shp, CRS(&quot;+init=epsg:4283&quot;))
shp2@proj4string@projargs
#now write
setwd(outdir)
dir()
writeOGR(shp2,  outfile, gsub(&quot;.shp&quot;, &quot;&quot;, outfile), driver = &quot;ESRI Shapefile&quot;)
setwd(projdir)
# Checking this shows it did not write the correct prj file, but I believe that this is because the GDA94 definition is not different to the WGS80 params.
# one other option is to manually replace that prj file with the correct text found at http://spatialreference.org/ref/epsg/4283/
# SO I did this to avoid any future confusions
# OLD GEOGCS[&quot;GRS 1980(IUGG, 1980)&quot;,DATUM[&quot;D_unknown&quot;,SPHEROID[&quot;GRS80&quot;,6378137,298.257222101]],PRIMEM[&quot;Greenwich&quot;,0],UNIT[&quot;Degree&quot;,0.017453292519943295]]
# NEW GEOGCS[&quot;GDA94&quot;,DATUM[&quot;D_GDA_1994&quot;,SPHEROID[&quot;GRS_1980&quot;,6378137,298.257222101]],PRIMEM[&quot;Greenwich&quot;,0],UNIT[&quot;Degree&quot;,0.017453292519943295]]
# checked with ArcMap and they align ok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;I asked on of my colleagues and here is his reply&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I usually use the sp and maptools library, rather than the readOGR and writeOGR functions.  Generally my workflow, as an example of projecting a GDA94 zone 55 file to WGS84, would be something like:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;shp &amp;lt;- readShapePoly(&quot;myfile.shp&quot;)
proj4string(shp) &amp;lt;- &quot;+proj=utm +zone=55 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;
shp2 &amp;lt;- spTransform(shp, CRS(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs &quot;))
writePolyShape(shp2,&quot;myfile_p.shp&quot;,proj4string=CRS(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs &quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I've never &lt;em&gt;noticed&lt;/em&gt; any problems doing this, the projection file is generated and I've never noticed any alignment problems in ArcMap.  It's true that I recall ArcMap sometimes displays an incorrect plaintext projection description, but things seem to be in the right place.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, I don't really ever rely on EPSG numbers in R - I just grab the proj4 strings and try to use them directly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO, see if PostGIS handles this OK&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I wonder if my FOSS GIS stack should still pass things through PostGIS as I suspect it does these things better.&lt;/li&gt;
&lt;li&gt;I might check the output if I run it through the DB use st_transform and then extract to shapefile.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>A Set Of Guidelines For Exploratory Data Analysis And Cleaning</title>
   <link href="http://ivanhanigan.github.com/2015/10/a-set-of-guidelines-for-exploratory-data-analysis-and-cleaning/"/>
   <updated>2015-10-14T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/a-set-of-guidelines-for-exploratory-data-analysis-and-cleaning</id>
   <content type="html">&lt;p&gt;The New York Times ran a piece on August 17, 2014: “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights” [1].  The article bemoaned the need for too much of what data scientists call “data wrangling”, “data munging” and “data janitor work”.  In essence this means data quality control processes.
A key task of my role as data manager/analyst is to perform Exploratory Data Analysis (EDA) to review data deposited for consistency, quality and its compliance with standards to ensure that reusability of data published in the portal is maximised.   To do this, I use relevant data formatting standards (for example the International Standards from ISO), undertake thorough taxonomic reviews of each dataset and have well documented procedures for dealing with miscellaneous errors such as data inconsistencies, duplicate variable names and reformatting numeric or character strings. I sometimes make changes to the data and give no further thoughts to it, but at  other times I need to make recommendations to the data provider and ask for their decisions/approvals on what to change.&lt;/p&gt;

&lt;p&gt;I have put down this set of guidelines for my procedures to create standardised data structures, based on things that have been recommended in the literature [2-6] to make data as re-usable as possible. Here is a list of standard amendments undertaken during my EDA process:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;identify any out-of-range values (based on the specified units), or questionable data in general;&lt;/li&gt;
&lt;li&gt;rename all files and variables using lower_case_with_underscores naming convention;&lt;/li&gt;
&lt;li&gt;tabulate frequencies and variable distributions, note any outliers for review;&lt;/li&gt;
&lt;li&gt;identify any opportunities to make wide data longer, or many files that can be merged;&lt;/li&gt;
&lt;li&gt;If you have multiple linked tables, each table should include a column that allows those tables to be linked unambiguously (such as the site_ID variables); check that linking variables that link two or more data tables are identical in each table&lt;/li&gt;
&lt;li&gt;check that values in linked files marry up to values in other files (eg a site code in one file that is missing from the spatial data file);&lt;/li&gt;
&lt;li&gt;write as CSV with quote encapsulated strings (for archival purposes);&lt;/li&gt;
&lt;li&gt;code missing data as NA, or identify if these were actually censored;&lt;/li&gt;
&lt;li&gt;coerce dates to ISO 8601 to be in the the YYYY-MM-DD format, or MMM-YYYY;&lt;/li&gt;
&lt;li&gt;cast nominal variables that use integer codes as character;&lt;/li&gt;
&lt;li&gt;check that all value labels in enumerated lists are described (ie codes for “1” = “low”, “2” = “mid” and “3” = “high”);&lt;/li&gt;
&lt;li&gt;attempt to identify and split any combined variables (like season AND year like “winter-97” or species and comments ie “Banksia Dead”);&lt;/li&gt;
&lt;li&gt;review any species lists against current scientific name conventions, recommend any modifications;&lt;/li&gt;
&lt;li&gt;rename any non-conformant species lists (for instance including comments such as Alive/Dead) to “fauna_descriptor” or “flora_descriptor”;&lt;/li&gt;
&lt;li&gt;identify any characters in numeric or date variables and replace with NA, (add to a comments variable if possible);&lt;/li&gt;
&lt;li&gt;identify any values that Excel may try to convert to date type (for eg. site code “1-5” will appear as 5-Jan and should be rewritten as “site_1-5”);&lt;/li&gt;
&lt;li&gt;use a GIS to confirm spatial coordinates and add geographical coordinates in decimal degrees (GDA94) if only supplied in metres UTM or AMG (always request the datum and the zone);&lt;/li&gt;
&lt;li&gt;check what the coordinates refer to (e.g. approximate location of SW corner of 1ha plot).&lt;/li&gt;
&lt;li&gt;rename files to be consistent with all data in the LTERN Data Portal.  Our standardised names have been created using controlled vocabularies.  Packages and files are designated tracking numbers – this is denoted by a plot network code and a unique “Package” number ascribed to each data package.  Each data package contains one or more data table which is the smallest trackable unit (denoted by a unique “Table” number).&lt;/li&gt;
&lt;li&gt;while we prefer deposit of plain text CSV files, if we receive Excel spreadsheets we check for hidden rows or columns that might not be intended for publication (and  may have been deposited as an oversight).&lt;/li&gt;
&lt;li&gt;it is always best to open Excel workbooks and use the in-built export function to save as CSV files for further re-use.  While R packages and other tools exist to programmatically extract the data from Excel, few tools that interoperate with Excel actually get the all the bug/feature cases right.  It has been noted that
&quot;because working with data that has passed through Excel is hard to get right, data that has passed through Excel is often wrong.&quot; [7]&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;References:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Lohr, S. &lt;a href=&quot;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0&quot;&gt;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., &amp;amp; Supp, S. (2013). Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution, 6(2), 1–10. &lt;a href=&quot;http://dx.doi.org/10.4033/iee.2013.6b.6.f&quot;&gt;http://dx.doi.org/10.4033/iee.2013.6b.6.f&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wickham, H. (Under Review). Tidy data. Journal of Statistical Software, VV(Ii).&lt;/li&gt;
&lt;li&gt;Leek, J. 2014. &lt;a href=&quot;https://github.com/jtleek/datasharing&quot;&gt;https://github.com/jtleek/datasharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Borer, E., Seabloom, E., Jones, M., and Schildhauer, M. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America 90:205–214. &lt;a href=&quot;http://dx.doi.org/10.1890/0012-9623-90.2.205&quot;&gt;http://dx.doi.org/10.1890/0012-9623-90.2.205&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Campbell, J. L., Rustad, L. E., Porter, J. H., Taylor, J. R., Dereszynski, E. W., Shanley, J. B., Gries, C., Henshaw, D. L., Martin, M. E., Sheldon, W. M., and Boose, E. R. 2013. Quantity is Nothing
without Quality: Automated QA/QC for Streaming Environmental Sensor Data. BioScience, 63,
574-585. &lt;a href=&quot;http://dx.doi.org/10.1525/bio.2013.63.7.10&quot;&gt;http://dx.doi.org/10.1525/bio.2013.63.7.10&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mount, J. 2014. Excel spreadsheets are hard to get right. &lt;a href=&quot;http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/&quot;&gt;http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>A Driver Script To Set Up A Data Analysis Pipeline</title>
   <link href="http://ivanhanigan.github.com/2015/10/a-driver-script-to-set-up-a-data-analysis-pipeline/"/>
   <updated>2015-10-12T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/a-driver-script-to-set-up-a-data-analysis-pipeline</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;In my previous post I reviewed a paper by Noble 2009 that proposed recommendations for best practice ways to set up a data analysis pipeline &lt;a href=&quot;http://ivanhanigan.github.io/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/&quot;&gt;http://ivanhanigan.github.io/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I was following up on a series of posts I made about other best practice recommendations &lt;a href=&quot;http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets-part-3/&quot;&gt;http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets-part-3/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In the paper by Noble it is suggested that a one should use a 'driver script' to automate creation of a directory structure, this is the exact way that &lt;code&gt;ProjectTemplate&lt;/code&gt; and &lt;code&gt;makeProject&lt;/code&gt; work as I described them in the series of posts.&lt;/li&gt;
&lt;li&gt;I think Noble's framework offers something new to the recomendations I had canvassed, that is the idea of chronological order of the contents of the results directory.  I think this is an eminently sensible idea and thought that the R function &lt;code&gt;Sys.Date()&lt;/code&gt; would be a great way to start off a project in this way.&lt;/li&gt;
&lt;li&gt;so I have put together the following R function, as an alternative to the &lt;code&gt;makeProject&lt;/code&gt; core function, that I thought I'd name so that there may be a family of makeProject functions, so that analysts have a range of to choose from.  The other candidate would be &lt;code&gt;makeProjectLong&lt;/code&gt;, which I will also put up before long.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;makeProjectNoble &amp;lt;- function(rootdir = getwd()){
  if(!exists(rootdir)) dir.create(rootdir)
  dir.create(file.path(rootdir,'doc'))
  dir.create(file.path(rootdir,'doc','paper'))
  sink(file.path(rootdir,'doc','workplan.Rmd'))
    cat(sprintf(&quot;---\ntitle: Untitled\nauthor: your name\ndate: %s\noutput: html_document\n---\n\n&quot;,
                Sys.Date()))
  sink()
  dir.create(file.path(rootdir,'data'))
  dir.create(file.path(rootdir,'data', Sys.Date()))
  dir.create(file.path(rootdir,'src'))
  dir.create(file.path(rootdir,'results')) 
  dir.create(file.path(rootdir,'results', Sys.Date())) 
  file.create(file.path(rootdir,'README.md'))
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Running this function will deploy the folders and files (I excluded the &lt;code&gt;bin&lt;/code&gt; folder for compiled binaries, as I believe that many data analysts may not need that, and those who do are geeky enough to write their own driver scripts.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/testProjectNoble.png&quot; alt=&quot;/images/testProjectNoble.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Rmarkdown script is waiting for the analysis plan to be pumped out, and work can begin&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/testProjectNobleMD.png&quot; alt=&quot;/images/testProjectNobleMD.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Noble, W. S. (2009). A quick guide to organizing computational biology projects. PLoS Computational Biology, 5(7), 1–5. &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1000424&quot;&gt;http://dx.doi.org/10.1371/journal.pcbi.1000424&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Coming To Grips With Citations In Reproducible Research Reports</title>
   <link href="http://ivanhanigan.github.com/2015/10/coming-to-grips-with-citations-in-reproducible-research-reports/"/>
   <updated>2015-10-09T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/coming-to-grips-with-citations-in-reproducible-research-reports</id>
   <content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Earlier this year I was pleased to stumble on to Petr Keil\'s &lt;a href=&quot;http://www.petrkeil.com/?p=2401&quot;&gt;Simple
template for scientific manuscripts in
Rmarkdown&lt;/a&gt; and the Github Repo
&lt;a href=&quot;https://github.com/petrkeil/Blog/tree/master/2015_03_12_R_ms_template&quot;&gt;https://github.com/petrkeil/Blog/tree/master/2015_03_12_R_ms_template&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I was already using Rmarkdown effectively for everything I wanted
except my bibliography, and this helped a lot.  But I eventually
found I needed to tweak the format of the citation style.  I tried
out a bunch of other CSL files but none felt just right.  I tried out
these after downloading from &lt;a href=&quot;https://github.com/citation-style-language/styles&quot;&gt;https://github.com/citation-style-language/styles&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;american-physiological-society.csl
annals-of-the-association-of-american-geographers.csl
biomed-central.csl
ecology.csl
pnas.csl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;SO I hacked the CSL file&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I used the &lt;code&gt;american-physiological-society.csl&lt;/code&gt; to get stuff I wanted and pasted into the &lt;code&gt;mee.csl&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;get the files from this link &lt;a href=&quot;/rmarkdown_utils/CitationsInRmarkdown.Rmd&quot;&gt;/rmarkdown_utils/CitationsInRmarkdown.Rmd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;get the files from this link &lt;a href=&quot;/rmarkdown_utils/meemodified.csl&quot;&gt;/rmarkdown_utils/meemodified.csl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;get the files from this link &lt;a href=&quot;/rmarkdown_utils/refs.bib&quot;&gt;/rmarkdown_utils/refs.bib&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;NB also that the csl file in petrkeil's repo is an older version from 2012 of the version in the citation-style repo called &lt;code&gt;methods-in-ecology-and-evolution&lt;/code&gt;.  The differences are not large though.&lt;/p&gt;

&lt;h3&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.1.1&lt;/span&gt; Example 1:&lt;/h3&gt;


&lt;p&gt;The journal article by &lt;span class=&quot;citation&quot;&gt;Michener et al. (1997)&lt;/span&gt; and another one &lt;span class=&quot;citation&quot;&gt;(Bodnar &lt;em&gt;et al.&lt;/em&gt; 2004)&lt;/span&gt; appear with their full URL even though I just want their DOI.&lt;/p&gt;


&lt;h3&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.1.2&lt;/span&gt; Example 2:&lt;/h3&gt;


&lt;p&gt;Some recent papers &lt;span class=&quot;citation&quot;&gt;(Open Science Collaboration 2015; Aiken &lt;em&gt;et al.&lt;/em&gt; 2015; Davey &lt;em&gt;et al.&lt;/em&gt; 2015)&lt;/span&gt; don’t have Volume info and I want to say [epub ahead of print].&lt;/p&gt;


&lt;h3&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.1.3&lt;/span&gt; Example 3:&lt;/h3&gt;


&lt;p&gt;This blog post on ‘evidence based data analysis pipeline’ by &lt;span class=&quot;citation&quot;&gt;Peng (2013)&lt;/span&gt; is one that definitely needs the URL and date accessed.&lt;/p&gt;




&lt;h1&gt;References&lt;/h1&gt;


&lt;p&gt;Aiken, A.M., Davey, C., Hargreaves, J.R. &amp;amp; Hayes, R.J. (2015). Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a pure replication. &lt;em&gt;International Journal of Epidemiology&lt;/em&gt;, dyv127. Retrieved from &lt;a href=&quot;http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv127.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv127&quot; class=&quot;uri&quot;&gt;http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv127.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv127&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Bodnar, A., Castorina, R., Desai, M., Duramad, P., Fischer, S., Klepeis, N., Liang, S., Mehta, S., Naumoff, K., Noth, E.M., Schei, M., Tian, L., Vork, K.L. &amp;amp; Smith, K.R. (2004). Lessons learned from ‘the skeptical environmentalist’: an environmental health perspective. &lt;em&gt;International journal of hygiene and environmental health&lt;/em&gt;, &lt;strong&gt;207&lt;/strong&gt;, 57–67. Retrieved from &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S1438463904702643&quot; class=&quot;uri&quot;&gt;http://www.sciencedirect.com/science/article/pii/S1438463904702643&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Davey, C., Aiken, A.M., Hayes, R.J. &amp;amp; Hargreaves, J.R. (2015). Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a statistical replication of a cluster quasi-randomized stepped-wedge trial. &lt;em&gt;International Journal of Epidemiology&lt;/em&gt;, dyv128. Retrieved from &lt;a href=&quot;http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv128.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv128&quot; class=&quot;uri&quot;&gt;http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv128.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv128&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Michener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. &amp;amp; Stafford, S.G. (1997). Nongeospatial metadata for the ecological sciences. &lt;em&gt;Ecological Applications&lt;/em&gt;, &lt;strong&gt;7&lt;/strong&gt;, 330–342. Retrieved from &lt;a href=&quot;http://www.scopus.com/inward/record.url?scp=0030616825\&amp;amp;partnerID=8YFLogxK&quot; class=&quot;uri&quot;&gt;http://www.scopus.com/inward/record.url?scp=0030616825\&amp;amp;partnerID=8YFLogxK&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. &lt;em&gt;Science&lt;/em&gt;, &lt;strong&gt;349&lt;/strong&gt;, aac4716–aac4716. Retrieved from &lt;a href=&quot;http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716&quot; class=&quot;uri&quot;&gt;http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Peng, R.D. (2013). Implementing Evidence-based Data Analysis: Treading a New Path for Reproducible Research. &lt;em&gt;Simply statistics&lt;/em&gt;. Retrieved July 26, 2015, from &lt;a href=&quot;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&quot; class=&quot;uri&quot;&gt;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&lt;/a&gt;&lt;/p&gt;


&lt;pre&gt;&lt;code&gt;# first replace the &amp;lt;macro name=&quot;access&quot;&amp;gt;
# with 
&amp;lt;macro name=&quot;access&quot;&amp;gt;
  &amp;lt;choose&amp;gt;
    &amp;lt;if variable=&quot;DOI&quot;/&amp;gt;
    &amp;lt;!--don't use if there is a DOI--&amp;gt;
    &amp;lt;else&amp;gt;
      &amp;lt;choose&amp;gt;
        &amp;lt;if variable=&quot;URL&quot;&amp;gt;
          &amp;lt;group delimiter=&quot; &quot; prefix=&quot; &quot;&amp;gt;
            &amp;lt;group&amp;gt;
              &amp;lt;text variable=&quot;URL&quot;/&amp;gt;
            &amp;lt;/group&amp;gt;
            &amp;lt;group prefix=&quot;[&quot; suffix=&quot;]&quot; delimiter=&quot; &quot;&amp;gt;
              &amp;lt;date variable=&quot;accessed&quot;&amp;gt;
                &amp;lt;date-part name=&quot;day&quot;/&amp;gt;
                &amp;lt;date-part name=&quot;month&quot; prefix=&quot; &quot; suffix=&quot; &quot; form=&quot;short&quot;/&amp;gt;
                &amp;lt;date-part name=&quot;year&quot;/&amp;gt;
              &amp;lt;/date&amp;gt;
            &amp;lt;/group&amp;gt;
          &amp;lt;/group&amp;gt;
        &amp;lt;/if&amp;gt;
      &amp;lt;/choose&amp;gt;
    &amp;lt;/else&amp;gt;
  &amp;lt;/choose&amp;gt;
&amp;lt;/macro&amp;gt;
#### Then add this ####
&amp;lt;macro name=&quot;date&quot;&amp;gt;
  &amp;lt;choose&amp;gt;
    &amp;lt;if variable=&quot;issued&quot;&amp;gt;
      &amp;lt;choose&amp;gt;
        &amp;lt;if type=&quot;article-journal&quot;&amp;gt;
          &amp;lt;date variable=&quot;issued&quot;&amp;gt;
            &amp;lt;date-part name=&quot;year&quot;/&amp;gt;
          &amp;lt;/date&amp;gt;
        &amp;lt;/if&amp;gt;
        &amp;lt;else&amp;gt;
          &amp;lt;date variable=&quot;issued&quot;&amp;gt;
            &amp;lt;date-part name=&quot;year&quot;/&amp;gt;
          &amp;lt;/date&amp;gt;
        &amp;lt;/else&amp;gt;
      &amp;lt;/choose&amp;gt;
    &amp;lt;/if&amp;gt;
    &amp;lt;else&amp;gt;
      &amp;lt;text term=&quot;no date&quot; prefix=&quot;[&quot; suffix=&quot;]&quot;/&amp;gt;
    &amp;lt;/else&amp;gt;
  &amp;lt;/choose&amp;gt;
&amp;lt;/macro&amp;gt;
#### And add this ####
      &amp;lt;else-if type=&quot;article-journal&quot;&amp;gt;
        &amp;lt;choose&amp;gt;
          &amp;lt;if variable=&quot;issue volume&quot; match=&quot;any&quot;&amp;gt;
            &amp;lt;text macro=&quot;title&quot; suffix=&quot; &quot;/&amp;gt;
            &amp;lt;text variable=&quot;container-title&quot; suffix=&quot; &quot; form=&quot;short&quot; font-style=&quot;italic&quot; strip-periods=&quot;true&quot;/&amp;gt;
            &amp;lt;text variable=&quot;volume&quot;/&amp;gt;
            &amp;lt;text variable=&quot;page&quot; prefix=&quot;: &quot;/&amp;gt;
            &amp;lt;text macro=&quot;date&quot; prefix=&quot;, &quot; suffix=&quot;.&quot;/&amp;gt;
          &amp;lt;/if&amp;gt;
          &amp;lt;else&amp;gt;
            &amp;lt;choose&amp;gt;
              &amp;lt;if variable=&quot;DOI&quot;&amp;gt;
                &amp;lt;text macro=&quot;title&quot; suffix=&quot; &quot;/&amp;gt;
                &amp;lt;text variable=&quot;container-title&quot; suffix=&quot; &quot; form=&quot;short&quot; font-style=&quot;italic&quot;/&amp;gt;
                &amp;lt;group prefix=&quot;(&quot; suffix=&quot;).&quot;&amp;gt;
                  &amp;lt;date variable=&quot;issued&quot;&amp;gt;
                    &amp;lt;date-part name=&quot;month&quot; prefix=&quot; &quot; suffix=&quot; &quot;/&amp;gt;
                    &amp;lt;date-part name=&quot;day&quot; suffix=&quot;, &quot;/&amp;gt;
                    &amp;lt;date-part name=&quot;year&quot;/&amp;gt;
                  &amp;lt;/date&amp;gt;
                &amp;lt;/group&amp;gt;
                &amp;lt;text variable=&quot;DOI&quot; prefix=&quot; doi: &quot;/&amp;gt;
              &amp;lt;/if&amp;gt;
              &amp;lt;else&amp;gt;
                &amp;lt;text variable=&quot;container-title&quot; suffix=&quot;. &quot; form=&quot;short&quot; font-style=&quot;italic&quot;/&amp;gt;
              &amp;lt;/else&amp;gt;
            &amp;lt;/choose&amp;gt;
          &amp;lt;/else&amp;gt;
        &amp;lt;/choose&amp;gt;
      &amp;lt;/else-if&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Now use this modified csl in the header of the RMD file instead of mee.csl&lt;/h2&gt;

&lt;h1&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.2&lt;/span&gt; NEW References&lt;/h1&gt;


&lt;p&gt;Aiken, A.M., Davey, C., Hargreaves, J.R. &amp;amp; Hayes, R.J. (2015).Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a pure replication &lt;em&gt;International Journal of Epidemiology&lt;/em&gt; (epub ahead of print July 2015). doi: &lt;a href=&quot;http://dx.doi.org/10.1093/ije/dyv127&quot;&gt;10.1093/ije/dyv127&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Bodnar, A., Castorina, R., Desai, M., Duramad, P., Fischer, S., Klepeis, N., Liang, S., Mehta, S., Naumoff, K., Noth, E.M., Schei, M., Tian, L., Vork, K.L. &amp;amp; Smith, K.R. (2004).Lessons learned from ‘the skeptical environmentalist’: an environmental health perspective. &lt;em&gt;International journal of hygiene and environmental health&lt;/em&gt; 207: 57–67, 2004.&lt;/p&gt;


&lt;p&gt;Davey, C., Aiken, A.M., Hayes, R.J. &amp;amp; Hargreaves, J.R. (2015).Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a statistical replication of a cluster quasi-randomized stepped-wedge trial &lt;em&gt;International Journal of Epidemiology&lt;/em&gt; (epub ahead of print July 2015). doi: &lt;a href=&quot;http://dx.doi.org/10.1093/ije/dyv128&quot;&gt;10.1093/ije/dyv128&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Michener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. &amp;amp; Stafford, S.G. (1997).Nongeospatial metadata for the ecological sciences &lt;em&gt;Ecological Applications&lt;/em&gt; 7: 330–342, 1997. &lt;a href=&quot;http://www.scopus.com/inward/record.url?scp=0030616825\&amp;amp;partnerID=8YFLogxK&quot;&gt;http://www.scopus.com/inward/record.url?scp=0030616825\&amp;amp;partnerID=8YFLogxK&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Open Science Collaboration. (2015).Estimating the reproducibility of psychological science &lt;em&gt;Science&lt;/em&gt; 349: aac4716–aac4716, 2015.&lt;/p&gt;


&lt;p&gt;Peng, R.D. (2013). Implementing Evidence-based Data Analysis: Treading a New Path for Reproducible Research. &lt;em&gt;Simply statistics&lt;/em&gt;. &lt;a href=&quot;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&quot;&gt;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&lt;/a&gt; [26 Jul. 2015]&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>A quick review of a quick guide to organizing computational biology projects</title>
   <link href="http://ivanhanigan.github.com/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/"/>
   <updated>2015-10-07T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects</id>
   <content type="html">&lt;p&gt;The organisation of material is a particularly vexatious topic.  For a data analysis project it is very important that the set of folders and files is logical and intuitive, as well as being well documented. The oft-heard exhortation by computer scientists to their users to 'Read The F-ing Manual' (RTFM) is perennial and rooted in the fundamental difficulty of readers to have the time required to read and digest the detailed information there-in.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In this post I review a paper that I was referred to by recent activity on the
Mozillascience studyGroupLesson 'Open Science Utility Belt':
&lt;a href=&quot;https://github.com/mozillascience/studyGroupLessons/issues/7&quot;&gt;https://github.com/mozillascience/studyGroupLessons/issues/7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;That group also holds a journal club &lt;a href=&quot;https://github.com/minisciencegirl/studyGroup/issues/20#issuecomment-134750483&quot;&gt;https://github.com/minisciencegirl/studyGroup/issues/20#issuecomment-134750483&lt;/a&gt; and they reviewed this paper:&lt;/li&gt;
&lt;li&gt;Noble, W. S. (2009). A quick guide to organizing computational biology projects. PLoS Computational Biology, 5(7), 1–5. &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1000424&quot;&gt;http://dx.doi.org/10.1371/journal.pcbi.1000424&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I missed out so I thought I'd put my notes up here for reference:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;core guiding principle is simple: Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why&lt;/li&gt;
&lt;li&gt;your future self may find it difficult to understand your current work.&lt;/li&gt;
&lt;li&gt;Noble's law: 'Everything you do, you will probably have to do over again'&lt;/li&gt;
&lt;li&gt;store all of the files relevant to one project in common root directory&lt;/li&gt;
&lt;li&gt;The exception to this rule is data/code that are used in multiple projects, they are standalone projects&lt;/li&gt;
&lt;li&gt;Within a given project, use a top-level organization that is logical first, then chronological  at the next level, and then logical organization next&lt;/li&gt;
&lt;li&gt;Core folders are &lt;code&gt;data&lt;/code&gt;, &lt;code&gt;results&lt;/code&gt;, &lt;code&gt;doc&lt;/code&gt; (versus Berndt Weiss' &lt;code&gt;dat&lt;/code&gt;, &lt;code&gt;ana&lt;/code&gt;, &lt;code&gt;doc&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Chronological order?  'tempting to apply a similar, logical organization... this approach is risky, because the logical structure of your final set of experiments may look drastically different from the form you initially designed. This is particularly true under the results directory, where you may not even know in advance what kinds of experiments you will need to perform'&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Recommended folder and file structures &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1000424.g001&quot;&gt;http://dx.doi.org/10.1371/journal.pcbi.1000424.g001&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;/projectname (eg msms)/
    /doc/
        /ms-analysis.html 
        /paper/
            /msms.tex
            /msms.pdf
    /data/
        /YYYY-MM-DD/
            /yeast/
                /README
                /yeast.sqt
            /worm/
                /README
                /worm.sqt
    /src/
        /ms-analysis.c
    /bin/
        /parse-sqt.py
    /results/
        /notebook.html 
        /YYYY-MM-DD-1/
            /runall
            /split1/
            /split2/
        /YYYY-MM-DD-2/
            /runall
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Use a driver script to automate creation of a directory structure&lt;/li&gt;
&lt;li&gt;maintain a chronologically organized lab notebook (I have been calling this a work 'log' sensu Scott Long's 2008 'Workflow book')&lt;/li&gt;
&lt;li&gt;create either a README file, or a command line driver script (he calls this &lt;code&gt;runall&lt;/code&gt;, but see also &lt;code&gt;main.R&lt;/code&gt; sensu the Reichian LCFD model)&lt;/li&gt;
&lt;li&gt;you should end up with a file that is parallel to the lab notebook entry. The lab notebook contains a prose description of the exper- iment, whereas the driver script contains all the gory details&lt;/li&gt;
&lt;li&gt;Version Control.  'Nuff said! But how to build capacity with Github when all my colleagues seem so confused by it?&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Naming Conventions For Computer Files</title>
   <link href="http://ivanhanigan.github.com/2015/10/naming-conventions-for-computer-files/"/>
   <updated>2015-10-05T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/naming-conventions-for-computer-files</id>
   <content type="html">&lt;p&gt;I'm working on a new Data Management Plan for a research group who
merge data from air pollution, meteorology, population census and
health outcome datasets.  The folder organisation is pretty much under
control now, but the file names are challenging.&lt;/p&gt;

&lt;p&gt;I'm searching for inspiration and was recommended this conversation:
&lt;a href=&quot;https://github.com/minisciencegirl/studyGroup/issues/20&quot;&gt;https://github.com/minisciencegirl/studyGroup/issues/20&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;File Organization and Naming:
...
I want to include details of what the settings were or what dataset I
started out with. Rather than saving a file name a mile long
&quot;FlightHomLog2_av1_Euc_ArrayEucl_AvgLink&quot;, there must be many
different better ways to organizing my file space.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;There is a bunch of good advice already here, and I recommend the slides &lt;a href=&quot;https://github.com/Reproducible-Science-Curriculum/rr-organization1/tree/master/slides/naming-slides&quot;&gt;https://github.com/Reproducible-Science-Curriculum/rr-organization1/tree/master/slides/naming-slides&lt;/a&gt; and the two PLOS articles, but I wanted to pull out the two things I think it is important to get right:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ordering in lists&lt;/li&gt;
&lt;li&gt;substring chunks that can be extracted&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think that the substring chunks are explained well in the slides link above (summary, use '_' or '-' to split the string), but I think that the ordering problem needs some thinking.&lt;/p&gt;

&lt;h2&gt;Tidy Data:&lt;/h2&gt;

&lt;p&gt;This all reminds me of words from Hadley Wickham about tidy data, and the order that columns should be arranged in tabular data.  The principles are similar I think.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;A good ordering makes it easier to scan the raw values. One way of
organizing variables is by their role in the analysis: are values
fixed by the design of the data collection, or are they measured
during the course of the experiment? Fixed variables describe the
experimental design and are known in advance. Computer scientists
often call fixed variables dimensions, and statisticians usually
denote them with subscripts on random variables. Measured variables
are what we actually measure in the study. Fixed variables should come
first, followed by measured variables, each ordered so that related
variables are contiguous. Rows can then be ordered by the first
variable, breaking ties with the second and subsequent (fixed)
variables. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Wickham, H. (2014). Tidy Data. JSS Journal of Statistical Software, 59(10).
Retrieved from &lt;a href=&quot;http://www.jstatsoft.org/&quot;&gt;http://www.jstatsoft.org/&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;One way that we did this:&lt;/h2&gt;

&lt;p&gt;Colleagues and I came up with the following protocol for an ecology and biodiversity database&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Project name (optional sub-project name)&lt;/li&gt;
&lt;li&gt;Data type (such as experimental unit, observational unit, and/or measurement methods)&lt;/li&gt;
&lt;li&gt;Geographic location (State, Country)&lt;/li&gt;
&lt;li&gt;Temporal frequency and coverge Annual or seasonal tranches&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Tidy data generalisable concepts are dimensions and variables&lt;/h2&gt;

&lt;p&gt;The concept of dimensions and variables can be useful here, and especially for deciding on filenames.  Dimensions are fixed or change slowly while variables change more quickly .  For example the project name is 'fixed', that is it does not change across the files, but the sub-project name does change, just more slowly (say there may be 2-3 different sub-projects within a project). Then there may be a set of data types, and these 'change' more quickly than the sub-project name (by change I mean, there are more of them).  Then the geographic and temporal variables might change quickest of all.&lt;/p&gt;

&lt;p&gt;So a general rule for the order of things can be stated
The more fixed variables should come first (those things that don't change, or don't change much),
followed by the more fluid variables (or things that change more across the project).
List elements can then be ordered so that the groups of things that are similar will always be contiguous, and vary sequentially within clusters.&lt;/p&gt;

&lt;p&gt;Perhaps an example would be easier to understand.  Here is a set of file names that we constructed for one of our ecological field sites (project) and plots (sub-project or measurement location):&lt;/p&gt;

&lt;h4&gt;Notice we also had a controlled vocabulary of data types and their acronyms before starting this&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;| Filename                                                            | Title                                                                                                                                 |
|---------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------|
| asn_fnqr_soil_charact_robson_2011.csv                               | Soil Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                                                              |
| asn_fnqr_soil_pit_robson_2012.csv                                   | Soil Pit Data, Water Content and Temperature, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                           |
| asn_fnqr_veg_seedling_robson_2010-2012.csv                          | Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                  |
| asn_fnqr_veg_seedling_transect_coord_robson_2010-2012.csv           | Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                  |
| asn_fnqr_core_1ha_robson_2014.csv                                   | Soil Pit Data, Soil Characterisation, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha plot, 2014                   |
| asn_fnqr_fauna_biodiversity_ctbcc_2012.csv                          | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2012                                      |
| asn_fnqr_fauna_biodiversity_ctbcc_2013.csv                          | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2013                                      |
| asn_fnqr_fauna_biodiversity_ctbcc_capetrib_2014.csv                 | Avifauna Monitoring, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2014                                                |
| asn_fnqr_fauna_biodiversity_ctbcc_lu11a_2014.csv                    | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2014                               |
| asn_fnqr_fauna_biodiversity_ctbcc_lu7a_2014.csv                     | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2014                                |
| asn_fnqr_fauna_biodiversity_ctbcc_lu7b_2014.csv                     | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7B, 2014                                |
| asn_fnqr_fauna_biodiversity_ctbcc_lu9a_2014.csv                     | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2014                                |
| asn_fnqr_fauna_biodiversity_ctbcc-lu11a_2009-2011.csv               | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                          |
| asn_fnqr_fauna_biodiversity_ctbcc-lu7a_2009-2011.csv                | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2009-2011                           |
| asn_fnqr_fauna_biodiversity_ctbcc-lu9a_2009-2011.csv                | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2009-2011                           |
| asn_fnqr_fauna_biodiversity_habitat codes_ctbcc-lu11a_2009-2011.pdf | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                          |
| asn_fnqr_fauna_biodiversity_habitat codes_ctbcc-lu9a_2009-2011.pdf  | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2009-2011                           |
| asn_fnqr_fauna_biodiversity_habitat_codes_ctbcc-lu7a_2009-2011.pdf  | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2009-2011                           |
| asn_fnqr_fauna_birds_capture_robson_2011-2014.csv                   | Bird Capture Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011-2014                                                 |
| asn_fnqr_fauna_birds_robson_2010-2014.csv                           | Bird Survey Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2014                                                  |
| asn_fnqr_fauna_invert_moth_robson_2009.csv                          | Moth Inventory at Canopy and Ground Level, Far North Queensland Rainforest SuperSite, Robson Creek, 2009                              |
| asn_fnqr_fauna_invert_moth_robson_2010.csv                          | Moth Inventory at Canopy and Ground Level, Far North Queensland Rainforest SuperSite, Robson Creek, 2010                              |
| asn_fnqr_fauna_invert_moth_robson_2011.csv                          | Moth Inventory at Canopy and Ground Level, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                              |
| asn_fnqr_fauna_invert_robson_25ha_2013                              | Invertebrate Fauna Survey, Far North Queensland Rainforest SuperSite, Robson Creek, 25 Ha Plot, 2013                                  |
| asn_fnqr_geo_tracks_100m_grid_robson_2010-2013.kml                  | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_geo_tracks_robson_2010-2013.kml                            | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_geo_tracks_robson_2010-2013.mdb                            | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_geo_tracks_trees_robson_2010-2013.kml                      | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_soil_cosmos_robson_2011.csv                                | Soil Sampling for Calibration of Cosmic Ray Soil Moisture Sensor, Far North Queensland Rainforest SuperSite, Robson Creek, 2011       |
| asn_fnqr_soil_pit_robson_2012.csv                                   | Soil Pit Data, Soil Characterisation, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                                   |
| asn_fnqr_soil_pit_robson_2013.csv                                   | Soil Pit Data, Water Content and Temperature, Far North Queensland Rainforest SuperSite, Robson Creek, 2013                           |
| asn_fnqr_soil_properties_ddc_2013.csv                               | Soil Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2013                                                 |
| asn_fnqr_soil_properties_ddc_2014.csv                               | Soil Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2014                                                 |
| asn_fnqr_soil_properties_robson_2014.csv                            | Soil Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2014                                                              |
| asn_fnqr_stream_chem_robson_201310.csv                              | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201311                                          |
| asn_fnqr_stream_chem_robson_201310-201405.csv                       | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201405                                          |
| asn_fnqr_stream_chem_robson_201311.csv                              | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201311                                          |
| asn_fnqr_stream_chem_std_methods_robson_2013.pdf                    | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201311                                          |
| asn_fnqr_stream_phys-chem_diagram_robson_2013.pdf                   | Stream Physico-Chemical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 201304-201305                                 |
| asn_fnqr_stream-phys-chem_robson_201304-201305.csv                  | Stream Physico-Chemical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 201304-201305                                 |
| asn_fnqr_veg_cwd_robson_core_1ha_2012.csv                           | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_dbh-h_capetrib_crane_plot_2001.csv                     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2001                               |
| asn_fnqr_veg_dbh-h_capetrib_crane_plot_2005.csv                     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2005                               |
| asn_fnqr_veg_dbh-h_capetrib_crane_plot_2010.csv                     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2010                               |
| asn_fnqr_veg_dbh-h_robson_25ha_2009-2015.csv                        | Vascular Plant Data ÔëÑ 10 cm DBH,  Far North Queensland Rainforest SuperSite, Robson Creek, 25 ha Plot, 2009-2015                    |
| asn_fnqr_veg_fruit_robson_25ha_2011-2015.csv                        | Fruit Phenology, Far North Queensland Rainforest SuperSite, Robson Creek, 25 ha Plot, 2011-2015                                       |
| asn_fnqr_veg_gentry_mid-stratum_robson_core_1ha_2012.csv            | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_gentry_sub-stratum_herbs_robson_core_1ha_2012.csv      | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_gentry_sub-stratum_shrubs_robson_core_1ha_2012.csv     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_lai_robson_core_1ha_2012.pdf                           | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_leaf_phys_capetrib_20120831.csv                        | Leaf Level Physiology, Chemistry and Structural Traits, Far North Queensland Rainforest SuperSite, Cape Tribulation, Crane Site, 2012 |
| asn_fnqr_veg_leaf_phys_master_aci_curves_robson_2012.csv            | Leaf Level Physiology, Chemistry and Structural Traits, Far North Queensland SuperSite, Robson Creek, 2012                            |
|                                                                     |                                                                                                                                       |
| asn_fnqr_veg_leaf_phys_master_ai_curves_robson_2012.csv             | Leaf Level Physiology, Chemistry and Structural Traits, Far North Queensland SuperSite, Robson Creek, 2012                            |
|                                                                     |                                                                                                                                       |
| asn_fnqr_veg_seedling_species_robson_2010-2012.csv                  | Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                  |
| asn_fnqr_veg_species_capetrib_crane_plot_2001.csv                   | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2001                               |
| asn_fnqr_veg_species_capetrib_crane_plot_2005.csv                   | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2005                               |
| asn_fnqr_veg_species_capetrib_crane_plot_2010.csv                   | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2010                               |
| asn_fnqr_veg_species_robson_2012.csv                                | Vegetation Species List, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                                                |
| asn_fnqr_veg_struct_robson_core_1ha_2012.csv                        | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_centre_images_2012.zip          | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_centre_images_2014.zip          | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_ne_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_ne_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_nw_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_nw_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_se_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_se_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_sw_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_sw_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_structure_robson_core_1 ha.csv                         | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_vascular plant list_robson_core_1 ha_2012.csv          | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_water_properties_robson_2013.csv                           | Stream Physico-Chemical Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2013                                           |
| asn_fnqr_water_properties_robson_2014.csv                           | Stream Physico-Chemical Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2014                                           |
| asn_fnqr_weather_capetrib_2006.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2006                                               |
| asn_fnqr_weather_capetrib_2007.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2007                                               |
| asn_fnqr_weather_capetrib_2008.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2008                                               |
| asn_fnqr_weather_capetrib_2009.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2009                                               |
| asn_fnqr_weather_capetrib_2010.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2010                                               |
| asn_fnqr_weather_capetrib_2011.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2011                                               |
| asn_fnqr_weather_capetrib_2012.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2012                                               |
| asn_fnqr_weather_capetrib_2013.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2013                                               |
| asn_fnqr_weather_capetrib_2014.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2014                                               |
| asn_fnqr_weather_ddc_2008.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2008                                      |
| asn_fnqr_weather_ddc_2009.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2009                                      |
| asn_fnqr_weather_ddc_2010.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2010                                      |
| asn_fnqr_weather_ddc_2011.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2011                                      |
| asn_fnqr_weather_ddc_2012.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2012                                      |
| asn_fnqr_weather_ddc_2013.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2013                                      |
| asn_fnqr_weather_robson_2010.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2010                                                   |
| asn_fnqr_weather_robson_2011.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                                                   |
| asn_fnqr_weather_robson_2012.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                                                   |
| asn_fnqr_weather_robson_2013.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2013                                                   |
| asn_fnqr_weather_robson_2014.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2014                                                   |
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Open Notebook Blogging Vs Twitter? Or Can I Do Both?</title>
   <link href="http://ivanhanigan.github.com/2015/10/open-notebook-blogging-vs-twitter-or-can-i-do-both/"/>
   <updated>2015-10-04T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/open-notebook-blogging-vs-twitter-or-can-i-do-both</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;I was tooling around someone's blog and noticed a link to this interesting talk &quot;Citation &amp;amp; Productivity Benefits from Open Science&quot; by &lt;a href=&quot;https://github.com/BillMills&quot;&gt;https://github.com/BillMills&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the slides markdown is here &lt;a href=&quot;https://github.com/BillMills/practicalOpenScience/blob/gh-pages/outline.md&quot;&gt;https://github.com/BillMills/practicalOpenScience/blob/gh-pages/outline.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the slide on &quot;Open Communication&quot; interested me, as I have turned up my open notebook blog to 11 recently with a couple of posts a week for the last few weeks&lt;/li&gt;
&lt;li&gt;the recommendation to &quot;Blog Early And Blog Often&quot; resonated here!&lt;/li&gt;
&lt;li&gt;I was intrigued by the comments&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;
used well, twitter can be a useful tool for frequent communication
paradigms as a distributor and aggregator of links to the content in
the other three bullet points. It can be really tough to stay on top
of everyone's blog, everyone's issue tracker and everything else; by
pushing links to our followers every time we have a new RFC out and
vice versa, we greatly simplify this process. See this example.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;the other three being: GitHub Issues/Working Open, Blogging/Journals of Brief Ideas and
Study Preregistration/'publication bias'&lt;/li&gt;
&lt;li&gt;RFC means 'Request For Comments'&lt;/li&gt;
&lt;li&gt;and so I clicked on the link to this example &lt;a href=&quot;https://twitter.com/MozillaScience/status/628990222651428864&quot;&gt;https://twitter.com/MozillaScience/status/628990222651428864&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/twits.png&quot; alt=&quot;/images/twits.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Warning, Danger&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This seemed like a fascinating discussion, one that I have been thinking about a lot&lt;/li&gt;
&lt;li&gt;and writing up here &lt;a href=&quot;https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets&quot;&gt;https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets&lt;/a&gt; in my thesis BUT...&lt;/li&gt;
&lt;li&gt;I also recently received this warning from a blogger I admire: &lt;a href=&quot;http://charliepark.org/stepping-away-from-twitter/&quot;&gt;http://charliepark.org/stepping-away-from-twitter/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;He warns strongly that people with my temperament for seeking feedback and approval may be hampered by using twitter too much:&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;Stepping Away From Twitter
June 12, 2015
I recently noticed an interesting pattern.
On days when I’m on Twitter, my ability to focus and 
get good work done falls off a cliff.

It’s not just “when you read Twitter first-thing in the day”, 
something I’ve heard people discuss. 
It was that I was on Twitter at all.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;But there are clear benefits right?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;So I had to take a step back from my recent explorations with linking up my social media and blogging interests&lt;/li&gt;
&lt;li&gt;Looking over what I am trying to achieve by keeping regular posts of my work, along with the need I feel to make sure people who are interested can &lt;em&gt;find&lt;/em&gt; my work, I began to despair&lt;/li&gt;
&lt;li&gt;but then I kept digging around and see a lot of people I admire linking up twitter and more scientific communications&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;So what?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I guess the real point of this post is to admit that I don't know how to use twitter&lt;/li&gt;
&lt;li&gt;I'm happy to report I feel comfortable now (a couple of years in) making regular open notebook entries to communicate what is going on in my 'Lonely Analyst' lab &lt;a href=&quot;http://simplystatistics.org/2013/08/09/embarrassing-typos-reveal-the-dangers-of-the-lonely-data-analyst/&quot;&gt;http://simplystatistics.org/2013/08/09/embarrassing-typos-reveal-the-dangers-of-the-lonely-data-analyst/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;but a couple of recent emails asking me for data that I published years ago &lt;a href=&quot;https://gislibrary-extreme-weather.anu.edu.au/poa_weather&quot;&gt;like this Australian Postcode Area Weather Estimates stuff&lt;/a&gt; made me think I really need to get my stuff exposed so it is &lt;strong&gt;LOT&lt;/strong&gt; more visible.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Complexity of Graphs Obfuscates, but Visual Grouping Helps Disentangle Things</title>
   <link href="http://ivanhanigan.github.com/2015/10/complexity-of-graphs-obfuscates-but-visual-grouping-helps-disentangle-things/"/>
   <updated>2015-10-03T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/complexity-of-graphs-obfuscates-but-visual-grouping-helps-disentangle-things</id>
   <content type="html">&lt;p&gt;Using diagrams has long been a technique used in science to describe the relationships (edges) between things (nodes).  Mathematics and geometry tools have been applied to ameliorate the problem of laying out the diagram for the most efficient use of space.  It is desirable to minimise the gaps between the nodes and also to ensure that lines do not overlap too much.  This is because the complexity of graphs obfuscates the details that we are trying to show. Visual grouping helps to disentangle the relationships.&lt;/p&gt;

&lt;p&gt;As an example, the relationship between drought and suicide is a complex system
where the effects are indirect. The focus is on a chain
of intermediary causal factors. These questions are usually explored
in the context of many other factors that describe human biological
variables and the socio-economic milieu.&lt;/p&gt;

&lt;p&gt;In this post I utilise the R package &lt;code&gt;DiagrammeR&lt;/code&gt; to construct a causal directed acyclic graph (DAG) of the putative effects of a set of selected causal factors from both natural and social capital theories.&lt;/p&gt;

&lt;p&gt;The following code produces graph
&lt;img src=&quot;/images/suicide-drought.png&quot; alt=&quot;/images/suicide-drought.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Or this interactive version &lt;a href=&quot;/viewhtml21f36d8c5d7d/index.html&quot;&gt;/viewhtml21f36d8c5d7d/index.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe style=&quot;border: none;&quot; height=&quot;400&quot; width=&quot;600&quot; src=&quot;/viewhtml21f36d8c5d7d/index.html&quot;&gt;&lt;/iframe&gt;


&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;library(DiagrammeR)
#### First create the outcome
nodes_outcome &amp;lt;- create_nodes(nodes = c('suicide','depression','anxiety'),
                        label = TRUE,
                        colour = &quot;black&quot;)

edges_outcome &amp;lt;- create_edges(from = c('depression','anxiety'),
                        to =   c(&quot;suicide&quot;, &quot;suicide&quot;)
                        )

graph_outcome &amp;lt;- create_graph(nodes_df = nodes_outcome,
                       edges_df = edges_outcome)
# just test this out
## render_graph(graph_outcome)

#### now the social capital factors  
nodes_social &amp;lt;- 
  create_nodes(nodes =  c(&quot;stress&quot;, &quot;decreased community support&quot;, &quot;migration&quot;),
               label = TRUE,
               color = &quot;blue&quot;)

edges_social &amp;lt;- create_edges(from =  c(&quot;stress&quot;, &quot;stress&quot;, &quot;decreased community support&quot;,
                          &quot;migration&quot;),
                        to =   c(&quot;anxiety&quot;, &quot;depression&quot;, &quot;anxiety&quot;,
                          &quot;decreased community support&quot;)
                        )
graph_social &amp;lt;- create_graph(nodes_df = nodes_social,
                       edges_df = edges_social)
# render_graph(graph_social)

#### now the financial capital factors
nodes_financial &amp;lt;- 
  create_nodes(nodes = c(&quot;employment&quot;, &quot;debt&quot;),
               label = TRUE,
               color = &quot;green&quot;)

edges_financial &amp;lt;- create_edges(from = c(&quot;employment&quot;, &quot;employment&quot;, &quot;debt&quot;),
                        to =   c(&quot;stress&quot;, &quot;debt&quot;, &quot;stress&quot;)
                        )
graph_financial &amp;lt;- create_graph(nodes_df = nodes_financial,
                       edges_df = edges_financial)
# render_graph(graph_financial)


#### now the natural capital factors
nodes_natural &amp;lt;- 
  create_nodes(nodes = c(&quot;drought&quot;, &quot;declined agricultural productivity&quot;, &quot;decreased food security&quot;),
               label = TRUE,
               color = &quot;red&quot;)

edges_natural &amp;lt;- create_edges(from = c(&quot;drought&quot;, &quot;drought&quot;, &quot;declined agricultural productivity&quot;,
                          &quot;declined agricultural productivity&quot;, &quot;declined agricultural productivity&quot;,
                          &quot;decreased food security&quot;,
                          &quot;drought&quot;),
                        to =   c(&quot;declined agricultural productivity&quot;, &quot;decreased food security&quot;, &quot;decreased food security&quot;,
                          &quot;anxiety&quot;, &quot;employment&quot;,
                          &quot;anxiety&quot;,
                          &quot;migration&quot;)
                        )
graph_natural &amp;lt;- create_graph(nodes_df = nodes_natural,
                       edges_df = edges_natural)
## render_graph(graph_natural)

# use create_graph on separate nodes and edges data frames, one for each cluster
# then access the dot codes for each
gr0 &amp;lt;- graph_outcome$dot_code
gr1 &amp;lt;- graph_social$dot_code
gr2 &amp;lt;- graph_financial$dot_code
gr3 &amp;lt;- graph_natural$dot_code
# then replace the graph with subgraph
gr0 &amp;lt;- gsub(&quot;digraph&quot;, &quot;subgraph cluster0&quot;, gr0)
gr1 &amp;lt;- gsub(&quot;digraph&quot;, &quot;subgraph cluster1&quot;, gr1)
gr2 &amp;lt;- gsub(&quot;digraph&quot;, &quot;subgraph cluster2&quot;, gr2)
gr3 &amp;lt;- gsub(&quot;digraph&quot;, &quot;subgraph cluster3&quot;, gr3)
# and then combine the subgraphs into one graph
gr_out &amp;lt;- sprintf(&quot;digraph{\n%s\n\n %s\n%s\n%s\n}&quot;, gr0, gr1, gr2, gr3)
cat(gr_out)
grViz(gr_out)
# If graphviz is installed and on linux call it with a shell command
sink(&quot;suicide-drought.dot&quot;)
cat(gsub(&quot;'&quot;,'&quot;', gr_out))
sink()
system(&quot;dot -Tpng suicide-drought.dot -o suicide-drought.png&quot;)
# Interactive
nodes &amp;lt;- combine_nodes(nodes_outcome, nodes_social, nodes_natural, nodes_financial)
edges &amp;lt;- combine_edges(edges_outcome, edges_social, edges_natural, edges_financial)

# Render graph
graph &amp;lt;- create_graph(nodes_df = nodes,
                      edges_df = edges)

render_graph(graph, output = &quot;visNetwork&quot;)    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Research Protocol For Manitoba Centre For Health Policy</title>
   <link href="http://ivanhanigan.github.com/2015/10/research-protocol-for-manitoba-centre-for-health-policy/"/>
   <updated>2015-10-02T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/research-protocol-for-manitoba-centre-for-health-policy</id>
   <content type="html">&lt;h4&gt;Version control&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;2013-12-02: This post was originally released
2015-10-02: The URL to the University Manitoba guidelines changed and has been updated.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;When we started a three year project on bushfire smoke we did some planning&lt;/li&gt;
&lt;li&gt;We had a deep discussion about this research management protocol from University Manitoba Centre for Health Policy &lt;a href=&quot;http://umanitoba.ca/faculties/health_sciences/medicine/units/community_health_sciences/departmental_units/mchp/protocol/media/manage_guidelines.pdf&quot;&gt;http://umanitoba.ca/faculties/health_sciences/medicine/units/community_health_sciences/departmental_units/mchp/protocol/media/manage_guidelines.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;That URL has changed several times since I first downloaded the guidelines, so I will also host a copy here &lt;a href=&quot;/pdfs/UManitoba_MCHP_manage_guidelines.pdf&quot;&gt;UManitoba_MCHP_manage_guidelines.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Data Management Plan Checklist&lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 U-Manitoba Centre for Health Policy Guidelines&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Confidentiality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Project Team Makeup&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-1&quot;&gt;1.2.1 Principal Investigator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-2&quot;&gt;1.2.2 Research Coordinator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-3&quot;&gt;1.2.3 The Programmer Coordinator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-4&quot;&gt;1.2.4 Programmer Analyst&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-5&quot;&gt;1.2.5 Research Support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 Project Team considerations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-1&quot;&gt;1.3.1 Roles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-2&quot;&gt;1.3.2 Continuity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-3&quot;&gt;1.3.3 Access levels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-4&quot;&gt;1.3.4 Working group&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-5&quot;&gt;1.3.5 Atmospherics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4&quot;&gt;1.4 File organization and Documentation Development.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-1&quot;&gt;1.4.1 Managing MCHP resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-2&quot;&gt;1.4.2 MCHP directory structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-3&quot;&gt;1.4.3 Managing project files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-4&quot;&gt;1.4.4 Recommended Directories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5&quot;&gt;1.5 Communication&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5-1&quot;&gt;1.5.1 E-mail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5-2&quot;&gt;1.5.2 Meetings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6&quot;&gt;1.6 Administrative&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6-1&quot;&gt;1.6.1 Time entry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-7&quot;&gt;1.7 Report preparation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-7-1&quot;&gt;1.7.1 Reliability and Validity Checks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8&quot;&gt;1.8 Project Completion&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-1&quot;&gt;1.8.1 Final Project Meeting.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-2&quot;&gt;1.8.2 Final Documentation Review.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-3&quot;&gt;1.8.3 System Cleanup.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-4&quot;&gt;1.8.4 Integration of new material to institution repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; U-Manitoba Centre for Health Policy Guidelines&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;These guidelines come from:
&lt;/p&gt;
&lt;p&gt;
\noindent &lt;a href=&quot;http://umanitoba.ca/faculties/medicine/units/mchp/protocol/media/manage_guidelines.pdf&quot;&gt;http://umanitoba.ca/faculties/medicine/units/mchp/protocol/media/manage_guidelines.pdf&lt;/a&gt;&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;
Most of the material below is taken verbatim from the original. Unfortunately many of the items described below have links to internal MCHP documents that we cannot access.  Nonetheless the structure of the guidelines provides a useful skeleton to frame our thinking. 
&lt;/p&gt;
&lt;p&gt;
The following areas should be reviewed with project team members near the beginning of the study and throughout the
project as needed:
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Confidentiality
&lt;/li&gt;
&lt;li&gt;Project team
&lt;/li&gt;
&lt;li&gt;File organization and documentation development
&lt;/li&gt;
&lt;li&gt;Communication
&lt;/li&gt;
&lt;li&gt;Administrative
&lt;/li&gt;
&lt;li&gt;Report Preparation
&lt;/li&gt;
&lt;li&gt;Project Completion
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.1&lt;/span&gt; Confidentiality&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-1&quot;&gt;

&lt;p&gt;Maintaining data access
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.2&lt;/span&gt; Project Team Makeup&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-2&quot;&gt;

&lt;p&gt;Roles and contact information should be documented on the project website for the following, where applicable (information may also be included on level of access approved for each team member).
&lt;/p&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.1&lt;/span&gt; Principal Investigator&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-1&quot;&gt;

&lt;p&gt;This is the lead person on the project, who assumes responsibility for delivering the project. The PI makes decisions on project direction and analysis requirements, with input from programmers and the research coordinator (an iterative process). If there is more than one PI (e.g., multi-site studies), overall responsibility for the study needs to be determined, and how the required work will be allocated and coordinated among the co-investigators. Researcher Workgroup website (internal link) 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.2&lt;/span&gt; Research Coordinator&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-2&quot;&gt;

&lt;p&gt;Th RC is always assigned to deliverables and is usually brought in on other types of projects involving multiple sites, investigators and/or programmers. Responsibilities include project documentation, project management (e.g., ensuring that timelines are met, ensuring that project specifications are being followed), and working with both investigator(s) and the Programmer Coordinator throughout the project to coordinate project requirements. 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.3&lt;/span&gt; The Programmer Coordinator&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-3&quot;&gt;

&lt;p&gt;The PC is a central management role who facilitates assignment of programming resources to projects, ensuring the best possible match among programmers and investigators.
Research Coordinator Workgroup website(internal link) 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.4&lt;/span&gt; Programmer Analyst&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-4&quot;&gt;

&lt;p&gt;This is primarily responsible for programming and related programming documentation (such that the purpose of the program and how results were derived can be understood by others). However, a major role may be taken in the analyses of the project as well, and this will characteristically vary with the project.
Programmer Analyst Workgroup website(internal link) 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.5&lt;/span&gt; Research Support&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-5&quot;&gt;

&lt;p&gt;This is primarily responsible for preparing the final product (i.e., the report), including editing and formatting of final graphs and manuscript and using Reference Manager to set up the references. Research support also normally sets up and attends working group meetings. All requests for research support go through the Office Manager.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.3&lt;/span&gt; Project Team considerations&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-3&quot;&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.1&lt;/span&gt; Roles&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-1&quot;&gt;

&lt;p&gt;It is important to clarify everyone's roles at the beginning of the project; for example, whether the investigator routinely expects basic graphs and/or programming logs from the programmer.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.2&lt;/span&gt; Continuity&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-2&quot;&gt;

&lt;p&gt;It is highly desirable to keep the same personnel, from the start of the project, where possible. It can take some time to develop a cohesive working relationship, particularly if work styles are not initially compatible. Furthermore, requesting others to temporarily fill in for team absences is generally best avoided, particularly for programming tasks (unless there is an extended period of absence). The original programmer will know best the potential impact of any changes that may need to be made to programming code.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.3&lt;/span&gt; Access levels&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-3&quot;&gt;

&lt;p&gt;Access to MCHP internal resources (e.g., Windows, Unix) need to be assessed for all team members and set up as appropriate to their roles on the project.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.4&lt;/span&gt; Working group&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-4&quot;&gt;

&lt;p&gt;A WG is always set up for deliverables (and frequently for other projects):
Terms of Reference for working group (internal)
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.5&lt;/span&gt; Atmospherics&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-5&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-4&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.4&lt;/span&gt; File organization and Documentation Development.&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-4&quot;&gt;

&lt;p&gt;All project-related documentation, including key e-mails used to update project methodology, should be saved within the project directory. Resources for directory setup and file development include:
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.1&lt;/span&gt; Managing MCHP resources&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-1&quot;&gt;

&lt;p&gt;This includes various process documents as well as an overview of the documentation process for incorporating research carried out by MCHP into online resources: Documentation Management Guide (internal)
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.2&lt;/span&gt; MCHP directory structure&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-2&quot;&gt;

&lt;p&gt;A detailed outline of how the Windows environment is structured at MCHP
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.3&lt;/span&gt; Managing project files&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-3&quot;&gt;

&lt;p&gt;How files and sub-directories should be organized and named as per the MCHP Guide to Managing Project Files (internal pdf). Information that may be suitable for incorporating into MCHP online resources should be identified; for example, a Concept Development section for subsequent integration of a new concept(s) into the MCHP
Concept Dictionary. The deliverable glossary is another resource typically integrated into the MCHP Glossary.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.4&lt;/span&gt; Recommended Directories&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-4&quot;&gt;

&lt;p&gt;NOTE this is a diversion from the MCHP guidelines.  These recommended directories are from a combination of sources that we have synthesised.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-1&quot;&gt;Background: concise summaries: possibly many documents for main project and any main analyses based on the 1:3:25 paradigm: one page of main messages; a three-page executive summary; 25 pages of detailed findings.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-2&quot;&gt;Proposals: for documents related to grant applications.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-3&quot;&gt;Approvals: for ethics applications.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-4&quot;&gt;Budget: spreadsheets and so-forth.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-5&quot;&gt;Data&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-5-1&quot;&gt;dataset1&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-5-2&quot;&gt;dataset2&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6&quot;&gt;Paper1&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-1&quot;&gt;Data&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-1-1&quot;&gt;merged dataset1 and 2&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2&quot;&gt;Analysis (also see &lt;a href=&quot;http://projecttemplate.net&quot;&gt;http://projecttemplate.net&lt;/a&gt; for a programmer oriented template)&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-1&quot;&gt;exploratory analyses&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-2&quot;&gt;data cleaning&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-3&quot;&gt;main analysis&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-4&quot;&gt;sensitivity analysis&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-5&quot;&gt;data checking&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-6&quot;&gt;model checking&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-7&quot;&gt;internal review&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3&quot;&gt;Document&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-1&quot;&gt;Draft&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-2&quot;&gt;Journal1&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-2-1&quot;&gt;rejected? :-(&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-3&quot;&gt;Journal2&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-3-1&quot;&gt;Response to reviews&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-4&quot;&gt;Versions: folders named by date - dump entire copies of the project at certain milestones/change points&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-5&quot;&gt;Archiving final data with final published paper&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-7&quot;&gt;Papers 2, 3, etc: same structure as paper 1 hopefully the project spawns several papers&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-8&quot;&gt;Communication: details of communication with stakeholders and decision makers&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-9&quot;&gt;Meetings: for organisation and records of meetings&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-10&quot;&gt;Contact details. table contacts lists&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-11&quot;&gt;Completion: checklists to make sure project completion is systematic.  Factor in a critical reflection of lessons learnt.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-12&quot;&gt;References&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-5&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.5&lt;/span&gt; Communication&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-5&quot;&gt;

&lt;p&gt;Project communication should be in written form, wherever possible, to serve as reference for project documentation. Access and confidentiality clearance levels for all involved in the project will determine whether separate communication plans need to be considered for confidential information.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5.1&lt;/span&gt; E-mail&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5-1&quot;&gt;

&lt;p&gt;provides opportunities for feedback/ discussion from everyone and for documenting key project decisions. Responses on any given issue would normally be copied to every project member, with the expectation of receiving feedback within a reasonable period of time - e.g.,a few days). The Research Coordinator should be copied on ALL project correspondence in order to keep the information up to date on the project website.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-5-1-1&quot;&gt;E-mail etiquette (internal)&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5.2&lt;/span&gt; Meetings&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5-2&quot;&gt;

&lt;p&gt;Regularly-scheduled meetings or conference calls should include all project members where possible. Research Coordinators typically arrange project team meetings and take meeting minutes, while Research Support typically arranges the Working Group meetings.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-5-2-1&quot;&gt;Tips for taking notes (internal)&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-5-2-2&quot;&gt;Outlook calendar&lt;br/&gt;
Used for booking rooms, it displays information on room availability and may include schedules of team members.
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-6&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.6&lt;/span&gt; Administrative&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-6&quot;&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-6-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.6.1&lt;/span&gt; Time entry&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-6-1&quot;&gt;

&lt;p&gt;Time spent on projects should be entered by all MCHP employees who are members of the project team.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-6-1-1&quot;&gt;website for time entry (internal)&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-6-1-2&quot;&gt;procedures for time entry (internal)&lt;br/&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-7&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-7&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.7&lt;/span&gt; Report preparation&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-7&quot;&gt;

&lt;p&gt;This includes:
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Policies - e.g., Dissemination of Research Findings
&lt;/li&gt;
&lt;li&gt;Standards - e.g., deliverable production, use of logos, web publishing
&lt;/li&gt;
&lt;li&gt;Guidelines - e.g., producing PDFs, powerpoint, and Reference Manager files
&lt;/li&gt;
&lt;li&gt;Other resources - e.g., e-mail etiquette, technical resources, photos.
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;

&lt;div id=&quot;outline-container-1-7-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-7-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.7.1&lt;/span&gt; Reliability and Validity Checks&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-7-1&quot;&gt;

&lt;p&gt;Making sure the numbers &quot;make sense&quot;. Carrying out these checks requires spelling out who will do which checks.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-7-1-1&quot;&gt;Data Validity Checks&lt;br/&gt;
A variety of things to check for at various stages of the study. Programming can be reviewed, for example, by checking to ensure all programs have used the right exclusions, the correct definitions, etc. , and output has been accurately transferred to graphs, tables, and maps for the report.
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-7-1-2&quot;&gt;Discrepancies between data sources&lt;br/&gt;
In this case it is MCHP and Manitoba Health Reports - an example of cross-checking against another source of data.
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-8&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.8&lt;/span&gt; Project Completion&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-8&quot;&gt;

&lt;p&gt;Several steps need to take place to &quot;finish&quot; the project:
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.1&lt;/span&gt; Final Project Meeting.&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-1&quot;&gt;

&lt;p&gt;Wind-up or debriefing meetings are held shortly after public release of a deliverable. Such meetings provide all team members with an opportunity to communicate what worked/did not work in bringing the project to completion, providing lessons learned for future deliverables.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.2&lt;/span&gt; Final Documentation Review.&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-2&quot;&gt;

&lt;p&gt;Findings from the wind-up meeting should be used to update and finalize the project website (including entering the date of release of report/paper). Both Windows and Unix project directories should be reviewed to ensure that only those SAS programs relevant to project analyses are kept (and well-documented) for future reference. Any related files which may be stored in a user directory should be moved to the project directory.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.3&lt;/span&gt; System Cleanup.&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-3&quot;&gt;

&lt;p&gt;When the project is complete, the Systems Administrator should be informed. Project directories, including program files and output data sets, will be archived to tape or CD. Tape backups are retained for a 5-year period before being destroyed so any project may be restored up to five years after completion.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.4&lt;/span&gt; Integration of new material to institution repository&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-4&quot;&gt;

&lt;p&gt;This is with MCHP resource repository - a general overview of this process is described in General Documentation Process {internal}.
&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reproducible Research Pipelines In Epidemiology</title>
   <link href="http://ivanhanigan.github.com/2015/10/reproducible-research-pipelines-in-epidemiology/"/>
   <updated>2015-10-01T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/10/reproducible-research-pipelines-in-epidemiology</id>
   <content type="html">&lt;p&gt;The scientific questions motivating my work explore the health
effects of environmental changes.  These include droughts, bushfires,
woodsmoke, dust storms, heat waves and local environmental
conditions. The research needed to disentangle health effects of
environmental changes from social factors. Some of the findings were
novel and unexpected. Adequate documentation of the methods was
problematic because of the many steps of data processing and
analysis. Reproducible research pipelines address the problem of
documenting data analyses by distributing data and code with
publications.&lt;/p&gt;

&lt;p&gt;Reproducibility is needed to improve credibility.  It is often
asserted in the literature that much research is not easy to
reproduce. It is not clear what an effective way to implement these
techniques is. The thesis asks how pipelines can be effectively
implemented in epidemiology. It describes methods for reproducible
research pipelines. It also demonstrates several applications of these
methods in environmental epidemiology.&lt;/p&gt;

&lt;p&gt;Environmental epidemiology requires us to study multifactorial
pathogenesis.  All diseases have multiple causal factors. To
understand the many factors affecting health, epidemiologists must
disentangle strands of a web of causal influences. Isolating factors
is difficult and risks being overly reductionist. These determinants
can interact in complex ways. Environmental epidemiologists often
narrow the focus to a single environmental cause and health effect. A
simple example is bushfire smoke and direct effects on
cardio-respiratory disease.  A more complex example is drought and
suicide where the effects are indirect. The focus is on a chain
of intermediary causal factors. These questions are usually explored
in the context of many other factors that describe human biological
variables and the socio-economic milieu.&lt;/p&gt;

&lt;p&gt;While there is greater weight given to evidence from experimental than
observational studies, experiments are difficult in environmental
health.  Analysis of observational data is often used instead. There
are problems inherent in observational studies that pertain to
variables that are confounders and effect modifiers.  Observational
studies face the principal problem of a large number of
inter-relationships between variables. These can confound or modify
effects.  It is vital to a valid analysis and meaningful
interpretation that we include these. It is problematic that
scientists select variables from a multitude of possibilities found in
the literature. Scientists also gather variables from a plethora of
possible data sources. There is a long process of hypothesising, study
design, data collection, cleaning, exploration, decision making,
preparation, data analysis, model building and model checking.  This
process has been described as a vast ‘garden of forking paths’ which
connect steps and decisions the analyst must make, but they could have
made others. These issues might result in mere
correlation interpreted as causation.&lt;/p&gt;

&lt;p&gt;Adequately documenting the methods and results of data analysis helps
safeguard against such mistakes. This thesis proposes that
reproducible research pipelines address the problem of adequate
documentation of data analysis.  This is because they make it easy to
check the methods. Assumptions are easy to challenge and results
verified in new analyses. Reproducible research pipelines extend
traditional research.  They do this by encoding the steps in a
computer ‘scripting’ language and distributing the data and code with
publications.  Traditional research moves through the steps of
hypothesis and design, measured data, analytic data, computational
results (for figures, tables and numerical results), and reports (text
and formatted manuscript).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Best Thing About Reproducibility Is Not Reproducibility, It Is Transparency And Rigour</title>
   <link href="http://ivanhanigan.github.com/2015/09/the-best-thing-about-reproducibility-is-not-reproducibility-but-transparency-and-rigour/"/>
   <updated>2015-09-28T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/the-best-thing-about-reproducibility-is-not-reproducibility-but-transparency-and-rigour</id>
   <content type="html">&lt;p&gt;Adequately documenting the methods and results of data analysis helps
safeguard against errors of execution and
interpretation. My PhD thesis proposes that
reproducible research pipelines address the problem of
adequate documentation of data analysis.&lt;/p&gt;

&lt;p&gt;A graphical view of the reproducible research pipeline concept is shown below.  The ideas  were introduced into epidemiology by Peng et al in 2006, although Peng has more recently been using the terms 'evidence based data analysis pipeline' (Peng 2013) and 'Data Science Pipeline' (Peng 2015). Both terms are useful, but I chose to follow the original phrase.  The graphical version shown below was introduced by Solymos and Feher (2008).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/reproduciblepipeline2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The best thing about reproducible work is not merely the ability to repeatedly arrive at the same result, but that having the organisational structures in place that are required for reproducibility also implicitly will improve the transparency and rigour of the work.  This is because they make
it easy to check the methods. Assumptions are easy
to challenge and results verified in new analyses.&lt;/p&gt;

&lt;p&gt;Reproducible
research pipelines extend traditional research.  They do this by
encoding the steps in a computer ‘scripting’ language and distributing
the data and code with publications.  Traditional research moves
through the steps of hypothesis and design, measured data, analytic
data, computational results (for figures, tables and numerical
results), and reports (text and formatted manuscript).&lt;/p&gt;

&lt;p&gt;This model of the research pipeline sees a new relationship possible between the author and the reader.  They approach the results and understandings of the research from opposite directions. Readers can dig deeper into the research to verify results or conduct similar studies. Reproducibility exists along a spectrum from minimum reproducibility that can be achieved by providing measured or analytic data and the analytic code. More reproducibility is gained by providing processing code necessary to transform original measured data into tidy data for analysis. Full reproducibility would include all stages of the pipeline.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Peng, R.D., Dominici, F. &amp;amp; Zeger, S.L. (2006). Reproducible epidemiologic research. American Journal of Epidemiology, 163, 783–789.  Retrieved from &lt;a href=&quot;http://dx.doi.org/10.1093/aje/kwj093&quot;&gt;http://dx.doi.org/10.1093/aje/kwj093&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Peng, R.D. (2013). Implementing Evidence-based Data Analysis: Treading a New Path for Reproducible Research. Simply statistics. Retrieved from &lt;a href=&quot;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&quot;&gt;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Peng, R.D. (2015). Report Writing for Data Science in R. leanpub. Retrieved from &lt;a href=&quot;https://leanpub.com/reportwriting&quot;&gt;https://leanpub.com/reportwriting&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sólymos, P. &amp;amp; Fehér, Z. (2008). The mefa package: a tool for reproducible data processing in biogeography. International Biogeography Society Newsletter.  Retrieved from &lt;a href=&quot;http://biogeography.blogspot.com.au/2008/04/mefa-package-tool-for-reproducible-data.html&quot;&gt;http://biogeography.blogspot.com.au/2008/04/mefa-package-tool-for-reproducible-data.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Reproducible Research And Managing Digital Assets Part 3 of 3. ProjectTemplate is appropriate for large scale</title>
   <link href="http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets-part-3/"/>
   <updated>2015-09-27T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets-part-3</id>
   <content type="html">&lt;h2&gt;Recap on this series of three posts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&quot;http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets/&quot;&gt;first post&lt;/a&gt; showed the recommended files and folders for a data analysis project from Scott Long&lt;/li&gt;
&lt;li&gt;That recommendation was pretty complex, with a few folders that I felt did not jump out as super-useful&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets-part-2/&quot;&gt;second post&lt;/a&gt; showed a very simple template from the R community called makeProject&lt;/li&gt;
&lt;li&gt;I really like that one as it seems to be the minimum amount of stuff needed to make things work.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;The ProjectTemplate framework&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I have been using John Myles Whites ProjectTemplate R package &lt;a href=&quot;http://projecttemplate.net/&quot;&gt;http://projecttemplate.net/&lt;/a&gt; for ages&lt;/li&gt;
&lt;li&gt;I really like the ease with which I can get up and running a new project&lt;/li&gt;
&lt;li&gt;and the ease with which I can pick up an old project and start adding new work&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Quote from John's first post&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;My inspiration for this approach comes from the rails command from
Ruby on Rails, which initializes a new Rails project with the proper
skeletal structure automatically. Also taken from Rails is
ProjectTemplate’s approach of preferring convention over
configuration: the automatic data and library loading as well as the
automatic testing work out of the box because assumptions are made
about the directory structure and naming conventions that will be used
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/&quot;&gt;http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I dont know anything about RoR but this philosophy works really well for my R programming too&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;if(!require(ProjectTemplate)) install.packages(ProjectTemplate); require(ProjectTemplate)
setwd(&quot;~/projects&quot;)
create.project(&quot;my-project&quot;)
setwd('my-project')
dir()
##  [1] &quot;cache&quot;       &quot;config&quot;      &quot;data&quot;        &quot;diagnostics&quot; &quot;doc&quot;        
##  [6] &quot;graphs&quot;      &quot;lib&quot;         &quot;logs&quot;        &quot;munge&quot;       &quot;profiling&quot;  
## [11] &quot;README&quot;      &quot;reports&quot;     &quot;src&quot;         &quot;tests&quot;       &quot;TODO&quot;   
##### these are very sensible default directories to create a modular
##### analysis workflow.  See the project homepage for descriptions

# now all you need to do whenever you start a new day 
load.project()
# and your workspace will be recreated and any new data automagically analysed in
# the manner you want
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Advanced usage of ProjectTemplate&lt;/h2&gt;

&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;ProjectTemplate Demo&lt;/h1&gt;




&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 The Compendium concept&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 The R code that produced this report&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3&quot;&gt;3 Inititalise R environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-4&quot;&gt;4 ProjectTemplate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-5&quot;&gt;5 Why?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-6&quot;&gt;6 The Reichian load, clean, func, do approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-7&quot;&gt;7 The Peng NMMAPSlite approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-8&quot;&gt;8 Init the project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-9&quot;&gt;9 dir()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-10&quot;&gt;10 The reports directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-11&quot;&gt;11 Do the analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-12&quot;&gt;12 Get the projecttemplate tutorial data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-13&quot;&gt;13 Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-14&quot;&gt;14 Load the analysis data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-15&quot;&gt;15 check the analysis data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-16&quot;&gt;16 Develop munge code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-17&quot;&gt;17 To munge or not to munge?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-18&quot;&gt;18 Cache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-19&quot;&gt;19 Plot first and second letter counts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-20&quot;&gt;20 Do generate plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-21&quot;&gt;21 First letter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-22&quot;&gt;22 Second letter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-23&quot;&gt;23 Report results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-24&quot;&gt;24 Produce final report&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-25&quot;&gt;25 Personalised project management directories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; The Compendium concept&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;\section{The Compendium concept}
My goal is to develop data analysis projects along the lines of the Compendium concept of Gentleman and Temple Lang (2007) \cite{Gentleman2007}.
Compendia are dynamic documents containing text, code and data.
Transformations are applied to the compendium to view its various aspects.
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code Extraction (Tangle): source code
&lt;/li&gt;
&lt;li&gt;Export (Weave): LaTeX, HTML, etc
&lt;/li&gt;
&lt;li&gt;Code Evaluation
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
I'm also following the orgmode technique of Schulte et al (2012) \cite{Schulte}
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;2&lt;/span&gt; The R code that produced this report&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-2&quot;&gt;


&lt;p&gt;
I support the philosophy of Reproducible Research &lt;a href=&quot;http://www.sciencemag.org/content/334/6060/1226.full&quot;&gt;http://www.sciencemag.org/content/334/6060/1226.full&lt;/a&gt;, and where possible I provide data and code in the statistical software R that will allow analyses to be reproduced.  This document is prepared automatically from the associated Emacs Orgmode file.  If you do not have access to the Orgmode file please contact me.
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;cat(&lt;span style=&quot;color: #2aa198;&quot;&gt;'&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; #######################################################################&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## The R code is free software; please cite this paper as the source.  &lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## Copyright 2012, Ivan C Hanigan &lt;a href=&quot;mailto:ivan.hanigan%40gmail.com&quot;&gt;&amp;lt;ivan.hanigan@gmail.com&amp;gt;&lt;/a&gt; &lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## This program is free software; you can redistribute it and/or modify&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## it under the terms of the GNU General Public License as published by&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## the Free Software Foundation; either version 2 of the License, or&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## (at your option) any later version.&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## &lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## This program is distributed in the hope that it will be useful,&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## but WITHOUT ANY WARRANTY; without even the implied warranty of&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## GNU General Public License for more details.&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## Free Software&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; ## 02110-1301, USA&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt; #######################################################################&lt;/span&gt;
&lt;span style=&quot;color: #2aa198;&quot;&gt;'&lt;/span&gt;)
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;3&lt;/span&gt; Inititalise R environment&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-3&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;MAKE SURE YOU HAVE THE CORE LIBS&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt; (!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(ProjectTemplate)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'ProjectTemplate'&lt;/span&gt;, repos=&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://cran.csiro.au'&lt;/span&gt;); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(ProjectTemplate)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt; (!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(lubridate)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'lubridate'&lt;/span&gt;, repos=&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://cran.csiro.au'&lt;/span&gt;); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(lubridate)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt; (!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(reshape)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'reshape'&lt;/span&gt;, repos=&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://cran.csiro.au'&lt;/span&gt;); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(reshape)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt; (!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(plyr)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'plyr'&lt;/span&gt;, repos=&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://cran.csiro.au'&lt;/span&gt;); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(plyr)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt; (!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(ggplot2)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'ggplot2'&lt;/span&gt;, repos=&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://cran.csiro.au'&lt;/span&gt;); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(ggplot2)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(mgcv)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'mgcv'&lt;/span&gt;, repos=&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://cran.csiro.au'&lt;/span&gt;);&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(mgcv);
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(splines)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(NMMAPSlite)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'NMMAPSlite'&lt;/span&gt;, repos=&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://cran.csiro.au'&lt;/span&gt;);&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(NMMAPSlite)
rootdir &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; getwd()  
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-4&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-4&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;4&lt;/span&gt; ProjectTemplate&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-4&quot;&gt;

&lt;p&gt;\section{ProjectTemplate}
This is a simple demo of the R package \emph{ProjectTemplate} &lt;a href=&quot;http://projecttemplate.net/&quot;&gt;http://projecttemplate.net/&lt;/a&gt; which is aimed at standardising the structure and general development of data analysis projects in R. 
A primary aim is to allow analysts to quickly get a project loaded up and ready to:
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;reproduce or 
&lt;/li&gt;
&lt;li&gt;create new data analyses.
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-5&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-5&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;5&lt;/span&gt; Why?&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-5&quot;&gt;

&lt;p&gt;It has been recognised on the R blogosphere that it 
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;is ``meant to handle very complex research projects'' (&lt;a href=&quot;http://bryer.org/2012/maker-an-r-package-for-managing-document-building-and-versioning&quot;&gt;http://bryer.org/2012/maker-an-r-package-for-managing-document-building-and-versioning&lt;/a&gt;) and 
&lt;/li&gt;
&lt;li&gt;is considered as being amongst the best approaches to the workflow for doing data analysis with R (&lt;a href=&quot;http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html&quot;&gt;http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html&lt;/a&gt;)
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-6&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-6&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;6&lt;/span&gt; The Reichian load, clean, func, do approach&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-6&quot;&gt;

&lt;p&gt;\section{The Reichian load, clean, func, do approach}
&lt;/p&gt;
&lt;p&gt;
The already mentioned blog post &lt;a href=&quot;http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html&quot;&gt;http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html&lt;/a&gt; also links to another`best' approach, the: 
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\emph{Reichian load, clean, func, do} approach &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;http://stackoverflow.com/a/1434424&lt;/a&gt;.  
&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;
By Josh Reich.  I've also followed to prepare this demo using the tutorial and data from the package website &lt;a href=&quot;http://projecttemplate.net/getting_started.html&quot;&gt;http://projecttemplate.net/getting_started.html&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-7&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-7&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;7&lt;/span&gt; The Peng NMMAPSlite approach&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-7&quot;&gt;

&lt;p&gt;\section{The Peng NMMAPSlite approach}
The other approach I followed was that of Roger Peng from Johns Hopkins and his NMMAPSlite R package \cite{Peng2004}.  Especially the function
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;readCity(name, collapseAge = &lt;span style=&quot;color: #b58900;&quot;&gt;FALSE&lt;/span&gt;, asDataFrame = &lt;span style=&quot;color: #b58900;&quot;&gt;TRUE&lt;/span&gt;)
&lt;/pre&gt;


&lt;p&gt;
Arguments
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;name  character, abbreviated name of a city
&lt;/li&gt;
&lt;li&gt;collapseAge   logical, should age categories be collapsed?
&lt;/li&gt;
&lt;li&gt;asDataFrame   logical, should a data frame be returned?)
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
Description: Provides remote access to daily mortality, weather, and
        air pollution data from the National Morbidity, Mortality, and
        Air Pollution Study for 108 U.S. cities (1987&amp;ndash;2000); data are
        obtained from the Internet-based Health and Air Pollution
        Surveillance System (iHAPSS)
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-8&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-8&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;8&lt;/span&gt; Init the project&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-8&quot;&gt;

&lt;p&gt;\section{Init the project}
First we want to initialise the project directory.
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;init&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(&lt;span style=&quot;color: #2aa198;&quot;&gt;'ProjectTemplate'&lt;/span&gt;)
create.project(&lt;span style=&quot;color: #2aa198;&quot;&gt;'analysis'&lt;/span&gt;,minimal=&lt;span style=&quot;color: #b58900;&quot;&gt;TRUE&lt;/span&gt;)
&lt;/pre&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-9&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-9&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;9&lt;/span&gt; dir()&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-9&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;init dir&lt;/span&gt;
dir(&lt;span style=&quot;color: #2aa198;&quot;&gt;'analysis'&lt;/span&gt;)
&lt;/pre&gt;


&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;cache&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;config&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;data&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;munge&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;README&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;src&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-10&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-10&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;10&lt;/span&gt; The reports directory&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-10&quot;&gt;

&lt;p&gt;  I've added the reports directory manually and asked the package author if this is generic enough to be in the defaults for 
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;minimal = &lt;span style=&quot;color: #b58900;&quot;&gt;TRUE&lt;/span&gt; 
&lt;/pre&gt;


&lt;p&gt;
I believe it may be as the \emph{Getting Started} guidebook states:
&lt;/p&gt;&lt;blockquote&gt;

&lt;p&gt;`It's meant to contain the sort of written descriptions of the results of your analyses that you'd \textbf{publish in a scientific paper.}
&lt;/p&gt;
&lt;p&gt;
With that report written &amp;hellip;, we've gone through \textbf{the simplest sort of analysis you might run with ProjectTemplate}. 
&lt;/p&gt;
&lt;/blockquote&gt;





&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;init reports&lt;/span&gt;
dir.create(&lt;span style=&quot;color: #2aa198;&quot;&gt;'analysis/reports'&lt;/span&gt;)
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-11&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-11&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;11&lt;/span&gt; Do the analysis&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-11&quot;&gt;

&lt;p&gt;\section{Do the analysis: use load,clean,func,do}
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;this is the start of the analysis, &lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;assumes the init.r file has been run&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(file.exists(&lt;span style=&quot;color: #2aa198;&quot;&gt;'analysis'&lt;/span&gt;)) setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;'analysis'&lt;/span&gt;)  
Sys.Date()
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;keep a track of the dates the analysis is rerun&lt;/span&gt;
getwd()
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;may want to keep a reference of the directory &lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;the project is in so we can track the history &lt;/span&gt;
&lt;/pre&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-12&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-12&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;12&lt;/span&gt; Get the projecttemplate tutorial data&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-12&quot;&gt;

&lt;p&gt;Get the data from &lt;a href=&quot;http://projecttemplate.net/letters.csv.bz2&quot;&gt;http://projecttemplate.net/letters.csv.bz2&lt;/a&gt; (I downloaded on 13-4-2012)
Put it in the data directory for auto loading.
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;analysis get tutorial data&lt;/span&gt;
download.file(&lt;span style=&quot;color: #2aa198;&quot;&gt;'http://projecttemplate.net/letters.csv.bz2'&lt;/span&gt;, 
  destfile = &lt;span style=&quot;color: #2aa198;&quot;&gt;'data/letters.csv.bz2'&lt;/span&gt;, mode = &lt;span style=&quot;color: #2aa198;&quot;&gt;'wb'&lt;/span&gt;)

&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-13&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-13&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;13&lt;/span&gt; Tools&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-13&quot;&gt;

&lt;p&gt;Edit the \emph{config/global.dcf} file to make sure that the load_libraries setting is turned on
&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-14&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-14&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;14&lt;/span&gt; Load the analysis data&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-14&quot;&gt;

&lt;p&gt;#\section{load}
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;analysis load&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(ProjectTemplate)
load.project()
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-15&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-15&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;15&lt;/span&gt; check the analysis data&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-15&quot;&gt;

&lt;p&gt;#\section{clean}
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;tail(letters)
&lt;/pre&gt;


&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;left&quot; /&gt;&lt;col class=&quot;left&quot; /&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;zyryan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;z&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;y&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;zythem&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;z&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;y&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;zythia&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;z&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;y&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;zythum&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;z&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;y&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;zyzomys&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;z&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;y&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;zyzzogeton&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;z&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;y&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-16&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-16&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;16&lt;/span&gt; Develop munge code&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-16&quot;&gt;

&lt;p&gt;#\section{load with processing (munge)}
&lt;/p&gt;
&lt;p&gt;
Edit the \emph{munge/01-A.R} script so that it contains the following two lines of code:
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;For our current analysis, we're interested in the total &lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;number of occurrences of each letter in the first and &lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;second letter positions and not in the words themselves.&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;compute aggregates&lt;/span&gt;
first.letter.counts &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; ddply(letters, c(&lt;span style=&quot;color: #2aa198;&quot;&gt;'FirstLetter'&lt;/span&gt;), 
  nrow)
second.letter.counts &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; ddply(letters, c(&lt;span style=&quot;color: #2aa198;&quot;&gt;'SecondLetter'&lt;/span&gt;), 
  nrow)
&lt;/pre&gt;

&lt;p&gt;
Now if we run with 
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;load.project()
&lt;/pre&gt;


&lt;p&gt;
all munging will happen automatically.  However&amp;hellip;
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-17&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-17&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;17&lt;/span&gt; To munge or not to munge?&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-17&quot;&gt;

&lt;p&gt;As you'll see on the website, once the data munging is completed and outputs cached, load.project() will keep recomputing work over and over.  
The author suggests we manually edit our configuration file.
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt; &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;edit the config file and turn munge on&lt;/span&gt;
 &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;load.project()&lt;/span&gt;
 &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;edit the config file and turn munge off&lt;/span&gt;
 &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;or my preference&lt;/span&gt;
 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;source&lt;/span&gt;(&lt;span style=&quot;color: #2aa198;&quot;&gt;'munge/01-A.r'&lt;/span&gt;)
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;which can be included in our first analysis script&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;but subsequent analysis scripts can just call load.project() &lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;without touching the config file&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-18&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-18&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;18&lt;/span&gt; Cache&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-18&quot;&gt;

&lt;p&gt;Once munging is complete we cache the results
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;cache(&lt;span style=&quot;color: #2aa198;&quot;&gt;'first.letter.counts'&lt;/span&gt;)
cache(&lt;span style=&quot;color: #2aa198;&quot;&gt;'second.letter.counts'&lt;/span&gt;)

&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;And need to keep an eye on the implications for our config file to avoid re-calculating these next time we &lt;/span&gt;

load.project()

&lt;/pre&gt;




&lt;p&gt;
#\section{do}
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-19&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-19&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;19&lt;/span&gt; Plot first and second letter counts&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-19&quot;&gt;

&lt;p&gt;Produce some simple density plots to see the shape of the first and second letter counts. 
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Create \emph{src/generate_plots.R}. Use the src directory to store any analyses that you run. 
&lt;/li&gt;
&lt;li&gt;The convention is that every analysis script starts with load.project() and then goes on to do something original with the data.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-20&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-20&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;20&lt;/span&gt; Do generate plots&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-20&quot;&gt;

&lt;p&gt;Write the first analysis script into a file in \textbf{src}
&lt;/p&gt;


&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(&lt;span style=&quot;color: #2aa198;&quot;&gt;'ProjectTemplate'&lt;/span&gt;)
load.project()
plot1 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot(first.letter.counts, aes(x = V1)) + 
  geom_density()
ggsave(file.path(&lt;span style=&quot;color: #2aa198;&quot;&gt;'reports'&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;'plot1.pdf'&lt;/span&gt;))

plot2 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; ggplot(second.letter.counts, aes(x = V1)) + 
  geom_density()
ggsave(file.path(&lt;span style=&quot;color: #2aa198;&quot;&gt;'reports'&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;'plot2.pdf'&lt;/span&gt;))
dev.off()
&lt;/pre&gt;


&lt;p&gt;
And now run it (I do this from a main `overview' script).
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;source&lt;/span&gt;(&lt;span style=&quot;color: #2aa198;&quot;&gt;'src/generate_plots.r'&lt;/span&gt;)
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-21&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-21&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;21&lt;/span&gt; First letter&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-21&quot;&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-22&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-22&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;22&lt;/span&gt; Second letter&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-22&quot;&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-23&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-23&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;23&lt;/span&gt; Report results&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-23&quot;&gt;

&lt;p&gt;\section{Report results}
We see that both the first and second letter distributions are very skewed. To make a note of this for posterity, we can write up our discovery in a text file that we store in the reports directory.
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;\documentclass[a4paper]{article}
\title{Letters analysis}
\author{Ivan Hanigan}
\begin{document}
\maketitle
blah blah blah
\end{document}


&lt;/pre&gt;





&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-24&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-24&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;24&lt;/span&gt; Produce final report&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-24&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;now run LaTeX on the file in reports/letters.tex&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-25&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-25&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;25&lt;/span&gt; Personalised project management directories&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-25&quot;&gt;


&lt;p&gt;
\section{Personalised project management directories}
&lt;/p&gt;





&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;####&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;init additional directories for project management&lt;/span&gt;
analysisTemplate()
&lt;/pre&gt;


&lt;pre class=&quot;src src-R&quot;&gt;dir()
&lt;/pre&gt;


&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;admin&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;analysis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;data&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;document&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;init.r&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;metadata&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;ProjectTemplateDemo.org&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;references&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;tools&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;versions&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
















&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reproducible Research And Managing Digital Assets Part 2 of 3. makeProject is simple</title>
   <link href="http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets-part-2/"/>
   <updated>2015-09-26T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets-part-2</id>
   <content type="html">&lt;p&gt;This post is about an effective and simple data management framework for analysis projects.
This post introduces Josh Reich's LCFD framework, originally introduced in this answer on the stack overflow website here &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;http://stackoverflow.com/a/1434424&lt;/a&gt;, and encoded into the makeProject R package &lt;a href=&quot;http://cran.r-project.org/web/packages/makeProject/makeProject.pdf&quot;&gt;http://cran.r-project.org/web/packages/makeProject/makeProject.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Literature Review Approach&lt;/h2&gt;

&lt;p&gt;This series of three posts is a summary of some of the most useful advice I have found based on my experience having implemented in my own work.&lt;/p&gt;

&lt;p&gt;This is the second post in a series of three entries regarding some evidence-based best practice approaches I have reviewed.  I have read many website articles and blog posts on a variety of approaches to the organisation of digital assets in a reporoducible research pipeline.
The material I've gathered in my ongoing search and opportunistic readings regarding best practice in this area have been recommended by practitioners which provides some weight of evidence.  In addition I have implemented some aspects of the many techniques and the reproducibility of my own work has improved greatly.&lt;/p&gt;

&lt;h2&gt;Digital Assets Management for Reproducible Research&lt;/h2&gt;

&lt;p&gt;The digital assets in a reproducible research pipeline include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Publication material (documents, figures, tables, literature)&lt;/li&gt;
&lt;li&gt;Data (raw measurements, data provided, data derived)&lt;/li&gt;
&lt;li&gt;Code (pre-processing, analysis and presentation)&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;How to use the &lt;code&gt;makeProject&lt;/code&gt; package&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;makeProject&lt;/code&gt; R package is designed to create a folder and some R scripts that are useful for generic workflow tasks.&lt;/li&gt;
&lt;li&gt;The theory is very similar to the approach described in the previous post about Scott Long's batch script: wfsetupsingle.bat &lt;a href=&quot;https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets&quot;&gt;https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# choose your project dir
setwd(&quot;~/projects&quot;)   
library(makeProject)
makeProject(&quot;makeProjectDemo&quot;)
#returns
&quot;Creating Directories ...
Creating Code Files ...
Complete ...&quot;
matrix(dir(&quot;makeProjectDemo&quot;))
#[1,] &quot;code&quot;       
#[2,] &quot;data&quot;       
#[3,] &quot;DESCRIPTION&quot;
#[4,] &quot;main.R&quot;     
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;This has set up some simple and sensible tools for a data analysis.&lt;/li&gt;
&lt;li&gt;Let's have a look at the &lt;code&gt;main.R&lt;/code&gt; script. This is the one file that is used to run all the modules of the project, found in the R scripts in the &lt;code&gt;code&lt;/code&gt; folder.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Project: makeProjectDemo
# Author: Your Name
# Maintainer: Who to complain to &amp;lt;yourfault@somewhere.net&amp;gt;

# This is the main file for the project
# It should do very little except call the other files

### Set the working directory
setwd(&quot;/home/ivan_hanigan/projects/makeProjectDemo&quot;)


### Set any global variables here
####################



####################


### Run the code
source(&quot;code/load.R&quot;)
source(&quot;code/clean.R&quot;)
source(&quot;code/func.R&quot;)
source(&quot;code/do.R&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I think that is very self-explanatory, but it does need some demonstration.  The next instalment in this three part blog post will describe the ProjectTemplate approach.  After that I will demonstrate ways that each of the three approaches can be used.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reproducible Research And Managing Digital Assets (1 of 3). Scott Long wrote a book and some great tutorial slides</title>
   <link href="http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets/"/>
   <updated>2015-09-25T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets</id>
   <content type="html">&lt;p&gt;This will be a series of three posts that describe some key evidence based best practice methods that have helped me plan and organise files and folders for data analysis.  I have found these via books and on websites.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scott Long's Workflow for Data Analysis with Stata&lt;/li&gt;
&lt;li&gt;Josh Reich's Least Commonly Fouled up Data analysis (LCFD) framework&lt;/li&gt;
&lt;li&gt;John Myles White's ProjectTemplate&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Toward evidence based best-practice data management systems&lt;/h2&gt;

&lt;p&gt;It is important for open science to have effective management of digital assets across the different phases of the research pipeline.  The traditional research pipeline moves from steps of hypothesis and design, measured data, analytic data, computational results (for figures, tables and numerical results), and reports (text and formatted manuscript).  Reproducible research pipelines extend traditional research by encoding the steps in code from a computer ‘scripting’ language, and distributing the data and code with publications.&lt;/p&gt;

&lt;p&gt;In this research pipeline context there are a large number of potential ways to manage digital assets (documents, data and code).  There are also many different motivating drivers that will affect the way that a scientist or group of scientists choose to manage their data and code.&lt;/p&gt;

&lt;p&gt;To deal with in house data management issues before starting and during analysis/reporting is critical for reproducible research.&lt;br/&gt;
I argue that more effective research pipelines can be achieved if scientists adopt the 'convention over configuration' paradigm and adopt best-practice systems based on evidence.&lt;/p&gt;

&lt;h3&gt;Long, S. (2015). Workflow for Reproducible Results.&lt;/h3&gt;

&lt;p&gt;For ages I was aware of the book from the Stata statistical program publishers &lt;a href=&quot;http://www.indiana.edu/~jslsoc/web_workflow/wf_home.htm&quot;&gt;http://www.indiana.edu/~jslsoc/web_workflow/wf_home.htm&lt;/a&gt;:&lt;/p&gt;

&lt;h4&gt;Citation:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Long, J. S. (2008). The Workflow of Data Analysis: 
Principles and Practice. Stata publishing.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Recently I stumbled across more recent workshop slides and tutorial material which I will discuss briefly.&lt;/p&gt;

&lt;h4&gt;Citation:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Long, S. (2015). Workflow for Reproducible Results. 
IV : Managing digital assets Workflow for Tools for your WF. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Retrieved from &lt;a href=&quot;http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf&quot;&gt;http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Long suggests a lot of practical things to do, but I will just focus here on the recommended file and folder structure:&lt;/p&gt;

&lt;h4&gt;Recommended project directory structure:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;\ProjectAcronym
    \- History starting YYYY-MM-DD
    \- Hold then delete 
    \Admin
    \Documentation 
    \Posted
         \Paper 1
             \Correspondence 
             \Text
             \Analysis
    \PrePosted 
    \Resources 
    \Write 
    \Work
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In another workshop report Long provides a useful tool to automatically create this structure on windows&lt;/li&gt;
&lt;li&gt;Long, S. (2012). Principles of Workflow in Data Analysis.
Retrieved from &lt;a href=&quot;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&quot;&gt;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;a bash version would be useful for linux and mac users, but also the R language can do this on all platforms with the &lt;code&gt;dir.create&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code: wfsetupsingle.bat&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# wfsetupsingle.bat 
REM workflow talk 2 \ wfsetupsingle.bat jsl 2009-07-12 
REM directory structure for single person.
FOR /F &quot;tokens=2,3,4 delims=/- &quot; %%a in (&quot;%DATE%&quot;) do set CDATE=%%c-%%a-%%b 
md &quot;- History starting \%cdate%&quot; 
md &quot;- Hold then delete &quot; 
md &quot;- Pre posted &quot; 
md &quot;- To clean&quot; 
md &quot;Documentation&quot; 
md &quot;Posted&quot; 
md &quot;Resources&quot;
md &quot;Text\- Versions\&quot; 
md &quot;Work\- To do&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Critical reflections&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This recommendation is very sensible, especially the suggestion of moving things through the pipeline as they evolve from things being worked on (Write/Work) to later phases when they have been polished to a point that they can be put down while preparations for distrubuting them are made (Preposted) and then once they are sent off into downstream publication phases (Posted) they are locked for ever in a archival state.&lt;/li&gt;
&lt;li&gt;I am not particularly keen on the names that have been chosen (Resources, Write and Work are quite ambiguous terms).&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Organising Graph Nodes And Edges In A Dataframe</title>
   <link href="http://ivanhanigan.github.com/2015/09/organising-graph-nodes-and-edges-in-a-dataframe/"/>
   <updated>2015-09-23T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/organising-graph-nodes-and-edges-in-a-dataframe</id>
   <content type="html">&lt;p&gt;I use the R package &lt;code&gt;DiagrammeR&lt;/code&gt; for creating graphs (the formal kind, connecting nodes with edges)
- This package is great and I like how it interacts with the Graphviz program
- One thing that I like to do in planning and organising data analysis projects is to make graphs and lists of the methods steps, inputs and Outputs
- A simple way to organise these things is in a dataframe (table) with a column for each step (node) and two others for inputs and outputs (edges)
- In my utilities R package &lt;code&gt;github.com/ivanhanigan/disentangle&lt;/code&gt; I have written functions that turn this table into a graphiviz DOT language script
- Recently I have needed to unpack the list for a more itemized view
- Both these functions are showcased below&lt;/p&gt;

&lt;h4&gt;Code: newnode&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# First create some test data, each step is a collection of edges 
# with inputs or outputs simple comma seperated lists
dat &amp;lt;- read.csv(textConnection('
cluster ,  step    , inputs                    , outputs                                , description                      
A  ,  siteIDs      , &quot;GPS, helicopter&quot;          , &quot;spatial, site doco&quot;                 , latitude and longitude of sites  
A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
B  ,  biomass      , spatial                   , biomass_g                              ,                                  
B  ,  correlations , &quot;exposures,trapped_no,biomass_g&quot; , report1                         , A study we published             
C  ,  paper1       , report1                   , &quot;open access repository, data package&quot; ,                                  
D  ,  biomass revision, new estimates          , biomass_g                              , this came late
'), stringsAsFactors = F, strip.white = T)    
str(dat)
# dat

# Now run the function and create a graph
nodes &amp;lt;- newnode(
  indat = dat,
  names_col = &quot;step&quot;,
  in_col = &quot;inputs&quot;,
  out_col = &quot;outputs&quot;,
  desc_col = &quot;description&quot;,
  clusters_col = &quot;cluster&quot;,
  nchar_to_snip = 40
  )  
sink(&quot;Transformations.dot&quot;)
cat(nodes)
sink()
#DiagrammeR::grViz(&quot;Transformations.dot&quot;)
system(&quot;dot -Tpng Transformations.dot -o Transformations.png&quot;)
browseURL(&quot;Transformations.png&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;

&lt;p&gt;That creates this diagram&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Transformations.png&quot; alt=&quot;/images/Transformations.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Now to showcase the tool that itemizes this list of inputs and outputs&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The original table has no capacity to add detail about each node as they are held as a list of inputs and outputs&lt;/li&gt;
&lt;li&gt;To add detail for each we need to unpack each list and create a new table with one row per node&lt;/li&gt;
&lt;li&gt;I decided to make this a long table with an identifier for each node about which step (edge) the node is an input or an output&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code: newnode_csv&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;nodes_graphy &amp;lt;- newnode_csv(
  indat = dat,
  names_col = &quot;step&quot;,
  in_col = &quot;inputs&quot;,
  out_col = &quot;outputs&quot;,
  clusters_col = 'cluster'
  ) 
# which creates this table
knitr::kable(nodes_graphy)
|cluster |name             |in_or_out |value                  |
|:-------|:----------------|:---------|:----------------------|
|A       |siteIDs          |input     |GPS                    |
|A       |siteIDs          |input     |helicopter             |
|A       |siteIDs          |output    |spatial                |
|A       |siteIDs          |output    |site doco              |
|A       |weather          |input     |BoM                    |
|A       |weather          |output    |exposures              |
|B       |trapped          |input     |spatial                |
|B       |trapped          |output    |trapped_no             |
|B       |biomass          |input     |spatial                |
|B       |biomass          |output    |biomass_g              |
|B       |correlations     |input     |exposures              |
|B       |correlations     |input     |trapped_no             |
|B       |correlations     |input     |biomass_g              |
|B       |correlations     |output    |report1                |
|C       |paper1           |input     |report1                |
|C       |paper1           |output    |open access repository |
|C       |paper1           |output    |data package           |
|D       |biomass revision |input     |new estimates          |
|D       |biomass revision |output    |biomass_g              |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This can now be useful for making a 'shopping list' of the data to aquire, transform, analyse or archive.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Open Notebook Science, Jekyll Blogs, Github and Jerry Seinfeld's Secret to Productivity</title>
   <link href="http://ivanhanigan.github.com/2015/09/open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity/"/>
   <updated>2015-09-22T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity</id>
   <content type="html">&lt;p&gt;The other day I reported that I've implemented a new open science task management regime &lt;a href=&quot;http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/&quot;&gt;http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This was instigated by my renewed enthusiasm for open science after a few high profile papers have come out in the last few months imploring scientists to take action on shonky statistics and the &quot;morass of poorly conducted data analyses, with errors ranging from trivial and strange to devastating&quot; (Peng 2015) &lt;a href=&quot;http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x&quot;&gt;http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I believe that making ones electronic notebook open is one of the most obvious and easily achieved things to do toward that ambition.  I also think that keeping the TODO-list in the forefront of ones mind and continuously checking things off the list is a great boost for productivity and keeping on track.  This culminates in the advice to keep momentum by doing something toward the plan on a daily basis, no matter how trivial.  This is sometimes called Jerry Seinfeld's secret to productivity: Just keep at it. Don't break the streak. &lt;a href=&quot;http://dirk.eddelbuettel.com/blog/2014/10/12/&quot;&gt;http://dirk.eddelbuettel.com/blog/2014/10/12/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So what was holding me back from a really useful daily publication of my open notebook?  I showed last post how I manage tasks in Emac Orgmode (a task organiser and calendar/agenda rolled up with code execution for running R scripts etc).  I also write my blog posts in orgmode.&lt;/p&gt;

&lt;p&gt;The only problem with that set up was that I was still using the code from Charlie Park &lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;http://charliepark.org/jekyll-with-plugins/&lt;/a&gt; which adds the inadequate commit description '&lt;code&gt;Latest build&lt;/code&gt;' every time.  What I needed was a way to actually log a summary of work each day, so I can look back over the history and know I actually did something everyday and was not just gaming the system by committing random little non-work additions (I want to balance this by doing &lt;em&gt;some&lt;/em&gt; work every day, but also take time off to read, exercise, socialize, and generally have fun).&lt;/p&gt;

&lt;p&gt;So anyway, the point of this post is to describe my revision to Charlie Park's code for building a jekyll blog:&lt;/p&gt;

&lt;h4&gt;Code: put in ~/.bash_profile&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;function bb() {
  cd ~/projects/ivanhanigan.github.com.raw &amp;amp;&amp;amp; jekyll b &amp;amp;&amp;amp; cp -r    
  ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com &amp;amp;&amp;amp; 
  cd ~/projects/ivanhanigan.github.com &amp;amp;&amp;amp; git add . -A  &amp;amp;&amp;amp; 
  git commit -m &quot;$*&quot; &amp;amp;&amp;amp; 
  git push
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;That bit about &lt;code&gt;$*&lt;/code&gt; was a bit difficult for me to get working as this is the first time I have written a bash script in anger.  The alternative was to use &lt;code&gt;$1&lt;/code&gt; and require the git commit message to be passed within quotes, which also makes sense but I did not do that.&lt;/li&gt;
&lt;li&gt;I also needed to change the terminal settings so that it always loads the bash_profile&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Bash terminal&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Edit &amp;gt; Profile preferences
Title and Command &amp;gt; Run command as a login shell 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;And so now I just have to deposit a markdown blog post into the jekyll &lt;code&gt;_posts&lt;/code&gt; folder and then&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Bash&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;bb Add a meaningful commit message about todays progress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;There you have it, a meaningful message regarding what I have been doing towards my scientific output every day.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seinfeld-streak-day9-1.png&quot; alt=&quot;/images/seinfeld-streak-day9.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seinfeld-streak-day9.png&quot; alt=&quot;/images/seinfeld-streak-day9.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Peng, R. (2015). The reproducibility crisis in science: 
A statistical counterattack. Significance, 12(3), 30–32. 
doi:10.1111/j.1740-9713.2015.00827.x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>My Newnode R Function Useful For Causal Directed Acyclic Graphs (DAGs)</title>
   <link href="http://ivanhanigan.github.com/2015/09/my-newnode-r-function-useful-for-causal-directed-acyclic-graphs/"/>
   <updated>2015-09-19T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/my-newnode-r-function-useful-for-causal-directed-acyclic-graphs</id>
   <content type="html">&lt;h1&gt;Aims&lt;/h1&gt;

&lt;p&gt;I have worked on a function that turns a &lt;code&gt;data.frame&lt;/code&gt; into a graphviz code in the dot language, with some of my preferred settings.  I realised that it might be useful for causal directed acyclic graphs.&lt;/p&gt;

&lt;p&gt;Causal diagrams are useful for conceptualising the pathways of cause and effect.  These diagrams are sometimes simplly informal pictures but have also been developed in a more formal way to be used in modelling.  These formal developments use concepts derived from the mathmatical abstraction of Graphs (fundamentally Graphs  are networks of linked 'nodes', with the links being termed 'edges').  Causal diagrams can either be constructed to depict two things: first are feedback loops (a vexatious property of complex systems that confounds modelling) while second are more simple chain-of-events type pathways which proceed from an upstream cause to a downstream effect in a single direction, without cycles, called 'Directed Acyclic Graphs or DAGs.  The loop diagrams are out of the scope of this present blog post because the DAGs are much more easily addressed by the tool that I am describing.&lt;/p&gt;

&lt;p&gt;To begin I am going to build on this other guy's blog post on causal DAGs with R
  &lt;a href=&quot;http://donlelek.github.io/2015/03/31/dags-with-r/&quot;&gt;http://donlelek.github.io/2015/03/31/dags-with-r/&lt;/a&gt;
I wanted to add an interface for building these.&lt;/p&gt;

&lt;p&gt;Some background to the concepts that I use are provided in the references below.&lt;/p&gt;

&lt;h1&gt;Materials and Methods&lt;/h1&gt;

&lt;p&gt;The DiagrammeR package which has been integrated within R-Studio has made access to the graphing tool &lt;code&gt;graphviz&lt;/code&gt; much easier than it used to be.  My function &lt;code&gt;causal_dag&lt;/code&gt; (avaiable in my &lt;code&gt;disentangle&lt;/code&gt; github package) essentially constructs the required &lt;code&gt;nodes&lt;/code&gt; and &lt;code&gt;edges&lt;/code&gt; for that package to use.  Optionally we can also include &lt;code&gt;labels&lt;/code&gt; to indicate the direction of the effect.&lt;/p&gt;

&lt;p&gt;To use the tool all you need to do is create a list of &lt;code&gt;edges&lt;/code&gt; and their associated &lt;code&gt;inputs&lt;/code&gt; nodes and &lt;code&gt;outputs&lt;/code&gt; nodes (as a comma separated values string) shown in the picture below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/causal-ssheet.png&quot; alt=&quot;causal-ssheet.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# read in the sheet
library(disentangle)
library(stringr)
causes &amp;lt;- readxl::read_excel(&quot;causal-ssheet.xlsx&quot;)
causes
nodes &amp;lt;- newnode(causes, &quot;edges&quot;, &quot;inputs&quot;, &quot;outputs&quot;)
cat(nodes)
# The result is a formated graph in the dot language with some of my
# preferred settings such as edges showing as 'records' and a spot to
# write a description or include literature about each process
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;See the DOT code in the Appendix&lt;/li&gt;
&lt;li&gt;to render the graph now DiagrammeR can use this text string R object to render this to SVG&lt;/li&gt;
&lt;li&gt;I think it does not do PNG or PDF though so I still use graphviz and dot directly&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;grViz(nodes)

# But I also use graphviz directly to produce a publishable image in
# pdf or png
sink(&quot;reproduce-donlelek.dot&quot;)
cat(nodes)
sink()# If graphviz is installed and on linux call it with a shell command
#system(&quot;dot -Tpdf reproduce-donlelek.dot -o reproduce-donlelek.pdf&quot;)
system(&quot;dot -Tpng reproduce-donlelek.dot -o reproduce-donlelek.png&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Results&lt;/h1&gt;

&lt;p&gt;Here I have reproduced the work of donlelek&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/reproduce-donlelek.png&quot; alt=&quot;reproduce-donlelek.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Future directions&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I'd like to make the edges implicit, so that the spreadsheet keeps track of the information about the causal process, but the graph just shows the lines connecting the nodes&lt;/li&gt;
&lt;li&gt;The edges are where the action is, so I need to add a direction of effect.  This would be in a &lt;code&gt;label&lt;/code&gt; column and added in a [ label = 'abc' ] tag for each edge&lt;/li&gt;
&lt;li&gt;the rankdir option is LR to make this go sideways, which seems more the norm for causal DAGs, left to right.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;References&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Greenland, S., Pearl, J., &amp;amp; Robins, J. M. (1999). Causal diagrams for
epidemiologic research. Epidemiology (Cambridge, Mass.), 10(1),
37–48. doi:10.1097/00001648-199901000-00008

Reid, C. E., Snowden, J. M., Kontgis, C., &amp;amp; Tager, I. B. (2012). The
role of ambient ozone in epidemiologic studies of heat-related
mortality. Environmental Health Perspectives, 120(12),
1627–30. doi:10.1289/ehp.1205251

Newell, B., &amp;amp; Wasson, R. (2001). Social System vs Solar System: Why
Policy Makers Need History. In: Conflict and Cooperation related to
International Water Resources : Historical Perspectives. In World
Water (Vol. 2002).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Appendix&lt;/h1&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#####################################################################
# The following output is automatically created by newnode()
# NOTE for some reason, to show on the blog, I had to replace all { braces with normal (
#####################################################################
digraph transformations (

&quot;Metritis&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Cistic Ovarian Disease&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Age&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Fertility effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Fertility effects |  ) ))&quot;]
&quot;Fertility effects&quot; -&amp;gt; &quot;Fertility&quot;


&quot;Metritis&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Retained Placenta&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Age&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Cistic Ovarian effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Cistic Ovarian effects |  ) ))&quot;]
&quot;Cistic Ovarian effects&quot; -&amp;gt; &quot;Cistic Ovarian Disease&quot;


&quot;Retained Placenta&quot; -&amp;gt; &quot;Metritis effects&quot;
&quot;Metritis effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Metritis effects |  ) ))&quot;]
&quot;Metritis effects&quot; -&amp;gt; &quot;Metritis&quot;


 &quot;Age&quot; -&amp;gt; &quot;Retained Placenta effects&quot;
&quot;Retained Placenta effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Retained Placenta effects |  ) ))&quot;]
&quot;Retained Placenta effects&quot; -&amp;gt; &quot;Retained Placenta&quot;


 )
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>If You Don't Find A Solution In R, Keep Googling!</title>
   <link href="http://ivanhanigan.github.com/2015/09/if-you-dont-find-a-solution-in-r-keep-googling/"/>
   <updated>2015-09-17T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/if-you-dont-find-a-solution-in-r-keep-googling</id>
   <content type="html">&lt;p&gt;I've learnt this lesson multiple times. It happens like this.  A solution is not immediately obvious in R so you might think of writing your own function.  Generally there is a solution you just did not google enough.
This time I was tricked a little because the GIS functions have been bad for a long time but getting better very rapidly recently.  A little while ago I had a very successful
outcome from using the &lt;code&gt;raster::extract&lt;/code&gt; function on a large raster file
to get the attributes for a set of points.  I needed to do the same
thing but this time for a shapefile and points.  I looked at the
raster package and saw you can use the &lt;code&gt;raster::intersect&lt;/code&gt; function
here, and it worked on the small sample data I tested with but failed
with the big dataset as it ran out of memory.  I assumed that R had not caught up with the GIS world yet and so I came up with this workaround below by splitting the points data layer into chunks.&lt;/p&gt;

&lt;p&gt;I then got access to ArcMap and was wondering whether it could do it, and it DID!
So then I googled a bit and found the solution was simple:&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;sp::over()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Here is my hack in case I ever need to pull out the bit that does the splitting up of the points file, or the tryCatch():&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;big_pt_intersect &amp;lt;- function(pts, ply, chunks = 100){
  idx &amp;lt;- split(pts@data, 1:chunks)
  #str(idx)
  for(i in 1:length(idx)){
  #i = 1
  print(i)
    ids &amp;lt;- idx[ [i] ][,1]
  #str(pts@data)
  qc &amp;lt;- pts[pts@data[,1] %in% ids,]
  #str(qc)
  tryCatch(
    chunk &amp;lt;-  raster::intersect(qc, ply), 
    error = function(err){print(err)})
  if(!exists('chunk_out')){

    chunk_out &amp;lt;- chunk@data
  } else {
    chunk_out &amp;lt;- rbind(chunk_out, chunk@data)
  }
  rm(chunk)

  }
  #str(chunk_out)
  return(chunk_out)
}
# NB warning about split length multiple is not fatal, just due to nonequal chunks 
# (ie the geocodes are 2009/100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Templates are Needed for Reproducible Research Reports (that Look Good)</title>
   <link href="http://ivanhanigan.github.com/2015/09/templates-needed-for-reproducible-research-reports-that-look-good/"/>
   <updated>2015-09-16T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/templates-needed-for-reproducible-research-reports-that-look-good</id>
   <content type="html">&lt;p&gt;I read with interest the the Transparency and Openness Promotion (TOP) Committee templates for guidelines to enhance transparency in the science that journals publish.&lt;/p&gt;

&lt;h4&gt;Citation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Supplementary Materials for Nosek, B. A., Alter, G., Banks, G. C.,
Borsboom, D., Bowman, S. D., Breckler, S. J., … Yarkoni,
T. (2015). Promoting an open research culture. Science, 348(6242),
1422–1425. doi:10.1126/science.aab2374
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I think though that guidelines like the suggestion to copy-paste bits of the manuscript leave a bit to be desired:&lt;/p&gt;

&lt;h4&gt;Quote;&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Authors document compliance by copy-pasting the relevant passages
in the paper that address the question into the form. For example,
when indicating how sample size was determined, authors copy paste
into the form the text in the paper that describes how sample size
was determined.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Reproducible Research Reports solve this problem by ensuring that the data preparation and analysis are executed in the same script that produces the manuscript, therefore a one-stop-shop for documentation of the entire study.&lt;/p&gt;

&lt;h2&gt;There is a need for Templates of Reproducible Research Reports (that look good!)&lt;/h2&gt;

&lt;p&gt;Rstudio provides very easy support for these documents if you use R.  In particular the option of a menu button to create a new report populates that report with the required header information and some example script to work off.  But the easiest option does not look so good.  This is the Rmarkdown option and it is very user friendly in terms of the markup language needed to write the descriptive language around your analysis (mostly plain text with a few simple options for heading styles etc) rather than the Sweave option which leads to the full blown LaTeX markup language that is a lot more complicated.&lt;/p&gt;

&lt;h4&gt;Boilerplate Rmarkdown header from Rstudio:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;---
title: &quot;Untitled&quot;
author: &quot;Ivan C. Hanigan&quot;
date: &quot;16 September 2015&quot;
output: html_document
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This is great for quick reporting of work as you go, but I  primarily write for output that will be printed (e.g. pdf docs). More specifically, I need the concept of a page, and to have full control over the placement of table and figure ‘environments’, stuff that is easy in LaTeX (once you figure out some of the esoteric parts of that language).&lt;/p&gt;

&lt;p&gt;To achieve a simple writing environment in Markdown but with the powerful layout options of LaTeX I reviewed this guys work but I think it takes it to an uneccessary level of complicated-ness
&lt;a href=&quot;https://github.com/jhollist/manuscriptPackage&quot;&gt;https://github.com/jhollist/manuscriptPackage&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So I went back to some of the old Sweave/Latex templates I had put together and ported it into a markdown header.&lt;/p&gt;

&lt;h4&gt;Boilerplate Rmarkdown header for pretty report&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;---
title: &quot;Untitled&quot;
author: &quot;Ivan C. Hanigan&quot;
date: &quot;16 September 2015&quot;
header-includes:
  - \usepackage{graphicx}
  - \usepackage{fancyhdr} 
  - \pagestyle{fancy} 
  - \usepackage{lastpage}
  - \usepackage{float} 
  - \floatstyle{boxed} 
  - \restylefloat{figure} 
  - \usepackage{url} 
  - \usepackage{color}
  - \lhead{Left Header}
  - \chead{Rmarkdown Rocks}
  - \rhead{\today}
  - \lfoot{Left Footer}
  - \cfoot{Centre Footer}
  - \rfoot{\thepage\ of \pageref{LastPage}}  
output: 
  pdf_document:
    toc: false
documentclass: article
classoption: a4paper
bibliography: references.bib
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Now the layout of tables and figures is done with latex&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Using the xtable package allows results to be displyed in tables
and has built in support for some R objects, so summrising the
linear fit above in ~\ref{ATable}

```{r, results='asis', type = 'tex'}
library(xtable)    
print(xtable(fit, caption=&quot;Example Table&quot;,
  digits=4,table.placement=&quot;ht&quot;,label=&quot;ATable&quot;), comment = F)    
```

## A Plot

Plots intergrate most easily if made seperately as can be seen in figure ~\ref{test}
```{r}
png(&quot;Rmarkdownfig.png&quot;)
plot(x,y,main=&quot;Example Plot&quot;,xlab=&quot;X Variable&quot;,ylab=&quot;Y Variable&quot;)
abline(fit,col=&quot;Red&quot;)
dev.off()
```
\begin{figure}[H]
\begin{center}
\includegraphics[width=.5\textwidth]{Rmarkdownfig.png}
\end{center}
\caption{Some Plot}
\label{test}
\end{figure}
\clearpage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I also realised that if this was to be a full report of a scientific study it would need to include some of the machinery needed for bibliographies.&lt;/p&gt;

&lt;h4&gt;Stuff for bibliographies&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;```{r, echo=F, results = 'hide', message = F, warning=F}
library(&quot;knitcitations&quot;)
library(&quot;bibtex&quot;)
cleanbib()
cite_options(citation_format = &quot;pandoc&quot;, check.entries = FALSE)

bib &amp;lt;- read.bibtex(&quot;C:/Users/Ivan/Dropbox/references/library.bib&quot;)

```

&amp;lt;!--Put data analysis and reporting here, then at the end of the doc--&amp;gt;

```{r, echo=F, message=F, eval=T}
write.bibtex(file=&quot;references.bib&quot;)
```

# References

&amp;lt;!--The bib will then be written following the final subheading--&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I hope this might help others develop their own templates for RRR that look great.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>task-management-like-an-open-science-hacker</title>
   <link href="http://ivanhanigan.github.com/2015/09/task-management-like-an-open-science-hacker/"/>
   <updated>2015-09-13T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2015/09/task-management-like-an-open-science-hacker</id>
   <content type="html">&lt;p&gt;I just read this impressive paper and it has really given me a push toward making this open lab notebook&lt;/p&gt;

&lt;h4&gt;Citation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Nosek, B. A., et al. (2015). Promoting an open research
culture. Science, 348(6242), 1422–1425. doi:10.1126/science.aab2374
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Quote&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The situation is a classic collective action problem. Many individual researchers lack
strong incentives to be more transparent, even though the credibility of science would 
benefit if everyone were more transparent.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;So I think I'll try to step up the pace of logging my daily scientific work.
One super easy thing to do is to publish my daily log from my task management in orgmode.
Indeed I am also reading at the moment this guy who says&lt;/p&gt;

&lt;h4&gt;Quote&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The core of your documentation is the research log.

Long, S. (2015). Reproducible Results and the Workflow of Data Analysis. 
Retrieved from http://www.indiana.edu/~jslsoc/ftp/WIM/wf wim 2015 2015-08-21@3.pdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Finally, I was struck by this reference &lt;a href=&quot;http://rich-iannone.github.io/about/2014/10/28/introduction.html&quot;&gt;http://rich-iannone.github.io/about/2014/10/28/introduction.html&lt;/a&gt; to something about 365+ day GitHub streaks. It was covered earlier by Geoff Greer, and by Dirk Eddelbuettel.&lt;/p&gt;

&lt;p&gt;It seems the basic concept is that you can leverage off an obsessive tendency by making sure you do something toward ticking off items from the task list every day.  The impulse to not breaking the chain is supposed to give you inspiration to keep going.  I think this might work well for my temperatment.&lt;/p&gt;

&lt;h2&gt;Emacs and orgmode&lt;/h2&gt;

&lt;p&gt;The set up of my daily log is pretty simple. After being set up by kjhealy's starter kit.
Then I modified the org-agenda-files which was set in the main el file that kjhealy provided  and then with the command C-c a a emacs will display my calendar.&lt;/p&gt;

&lt;p&gt;When I open emacs in the morning I  open the agenda and this also opens research-log file.  I move to that buffer, then I use this key command to insert a new entry for todays date&lt;/p&gt;

&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt; (define-skeleton org-journalentry
   &quot;Template for a journal entry.&quot;
   &quot;project:&quot;
   &quot;*** &quot; (format-time-string &quot;%Y-%m-%d %a&quot;) &quot; \n&quot;
   &quot;**** TODO-list \n&quot;
   &quot;***** TODO \n&quot;
   &quot;**** timesheet\n&quot;
   &quot;#+begin_src txt :tangle work-log.csv :eval no :padline no\n&quot;
   (format-time-string &quot;%Y-%m-%d %a&quot;) &quot;, &quot; str &quot;, 50\n&quot; 
   &quot;#+end_src\n&quot;
 )
 (global-set-key [C-S-f5] 'org-journalentry)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This creates a new date, a stub of a TODO for anything ad hoc and a entry into my timesheet.csv file.&lt;/p&gt;

&lt;p&gt;I then select from TODO items from a global list that I keep at the top of the file, and cut/paste them into todays list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/agenda.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Great so I just moved this research-log orgmode file into my blog github repo, and with the help of charlie park's bash script I am good to go&lt;/p&gt;

&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;alias build_blog=&quot;cd ~/projects/ivanhanigan.github.com.raw; jekyll b;
cp -r ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com;
cd ~/projects/ivanhanigan.github.com;git add .;git commit -am 'Latest build.';git push&quot;
alias bb=&quot;build_blog&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;So this will put the resulting changes onto my open lab book website here &lt;a href=&quot;https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org&quot;&gt;https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Things to note:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I found this list of tips &lt;a href=&quot;http://natashatherobot.com/streak-github-mistakes/&quot;&gt;http://natashatherobot.com/streak-github-mistakes/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In particular I realise I need to make my daily push by 4:50 PM in Canberra ACT as this is 11:50 PM the previous day for Github, Pacific Time (PT)&lt;/li&gt;
&lt;li&gt;I also will need to ensure I don't publish sensitive (or embarrasing entries).&lt;/li&gt;
&lt;li&gt;I'll try to keep the identity of my collaborators private as well, so just use their initials rather than names.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 

</feed>
