<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>Recology</title>
		<description>An exploration of using R for ecology, evolution, and open science.</description>
		<link>http://schamberlain.github.com</link>
		
			<item>
				<title>What Do Scientists Who Write Metadata Use To Do It? And Why?</title>
				<description>&lt;ul&gt;
&lt;li&gt;The extent to which scientists write metadata is probably lower than it ought to be&lt;/li&gt;
&lt;li&gt;The level of metadata written during science projects is probably described generally as 'bare-minimum' and &quot;the minimum needed for one-self to come back to and understand what one did&quot;&lt;/li&gt;
&lt;li&gt;It sometimes seems that even the bare minimum for one-self is not being kept very often&lt;/li&gt;
&lt;li&gt;I argue that the reasons for less-than-adequate metadata can be understood by looking at&lt;/li&gt;
&lt;li&gt;1) the culture of the scienctists displinary background via training&lt;/li&gt;
&lt;li&gt;2) the tools available and&lt;/li&gt;
&lt;li&gt;3) institutional  requirements to produce metadata (both about data or access to data)&lt;/li&gt;
&lt;li&gt;In my ongoing &lt;a href=&quot;http://ivanhanigan.github.io/2013/10/two-main-types-of-data-documentation-workflow/&quot;&gt;series of blog posts I am exploring the tools available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In this post I just wanted to start the discussion about discipline culture and institutional requirements.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Discipline Culture&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I trained in Geography in the age of GIS and this community uses metadata a lot&lt;/li&gt;
&lt;li&gt;Due to the prevalance of the digital map (collection of layers) which is a derivative data output&lt;/li&gt;
&lt;li&gt;Need to know the source of all the layers&lt;/li&gt;
&lt;li&gt;first law of GIS is &quot;garbage in, garbage out&quot;&lt;/li&gt;
&lt;li&gt;I was trained in the ANSLIC standard from the start&lt;/li&gt;
&lt;li&gt;ArcGIS has a tool called ArcCatalog which makes metadata easy to create and view&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Institutional Requirements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The ARC and NHMRC say they are going to require more metadata (and even data deposit)&lt;/li&gt;
&lt;li&gt;Restrictions on data access make it necessary to describe at least the metadata around provision agreements, licence, allowable access&lt;/li&gt;
&lt;li&gt;A supporting management level who value the metadata as research output (alongside a peer reviewed paper metadata pales in comparison)&lt;/li&gt;
&lt;li&gt;My old boss used to say &quot;Work Not Published Is Work Not Done&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;This reminds me of Approaches and Barriers to Reproducible Research&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;In 2011 BiostatMatt (Matt Shotwell) published &lt;a href=&quot;http://biostatmatt.com/uploads/shotwell-interface-2011.pdf&quot;&gt;a survey of biostatisticians&lt;/a&gt;
VUMC Dept. of Biostatistics to assess:&lt;/li&gt;
&lt;li&gt;the prevalence of fully scripted data analyses&lt;/li&gt;
&lt;li&gt;the prevalence of literate programming practices&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To assess the perceived barriers to reproducible research the also asked:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;What The biggest obstacle to always reproducibly scripting your work?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;pre&gt;&lt;code&gt;| Barrier                                                  | Staff | Faculty |
|----------------------------------------------------------+-------+---------|
| No signifcant obstacles.                                 |     8 |      10 |
| I havent learned how.                                    |     0 |       0 |
| It takes more time.                                      |     7 |       7 |
| It makes collaboration difficult (eg. file compatibility)|     4 |       2 |
| The software I use doesnt facilitate reproducibility.    |     0 |       0 |
| Its not always necessary for my work to be reproducible. |     2 |       0 |
| Other                                                    |     2 |       1 |
|----------------------------------------------------------+-------+---------|
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;So what about the Approaches and Barriers to Me Writing Metadata?&lt;/h3&gt;

&lt;p&gt;With a sample size of one I asked myself these questions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;| Q                                                  | A                                                                    |
|----------------------------------------------------+----------------------------------------------------------------------|
| Do I fully document data (to a metadata standard?) | Occasionally, using DDI for high value raw inputs and final products |
| Do I employ data documentation practices           | I use a tool I created to write minimal metadata occasionally        |
| What are the main barriers?                        | takes more time, The software doesnt facilitate, not always necessary|
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The tools need to help write metadata

&lt;ul&gt;
&lt;li&gt;the Institution needs to require metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Shotwell, M.S. and Alvarez, J.M. 2011. Approaches and Barriers to Reproducible Practices in Biostatistics.
http://biostatmatt.com/uploads/shotwell-interface-2011.pdf&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/what-do-scientists-who-write-metadata-use-to-do-it-and-why/</link>
			</item>
		
			<item>
				<title>librarians-and-python</title>
				<description>&lt;p&gt;I stumbled on these posts by &quot;Data Scientist Training for Librarians&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://altbibl.io/dst4l/exploratory-data-analysis-and-statistics-using-pandas-and-matplotlib/&quot;&gt;http://altbibl.io/dst4l/exploratory-data-analysis-and-statistics-using-pandas-and-matplotlib/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://altbibl.io/dst4l/pandas-munging-stats-and-visualization/&quot;&gt;http://altbibl.io/dst4l/pandas-munging-stats-and-visualization/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I;ve been wanting to learn more python.  I don't think it'll be ready for statistical modelling for a while, but I a want to be ready when it is.&lt;/p&gt;

&lt;h4&gt;R Code: get the olive oil dataset&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&quot;pgmm&quot;)
require(pgmm)
data(olive)
write.csv(olive, &quot;~/olive.csv&quot;, row.names = F)
olive &amp;lt;- read.csv(&quot;~/olive.csv&quot;)
names(olive) &amp;lt;- tolower(names(olive))
str(olive)
write.csv(olive, &quot;~/olive.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;OK now reproduce the example&lt;/h3&gt;

&lt;p&gt;I quite like the histograms from the second example.&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;%pylab inline
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.colors as colors

acidlist=['palmitic', 'palmitoleic', 'stearic', 'oleic', 'linoleic', 'linolenic', 'arachidic', 'eicosenoic']
dfsub=df[acidlist].apply(lambda x: x/100.0)
dfsub.head()

rkeys=[1,2,3]
rvals=['South','Sardinia','North']
rmap={e[0]:e[1] for e in zip(rkeys,rvals)}
rmap

fig, axes=plt.subplots(figsize=(10,20), nrows=len(acidlist), ncols=1) # sets up the framework for the graphs. Acidlist is defined elsewhere, and is a list of the acids we’re interested in.
i=0 # Sets a counter to 0

for ax in axes.flatten(): # Starts a loop to go through our plot and render each row

    acid=acidlist[i]
    seriesacid=df[acid] # creates seriesacid and sets it to df[acid], a list of the percent composition of the acid in the current iteration that’s in each olive oil.

    minmax=[seriesacid.min(), seriesacid.max()] # the minimum and maximum values plotted will be the minimum and maximum percentages that we find in the data

    for k,g in df.groupby('region'):  # starts a loop in the loop to plot the values by region
        style = {'histtype':'stepfilled', 'alpha':0.5, 'label':rmap[k], 'ax':ax}
        g[acid].hist(**style)

        ax.set_xlim(minmax)

        ax.set_title(acid)

        ax.grid(False)

        #construct legend
        ax.legend()
    i=i+1 # increments the counter, to move the loop on to the next acid.

    fig.tight_layout()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/acid.png&quot; alt=&quot;acid.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;I am not sure how to do the transparency but the rest of it would make more sense to me with R&lt;/p&gt;

&lt;p&gt;Will try to reproduce in R for head-to-head shoot out.&lt;/p&gt;
</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/librarians-and-python/</link>
			</item>
		
			<item>
				<title>handling-survey-data-with-r</title>
				<description>&lt;p&gt;R is generally very good for handling many different data types but&lt;/p&gt;

&lt;h3&gt;R has problems with survey data&lt;/h3&gt;

&lt;p&gt;This post is a stub about what packages Ive found with methods allowing to handle efficiently survey data: handle variable labels, values labels, and retrieve information about missing values&lt;/p&gt;

&lt;h4&gt;Base R:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;## Not run:
require(foreign)
analyte  &amp;lt;- read.spss(filename, to.data.frame=T) 
varslist &amp;lt;- as.data.frame(attributes(analyte)$variable.labels)
# this gives a pretty useful thing to use
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;While I was digging around in &lt;a href=&quot;http://mephisto.unige.ch/traminer&quot;&gt;TraMineR&lt;/a&gt; I found this link to Dataset, Emmanuel Rousseaux's package for handling, documenting and describing data sets of survey data.&lt;/p&gt;

&lt;h4&gt;Code:Dataset, a package for handling-survey-data-with-r&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;if(!require(Dataset)) install.packages(&quot;Dataset&quot;, repos=&quot;http://R-Forge.R-project.org&quot;);
require(Dataset)
data(dds)
str(dds)
# cool
description(dds$sexe)
# excellent!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;I'm sure there are plenty of other approaches.  I'll add them as I find them'&lt;/p&gt;
</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/handling-survey-data-with-r/</link>
			</item>
		
			<item>
				<title>Github And Reproducible Research Report Casestudy Hutchinson Drought Index</title>
				<description>&lt;h3&gt;Background&lt;/h3&gt;

&lt;p&gt;This is an example of using github for a Reproducible Research Report (RRR) using a casestudy of the  Hutchinson Drought Index.  The project is available under GNU licence at &lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex&quot;&gt;https://github.com/ivanhanigan/HutchinsonDroughtIndex&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Aims&lt;/h3&gt;

&lt;p&gt;This example will clone the repository from github and run the replication codes using the replication datasets.&lt;/p&gt;

&lt;h3&gt;Materials and Methods&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;assumes you are connected to the internet&lt;/li&gt;
&lt;li&gt;assumes you have git, a github account and ssh key set up&lt;/li&gt;
&lt;li&gt;assumes you have R and GDAL configured&lt;/li&gt;
&lt;li&gt;assumes BoM and ABS have kept their data in the locations I specified to download from&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Hutchinsons Drought Index&lt;/h3&gt;

&lt;p&gt;The Hutchinson Drought Index (or Drought Severity Index) is a climatic drought index
that was designed to reflect agricultural droughts using only rainfall data.&lt;/p&gt;

&lt;p&gt;The index was invented by Professor Mike Hutchinson at the ANU in 1992 (1) and
this project includes R codes written to describe the calculations
and also to download data (2,3) to play with.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;h4&gt;Replication Codes to run in the terminal&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:ivanhanigan/HutchinsonDroughtIndex.git ~/data/HutchinsonDroughtIndex
R
setwd(&quot;~/data/HutchinsonDroughtIndex&quot;)
source(&quot;HutchinsonDroughtIndex_go.r&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;WARNING this downloads 5.4MB from BoM and 19.9MB from ABS&lt;/li&gt;
&lt;li&gt;This runs the codes and produces graphs&lt;/li&gt;
&lt;li&gt;an alternative workflow would be to run the sweave file in the reports directory but I dont like that method as much and have set all the evaluation commands to false.&lt;/li&gt;
&lt;li&gt;it will run now that we have the graphs though, to create a report:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;setwd(&quot;reports&quot;)
Sweave(&quot;HutchinsonDroughtIndex_transformations_doc.Rnw&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;which creates a tex file that can create &lt;a href=&quot;/pdfs/HutchinsonDroughtIndex_transformations_doc.pdf&quot;&gt;this pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;but the simplest example I gave above will compute the index and create the graphs (one is shown below)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/CentralWestDrought8283.png&quot; alt=&quot;CentralWestDrought8283.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;So what actually happened?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;This project adheres to the &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;Reichian LCFD model of writing R code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In this the code is bundled into modules Func (tools), Load, Do, Clean (check).  These live in the src directory (following teh White ProjectTemplate model - well almost, He would put tools/func into lib)&lt;/li&gt;
&lt;li&gt;then a main.r (or go.r) script calls these modules&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;go.r&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt; source('src/HutchinsonDroughtIndex_tools.r')
 source('src/HutchinsonDroughtIndex_load.r')
 source('src/HutchinsonDroughtIndex_do.r')
 source('src/HutchinsonDroughtIndex_check.r')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The really interesting bit is &lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex/blob/master/src/HutchinsonDroughtIndex_tools_droughtIndex.r&quot;&gt;a tool written for this project&lt;/a&gt; that is called from within tools.r&lt;/p&gt;

&lt;p&gt;   source('src/HutchinsonDroughtIndex_tools_droughtIndex.r')&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;A keen reader would look in that script to find out exactly what the function does&lt;/li&gt;
&lt;li&gt;A keen author would push that function to an R package (a super keen bean would publish this on CRAN)&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Discussion&lt;/h3&gt;

&lt;h4&gt;Strengths:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;full codes are provided to reproduce the data access, manipulation and analysis&lt;/li&gt;
&lt;li&gt;the gory details are hidden from the casual user&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Weaknesses:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;the important details are hidden from the technical user&lt;/li&gt;
&lt;li&gt;the script depends on a bunch of things that might not be true&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;This is an example of a self-contained RRR&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Smith, D. I, Hutchinson, M. F, &amp;amp; McArthur, R. J. (1992) Climatic and
Agricultural Drought: Payments and Policy. (Centre for Resource and Environmental
Studies, Australian National University, Canberra, Australia).&lt;br/&gt;
http://fennerschool-research.anu.edu.au/spatio-temporal/publications/cres_paper1992.pdf&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bureau of Meteorology High Quality Monthly precipitation data
downloaded on 2012-01-09
from ftp://ftp.bom.gov.au/anon/home/ncc/www/change/HQmonthlyR/HQ_monthly_prcp_txt.tar&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Australian Bureau of Statistics Statistical Divisions 2006
downloaded on 2012-01-09
from http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1259.0.30.0022006?OpenDocument&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Financial support was provided by Professor Tony McMichael's
&quot;Australia Fellowship&quot; from the the National Health and Medical Research Council, via the
National Centre for Epidemiology and Population Health, Australian National University.&lt;/p&gt;
</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/github-and-reproducible-research-report-casestudy-hutchinson-drought-index/</link>
			</item>
		
			<item>
				<title>climatic-drought-guestpost-by-luciana-porfirio</title>
				<description>&lt;h3&gt;Guest post by Luciana Porfirio with code contributions by Francis Markham&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Recently I encountered a student doing an analysis who was doing everything in excel, and I couldn't contain my mouth and say R would be better... but it isn't a simple task. So any here are some tips.&lt;/li&gt;
&lt;li&gt;There is a table with 62 years * 12 months of rain data in mm. We calculated the cumulative distribution using ecdf: empirical cumulative distribution function. So the table looks like this: Year Jan Feb Mach (...) each cell contains the cum dist.&lt;/li&gt;
&lt;li&gt;We also got the number of months per year with less than X mm of rain. But he needs to know how many months  are between the drought months, regardless the year. So, I thought that converting the data into a ts was going to facilitate the task, but it doesn't, because now I don't' have columns and rows any longer.&lt;/li&gt;
&lt;li&gt;But I struggled to handle a ts object to do for example an ifelse.&lt;/li&gt;
&lt;li&gt;Luckily there are many many tricks about R, and with the code below we solved all his problems (and many months of work in excel).&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/data/raj_rain_data.csv&quot;&gt;Get the example data here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is how it looks like:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;###############################################
require(reshape2)

###############################################
#read csv file with rain BoM data
dat = read.csv('raj_rain_data.csv')

fn &amp;lt;- apply(dat[,2:13], 2, ecdf) # equivalent to Excel's percentrank function, only for cols 2 to 13 but you need to apply the function to each month

#this fun does all the months at once
fn2 = data.frame(t(do.call(&quot;rbind&quot;, sapply(1:12, FUN = function(i) fn[[i]](dat[,i+1]), simplify = FALSE))))
colnames(fn2) = colnames(dat[,2:13])
fn2$year = dat$Year

#if the value is lower than 0.4, retrieves a 1, otherwise 0
fn2$drought = rowSums(ifelse(apply(fn2[,1:12], 2, FUN= function (x) x &amp;lt; 0.4)==TRUE, 1,0))

#if the value is lower than 0.1, retrieves a 1, otherwise 0
fn2$extreme = rowSums(ifelse(apply(fn2[,1:12], 2, FUN= function (x) x &amp;lt; 0.1)==TRUE, 1,0))

str(fn2)
names(fn2)

#ts didn't work - Francis suggested to melt the data.frame # Melt
#fn2 to tall data set
fn2.tall &amp;lt;- melt(fn2, id.vars=&quot;year&quot;)

# Get the year-month into date format
fn2.tall$date &amp;lt;- with(fn2.tall,
                             as.Date(paste(&quot;1&quot;, variable, year), &quot;%d %b %Y&quot;))

# Convert dates into months since 0 BC/AD (arbitrary, but doesn't
# matter)
#I'm not using the month.idx with Ivan's solution (remove)
fn2.tall$mnth.idx &amp;lt;- sapply(fn2.tall$date, function(x){
  12*as.integer(format(x, &quot;%Y&quot;)) + (as.integer(format(x, &quot;%m&quot;)) - 1)
})

# Sort by date
fn2.tall &amp;lt;- fn2.tall[order(fn2.tall$date),]


#Ivan's solution

x&amp;lt;-fn2.tall$value&amp;lt;0.4
xx &amp;lt;- (cumsum(!x) + 1) * x
x2&amp;lt;-(seq_along(x) - match(xx, xx) + 1) * x
fn2.tall$count&amp;lt;-x2

#counts the number of cases of drought
as.data.frame(table(fn2.tall$count))
###############################################
###############################################
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>Fri Nov 01 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/climatic-drought-guestpost-by-luciana-porfirio/</link>
			</item>
		
			<item>
				<title>quantum-gis-visualisations</title>
				<description>&lt;ul&gt;
&lt;li&gt;This is a quick post on Quantum GIS for spatial data visualisation&lt;/li&gt;
&lt;li&gt;it is also a follow up on &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/06/test-gislibrary/&quot;&gt;this post about area concordance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quantum GIS is getting pretty good but is still a bit tricky to make good looking maps&lt;/li&gt;
&lt;li&gt;QGIS can use &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/04/quantumgis-and-postgis/&quot;&gt;remote PostGIS geodatabases on the Cloud as the backend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code: use postgis to create area-concordance&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(devtools)
install_github(&quot;gisviz&quot;, &quot;ivanhanigan&quot;)
require(gisviz)
require(swishdbtools)
ch &amp;lt;- connect2postgres2(&quot;gislibrary&quot;)
# make a temporary tablename, to avoid clobbering
temp_table &amp;lt;- swish_temptable(&quot;gislibrary&quot;)
temp_table &amp;lt;- paste(&quot;public&quot;, temp_table$table, sep = &quot;.&quot;)
temp_table
# this is going to be public.foo11c7673416ea

sql &amp;lt;- postgis_concordance(conn = ch, source_table = &quot;abs_sla.nswsla91&quot;,
   source_zones_code = 'sla_id', target_table = &quot;abs_sla.nswsla01&quot;,
   target_zones_code = &quot;sla_code&quot;,
   into = temp_table, tolerance = 0.01,
   subset_target_table = &quot;cast(sla_code as text) like '105%'&quot;, 
   eval = F) 
cat(sql)
dbSendQuery(ch, sql)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;now connect to PostGIS using QGIS &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/04/quantumgis-and-postgis/&quot;&gt;as described in this tute&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and add the layer to the map&lt;/li&gt;
&lt;li&gt;Style it how you like, I also added NSWSLA1991 over the top&lt;/li&gt;
&lt;li&gt;go into the &quot;new print composer&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/qgis-new-print-composer.png&quot; alt=&quot;qgis-new-print-composer.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/qgis-add-new-map.png&quot; alt=&quot;qgis-add-new-map.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;hit the export to image and viola&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/qgis-export-image.png&quot; alt=&quot;qgis-export-image.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Don't forget to clean up the database!&lt;/h3&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;dbSendQuery(ch, sprintf(&quot;drop table %s&quot;, temp_table))
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>Thu Oct 31 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/quantum-gis-visualisations/</link>
			</item>
		
			<item>
				<title>notes-on-spatial-stats-meeting-with-sarunya-sujaritpong</title>
				<description>&lt;ul&gt;
&lt;li&gt;Yesterday I met with Sarunya Sujaritpong a PhD student working with &lt;a href=&quot;http://ivanhanigan.github.io/2013/10/spatially-structured-time-series-with-nmmaps/&quot;&gt;spatially structured time-series models as described previously&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Her supervisor Keith Dear has given me a lot of good stats advice in the past and one bit I keep thinking about is that a time series model can be fit for multiple spatial areal units of the same city and residual spatial auto-correlation in the errors should not be too much of a concern&lt;/li&gt;
&lt;li&gt;The problem would be if you get strong spatial autocorrelation of the residuals this indicates that the assumption of independent errors is violated and you will have tighter confidence intervals around the coefficients of interest than is really the case, inflating the signficance estimated for the relative risk&lt;/li&gt;
&lt;li&gt;The beta coefficient itself shouldn't be affected.&lt;/li&gt;
&lt;li&gt;So long as biostatisticians like Keith are comfortable with not addressing this issue in spatially structured time-series that is great but I see people are &lt;a href=&quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0043360&quot;&gt;starting to include this in their models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To date I've mostly seen it done in spatial (cross sectional) data analysis, not spatial times-series&lt;/li&gt;
&lt;li&gt;I'm preparing for the day when I might need to address this for a reviewer and would like to know what to do about it in case that happens&lt;/li&gt;
&lt;li&gt;So I asked Sarunya for a discussion about her research&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Sarunya's model is essentially like this&lt;/h3&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;fit &amp;lt;- glm(deaths ~ pollutant1 + pollutant2 + pollutant ... +
       ns(temp, df) + ns(humidity, df) +
       ns(time, df = 7*numYears) +
       SLA * ns(time, df = 2),
       data = analyte, family = poisson
       )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;SLA is Statistical Local Area (now called SA2, like a suburb)&lt;/li&gt;
&lt;li&gt;Sarunya explained that the research question was if the magnitude of the coeff on pollutant1 differed between this model and the old style of model where an entire city is used as the unit of analysis per day and exposure estimates are calculated as averages across several monitoring stations in the city&lt;/li&gt;
&lt;li&gt;turns out that this comparison is still valid EVEN IF THE STANDARD ERROR IS BIASED DUE TO RESIDUAL SPATIAL AUTOCORRELATION&lt;/li&gt;
&lt;li&gt;therefore this study avoids the issue by it's intentional design to compare betas not se&lt;/li&gt;
&lt;li&gt;Interestingly Sarunya explained that the stats theory suggests that even if the exposure precision is increased (exposure misclassification bias is decreased) the se on the beta will not be affected.&lt;/li&gt;
&lt;li&gt;this is fascinating in itself, but a separate issue for another post&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;So it looks like the extent a study might need to consider the issue of potential residual spatial autocorrelation depends alot on what questions are being asked and what inferences will be attempted from the results&lt;/li&gt;
&lt;li&gt;if the aim of the study is to estimate the magnitude AND CONFIDENCE INTERVALS of an exposure's relative risk (especially some novel exposure such as interstellar space dust across the suburbs) then the issue might become important to address.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;THANKS Sarunya!&lt;/p&gt;
</description>
				<published>Thu Oct 31 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/notes-on-spatial-stats-meeting-with-sarunya-sujaritpong/</link>
			</item>
		
			<item>
				<title>if-disease-incidence-varies-with-age-control-for-it</title>
				<description>&lt;ul&gt;
&lt;li&gt;today I was in a meeting where the discussion turned to maps of crude incidence rates, age-standardised rates and regression models controlling for age&lt;/li&gt;
&lt;li&gt;If disease incidence does not vary with age then there is not much point in controlling for this,&lt;/li&gt;
&lt;li&gt;apart from the work done in exploratory data analysis to ascertain whether or not this is the case&lt;/li&gt;
&lt;li&gt;I proposed that if you've done all the work on age-specific rates needed to test if incidence varies with age, then you might as well present age adjusted rates seeing as you've already done the work anyway&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;An example&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The project is highly confidential due to the nature of the data&lt;/li&gt;
&lt;li&gt;We;ll call the study location South Kingsland to protect the identity&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# get spatial data
require(swishdbtools) # github package, under
                      # swish-climate-impact-assessment

Tab1 &amp;lt;- read.csv(&quot;data/dataset.csv&quot;, sep= &quot;,&quot;, header = TRUE, stringsAsFactor = F)
Tab1 &amp;lt;- as.data.frame(table(Tab1$SLA))
nrow(Tab1)
write.csv(Tab1, &quot;data/SLA.csv&quot;, row.names = F)
load2postgres(&quot;data/SLA.csv&quot;, schema=&quot;ivan_hanigan&quot;, tablename=&quot;sla&quot;, ip = &quot;brawn.anu.edu.au&quot;,
              db = &quot;gislibrary&quot;, pguser = &quot;ivan_hanigan&quot;, printcopy = F)


ch  &amp;lt;- connect2postgres2(&quot;gislibrary&quot;)
sql_subset(ch, &quot;ivan_hanigan.sla&quot;, eval = T, limit = 10)


sql_subset(ch, &quot;abs_sla.qldsla01&quot;, select =  &quot;sla_name, st_y(st_centroid(geom))&quot;, eval = T, limit = 10)
dbSendQuery(ch,
  &quot;select sla_name, geom
  into southkingsland
  from abs_sla.qldsla01 t1
  join
  sla t2
  on t1.sla_name = t2.var1;
  alter table southkingsland add column gid serial primary key;
  &quot;)
# these are missing from spatial data
## 42                      West End                          &amp;lt;NA&amp;gt;
## 43                OVERSEAS-OTHER                          &amp;lt;NA&amp;gt;
## 44                  Yarrabah (S)                          &amp;lt;NA&amp;gt;
## 45     Rowes Bay-Belgian gardens                          &amp;lt;NA&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;visualise with QGIS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/southkingsland.png&quot; alt=&quot;southkingsland.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
require(plyr)

# load
Tab2 &amp;lt;- read.csv(&quot;data/dataset.csv&quot;, sep= &quot;,&quot;, header = TRUE, stringsAsFactor = F)
names(Tab2)
# [1] &quot;SLA&quot;       &quot;agegrp&quot;    &quot;Sex&quot;       &quot;district&quot;  &quot;dm&quot;        &quot;ninf&quot;     
# [7] &quot;popSLA&quot;    &quot;Month&quot;     &quot;Year&quot;      &quot;groupyear&quot;
test_sla  &amp;lt;- names(table(Tab2$SLA)[1])
subset(Tab2, SLA == test_sla &amp;amp; Year == 2001 &amp;amp; agegrp == 1 &amp;amp; Sex == &quot;MALE&quot;)
subset(Tab2, SLA == test_sla &amp;amp; Year == 2001 &amp;amp; agegrp == 1 &amp;amp; Sex == &quot;FEMALE&quot;)

# clean
summary(Tab2)
## remove any SLA with NA populations
Tab2 &amp;lt;- Tab2[which(!is.na(Tab2$popSLA)),]     

# do
## first total cases by annualised pop
analyte  &amp;lt;- ddply(Tab2, c(&quot;SLA&quot;, &quot;Year&quot;, &quot;agegrp&quot;, &quot;Sex&quot;), summarise,
                  ninf=sum(ninf),
                  popSLA=mean(popSLA)
                  )
#str(analyte)

#subset(analyte, SLA == test_sla)
subset(analyte, SLA == test_sla &amp;amp; Year == 2001 &amp;amp; agegrp == 1)

## now annual totals for study region
analyte2  &amp;lt;- ddply(analyte, c(&quot;Year&quot;, &quot;agegrp&quot;), summarise,
                  ninf=sum(ninf),
                  popSLA=sum(popSLA)
                  )
str(analyte2)

qc &amp;lt;- subset(analyte2,  Year == 2001)
qc
sum(qc$popSLA)

## now totals
qc  &amp;lt;- ddply(analyte2, c(&quot;Year&quot;), summarise,
             ninf=sum(ninf),
             popSLA=sum(popSLA)
             )
qc
sum(qc$ninf)
mean(qc$popSLA)

## totals by age
subset(analyte2,  agegrp == 1)
analyte3  &amp;lt;- ddply(analyte2, c(&quot;agegrp&quot;), summarise,
                  ninf=sum(ninf),
                  popSLA=mean(popSLA)
                  )
analyte3
sum(analyte3$popSLA)
analyte3$asr  &amp;lt;- (analyte3$ninf / analyte3$popSLA) * 1000
analyte3

png(&quot;graphs/south-kingsland-age-specific-rates.png&quot;) 
mp &amp;lt;- barplot(analyte3$asr, names.arg = analyte3$agegrp)
box()
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;Plot Age Specific Rates&lt;/h3&gt;

&lt;p&gt;Great so with that bit of work we should have an idea of the variation of incidence by age&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/south-kingsland-age-specific-rates.png&quot; alt=&quot;south-kingsland-age-specific-rates.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Results and Discussion&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;It looks like the incidence does vary with age&lt;/li&gt;
&lt;li&gt;TODO make graphs per year (the scale of the above graph is wrong - decades in the numerator but annualised pops in the denominator)&lt;/li&gt;
&lt;li&gt;TODO make graphs by city, by sex&lt;/li&gt;
&lt;li&gt;age-standardised rates probably need different age categories, only 4 or 5?&lt;/li&gt;
&lt;li&gt;worth commenting on the liklihood that age is not important for time-series models at the city scale, unless age structure changes substantially over time (this is an analysis for the spatial pattern only)&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Clean up database!&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;dbSendQuery(ch, &quot;drop table sla&quot;)
dbSendQuery(ch, &quot;drop table southkingsland&quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>Thu Oct 31 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/if-disease-incidence-varies-with-age-control-for-it/</link>
			</item>
		
			<item>
				<title>morpho-and-reml-boilerplate-streamline-the-process-of-metadata-entry</title>
				<description>&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Disentangle Things&lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 morpho-and-reml-boilerplate-streamline-the-process-of-metadata-entry&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Speed and Rigour&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 Analysts can often trade-off completeness of documentation for speed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4&quot;&gt;1.4 Librarians produce gold plated documentation and can take longer to produce this&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5&quot;&gt;1.5 An example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6&quot;&gt;1.6 Embracing Inaccuracy and Incompleteness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-7&quot;&gt;1.7 Aim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8&quot;&gt;1.8 Step 1: load a simple example dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-9&quot;&gt;1.9 Step 2 create a function to deliver the minimal metadata object&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-10&quot;&gt;1.10 reml&lt;sub&gt;boilerplate&lt;/sub&gt;-code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-11&quot;&gt;1.11 reml&lt;sub&gt;boilerplate&lt;/sub&gt;-test-code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-12&quot;&gt;1.12 Results: This loads into Morpho with some errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-13&quot;&gt;1.13 Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; morpho-and-reml-boilerplate-streamline-the-process-of-metadata-entry&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1&lt;/span&gt; Background&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1&quot;&gt;


&lt;ul&gt;
&lt;li&gt;The Morpho/Metacat system is great for a data repository
&lt;/li&gt;
&lt;li&gt;Morpho also claims to be suitable for Ecologists to document their data
&lt;/li&gt;
&lt;li&gt;But in my experience it leaves a little to be desired in ease of use for both purposes
&lt;/li&gt;
&lt;li&gt;Specifically the speed that documentation can be entered into Morpho is slow
&lt;/li&gt;
&lt;li&gt;This post is a first attempt to create some boilerplate code to quickly generate EML metadata using REML.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2&lt;/span&gt; Speed and Rigour&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2&quot;&gt;

&lt;p&gt;As I noted in a previous post, there are [two types of data documentation workflow](&lt;a href=&quot;http://ivanhanigan.github.io/2013/10/two-main-types-of-data-documentation-workflow/&quot;&gt;http://ivanhanigan.github.io/2013/10/two-main-types-of-data-documentation-workflow/&lt;/a&gt;).  
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GUI
&lt;/li&gt;
&lt;li&gt;Programatic
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  
I also think there are two types of users with different motivations and constraints:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1) Data Analysts
&lt;/li&gt;
&lt;li&gt;2) Data Librarians
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3&lt;/span&gt; Analysts can often trade-off completeness of documentation for speed&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3&quot;&gt;

&lt;p&gt;In my view the Analysts group of users need a tool that will very rapidly document their data and workflow steps and can live with a bit less rigour in the quality of documentation.  Obviously this is not ideal but seems an inevitable trade-off needed to enable analysts to keep up the momentum of the data processing and modelling without getting distracted by tedious (and potentially unnecessary) data documentation tasks.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4&lt;/span&gt; Librarians produce gold plated documentation and can take longer to produce this&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4&quot;&gt;

&lt;p&gt;On the other hand the role of the Librarian group is to produce documentation to the best level possible (given time and resource constraints) the datasets and methodologies that lead to the creation of the datasets.  For that group Rigour will take precedence and there will be a trade-off in terms of the amount of time needed to produce the documentation.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5&lt;/span&gt; An example&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5&quot;&gt;

&lt;p&gt;As an example of the two different groups, an analyst working with weather data in Australia may want to specify that their variable &quot;temperature&quot; is the average of the daily maxima and minima, but might not need to specify that the observations were taken inside a Stevenson Screen, or even if they are in Celsius, Farenhiet or Kelvin.  They will be very keen to start the analysis to identify any associations between weather variables and the response variable they are investigating.   The data librarian on the other hand will be more likely to need to include this information so that the users of the temperature data do not mis-interpret it.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-6&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.6&lt;/span&gt; Embracing Inaccuracy and Incompleteness&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-6&quot;&gt;


&lt;ul&gt;
&lt;li&gt;I've been talking about this for a while got referred to this document by Ben Davies at the ANUSF
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[http://thedailywtf.com/Articles/Documentation-Done-Right.aspx](&lt;a href=&quot;http://thedailywtf.com/Articles/Documentation-Done-Right.aspx&quot;&gt;http://thedailywtf.com/Articles/Documentation-Done-Right.aspx&lt;/a&gt;)
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;It has this bit:
&lt;/li&gt;
&lt;/ul&gt;




&lt;pre class=&quot;src src-R&quot;&gt;  
   
Embracing Inaccuracy and Incompleteness 
    
The immediate answer to what&amp;#8217;s the right way to do documentation is
clear: produce the least amount of documentation needed to facilitate
the most understanding, and be very explicit about which documentation
is to be maintained and which is to be archived (i.e., read-only and
left to rot).
&lt;/pre&gt;


&lt;ul&gt;
&lt;li&gt;Roughly speaking, a full EML document produced by Morpho is a bit like a whole bunch of cruft that isnt needed and gets in the way (and is more confusing)
&lt;/li&gt;
&lt;li&gt;Whereas a minimal version Im thinking of covers almost all the generic entries providing the &quot;minimum amount of stuff to make it work right&quot;.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-7&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-7&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.7&lt;/span&gt; Aim&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-7&quot;&gt;


&lt;ul&gt;
&lt;li&gt;This experiment aims to speed up the creation of a minimal &quot;skeleton&quot; of metadata to a level that both the groups above can be comfortable with AS A FIRST STEP.
&lt;/li&gt;
&lt;li&gt;It is assumed that additional steps will then need to be taken to complete the documentation, but the automation of the first part of the process should shave off enough time to suit the purposes of both groups
&lt;/li&gt;
&lt;li&gt;It is an imperative that the quick-start creation of the metadata does not end up costing the documentor more time later on down the track if they need to go back to many of the elements for additional editing.
&lt;/li&gt;
&lt;/ul&gt;





&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8&lt;/span&gt; Step 1: load a simple example dataset&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8&quot;&gt;

&lt;p&gt;I've been using a [fictitious dataset from a Statistics Methodology paper by Ritschard 2006](&lt;a href=&quot;http://ivanhanigan.github.io/2013/10/test-data-for-classification-trees/&quot;&gt;http://ivanhanigan.github.io/2013/10/test-data-for-classification-trees/&lt;/a&gt;).  It will do as a first cut but when it comes to actually test this out it would be good to have something that would take a bit longer (so that the frustrations of using Morpho become very apparent).
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;  &lt;span style=&quot;color: #586e75;&quot;&gt;#### &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;R Code:&lt;/span&gt;
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;func&lt;/span&gt;
      &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(devtools)
      install_github(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;disentangle&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;ivanhanigan&quot;&lt;/span&gt;)
      &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(disentangle)
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;load&lt;/span&gt;
      fpath &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; system.file(
          file.path(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;extdata&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;civst_gend_sector_full.csv&quot;&lt;/span&gt;),
          package = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;disentangle&quot;&lt;/span&gt;
          )
      data_set &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(fpath)
      summary(data_set)
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;store it in the current project workspace&lt;/span&gt;
      write.csv(data_set, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data/civst_gend_sector_full.csv&quot;&lt;/span&gt;, row.names = F)
      



&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| divorced/widowed: 33 | female:132 | primary  :116 | Min.   : 128.9 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| married         :120 | male  :141 | secondary: 99 | 1st Qu.: 768.3 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| single          :120 | nil        | tertiary : 58 | Median : 922.8 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| nil                  | nil        | nil           | Mean   : 908.4 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| nil                  | nil        | nil           | 3rd Qu.:1079.1 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| nil                  | nil        | nil           | Max.   :1479.4 |&lt;/span&gt;

&lt;/pre&gt;




&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-9&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-9&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.9&lt;/span&gt; Step 2 create a function to deliver the minimal metadata object&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-9&quot;&gt;

&lt;ul&gt;
&lt;li&gt;the package REML will create a EML metadata document quite easily
&lt;/li&gt;
&lt;li&gt;I will assume that a lot of the data elements are self explanatory and take column names and factor levels as the descriptions
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-10&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-10&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.10&lt;/span&gt; reml&lt;sub&gt;boilerplate&lt;/sub&gt;-code&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-10&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:reml_boilerplate&lt;/span&gt;
 
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;func&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(reml)) {
  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(devtools)
  install_github(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;reml&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;ropensci&quot;&lt;/span&gt;)
  } 
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(reml)

&lt;span style=&quot;color: #268bd2;&quot;&gt;reml_boilerplate&lt;/span&gt; &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;function&lt;/span&gt;(data_set, created_by = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;Ivan Hanigan &amp;lt;&lt;a href=&quot;mailto:ivanhanigan&amp;#64;gmail.com&quot;&gt;ivanhanigan&amp;#64;gmail.com&lt;/a&gt;&amp;gt;&quot;&lt;/span&gt;, data_dir = getwd(), titl = &lt;span style=&quot;color: #b58900;&quot;&gt;NA&lt;/span&gt;, desc = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;&quot;&lt;/span&gt;)
{

  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;essential&lt;/span&gt;
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(is.na(titl)) &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;stop&lt;/span&gt;(print(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;must specify title&quot;&lt;/span&gt;))
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;we can get the col names easily&lt;/span&gt;
  col_defs &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; names(data_set)
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;next create a list from the data&lt;/span&gt;
  unit_defs &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; list()
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:ncol(data_set))
    {
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;i = 4&lt;/span&gt;
      &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(is.numeric(data_set[,i])){
        unit_defs[[i]] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;numeric&quot;&lt;/span&gt;
      } &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;else&lt;/span&gt; {
        unit_defs[[i]] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; names(table(data_set[,i]))          
      }
    }
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;unit_defs&lt;/span&gt;
  
  ds &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; data.set(data_set,
                 col.defs = col_defs,
                 unit.defs = unit_defs
                 )
  &lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;str(ds)&lt;/span&gt;

  metadata  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; metadata(ds)
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;needs names&lt;/span&gt;
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:ncol(data_set))
    {
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;i = 4&lt;/span&gt;
      &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(is.numeric(data_set[,i])){
        names(metadata[[i]][[3]]) &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;number&quot;&lt;/span&gt;
      } &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;else&lt;/span&gt; {
        names(metadata[[i]][[3]]) &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; metadata[[i]][[3]]
      }
    }
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;metadata&lt;/span&gt;
  oldwd &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; getwd()
  setwd(data_dir)
  eml_write(data_set, metadata,
            title = titl,  
            description = desc,
            creator = created_by
            )
  setwd(oldwd)
  sprintf(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;your metadata has been created in the '%s' directory&quot;&lt;/span&gt;, data_dir)
  }
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-11&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-11&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.11&lt;/span&gt; reml&lt;sub&gt;boilerplate&lt;/sub&gt;-test-code&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-11&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:reml_boilerplate-test&lt;/span&gt;

analyte &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data/civst_gend_sector_full.csv&quot;&lt;/span&gt;)
reml_boilerplate(
  data_set = analyte,
  created_by = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;Ivan Hanigan &amp;lt;&lt;a href=&quot;mailto:ivanhanigan&amp;#64;gmail.com&quot;&gt;ivanhanigan&amp;#64;gmail.com&lt;/a&gt;&amp;gt;&quot;&lt;/span&gt;,
  data_dir = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data&quot;&lt;/span&gt;,
  titl = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;civst_gend_sector_full&quot;&lt;/span&gt;,
  desc = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;An example, fictional dataset&quot;&lt;/span&gt;
  )

dir(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data&quot;&lt;/span&gt;)
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-12&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-12&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.12&lt;/span&gt; Results: This loads into Morpho with some errors&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-12&quot;&gt;

&lt;ul&gt;
&lt;li&gt;Notably unable to import the data file
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
&lt;img src=&quot;/images/morpho-reml-boilerplate.png&quot; alt = &quot;morpho-reml-boilerplate.png&quot;&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Also &quot;the saved document is not valid for some reason&quot;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
&lt;img src=&quot;/images/morpho-reml-boilerplate2.png&quot; alt = &quot;morpho-reml-boilerplate2.png&quot;&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-13&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-13&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.13&lt;/span&gt; Conclusions&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-13&quot;&gt;

&lt;ul&gt;
&lt;li&gt;This needs testing
&lt;/li&gt;
&lt;li&gt;A real deal breaker is if the EML is not valid 
&lt;/li&gt;
&lt;li&gt;In some cases not having the data table included will be a deal breaker (ie KNB repositories designed for downloading complete data packs
&lt;/li&gt;
&lt;li&gt;A definite failure would be that even if it is quicker to get started if it takes a long time and is difficult to fix up it might increase the risk of misunderstandings.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</description>
				<published>Tue Oct 29 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/morpho-and-reml-streamline-the-process-of-metadata-entry/</link>
			</item>
		
			<item>
				<title>counting-number-of-consecutive-months-in-drought</title>
				<description>&lt;p&gt;I got this question today from a Drought researcher:&lt;/p&gt;

&lt;h4&gt;Question:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;There a table with 62 years * 12 months of rain data in mm. We
calculated the cumulative distribution using ecdf: empirical
cumulative distribution function. So the table looks like this:
Year Jan Feb Mach (…) each cell contains the cum dist.

We also got the number of months per year with less than X mm of
rain. But he needs to know how many months are between the
drought months, regardless the year. So, I thought that
converting the data into a ts was going to facilitate the task,
but it doesn’t, because now I don’t’ have columns and rows any
longer.

How do I handle a ts object to do for example an ifelse?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I answered:&lt;/p&gt;

&lt;p&gt;This looks a lot like the Hutchinson Drought Index which I've coded up here&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex&quot;&gt;https://github.com/ivanhanigan/HutchinsonDroughtIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex/blob/master/src/HutchinsonDroughtIndex_tools_droughtIndex.r&quot;&gt;https://github.com/ivanhanigan/HutchinsonDroughtIndex/blob/master/src/HutchinsonDroughtIndex_tools_droughtIndex.r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Also there is a completely reproducible example with BoM rainfall data at &lt;a href=&quot;https://github.com/ivanhanigan/SuicideAndDroughtInNSW&quot;&gt;https://github.com/ivanhanigan/SuicideAndDroughtInNSW&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is the key bit for counting consecutive months below a threshold&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;x&amp;lt;-data$index&amp;lt;=-1
xx &amp;lt;- (cumsum(!x) + 1) * x
x2&amp;lt;-(seq_along(x) - match(xx, xx) + 1) * x
data$count&amp;lt;-x2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;
I got it from &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2007-June/134594.html&quot;&gt;https://stat.ethz.ch/pipermail/r-help/2007-June/134594.html&lt;/a&gt;
and looked in wonder at the structure and am amazed it works.&lt;/p&gt;

&lt;p&gt;I also really like the cumulative summing version a lot&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;data$sums&amp;lt;-as.numeric(0)
y &amp;lt;- ifelse(data$index &amp;gt;= -1, 0, data$index)
f &amp;lt;- data$index &amp;lt; -1
f &amp;lt;- (cumsum(!f) + 1) * f
z &amp;lt;- unsplit(lapply(split(y,f),cumsum),f)
data$sums &amp;lt;- z
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;
Several things I'd like to change.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I like the idea of using a wide dataset and ecdf rather than my loop thru months&lt;/li&gt;
&lt;li&gt;I'd like to look into Joe Wheatley;s approach &lt;a href=&quot;]http://joewheatley.net/20th-century-droughts/&quot;&gt;http://joewheatley.net/20th-century-droughts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I was defeated by Mike's request to make a different threshold for breaking a drought than starting a drought.  went back to a for loop&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Tue Oct 29 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/counting-number-of-consecutive-months-in-drought/</link>
			</item>
		
	</channel>
</rss>