<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>disentangle</title>
		<description>Disentangle Things</description>
		<link>http://ivanhanigan.github.com</link>
		
			<item>
				<title>Organising Graph Nodes And Edges In A Dataframe</title>
				<description>&lt;p&gt;I use the R package &lt;code&gt;DiagrammeR&lt;/code&gt; for creating graphs (the formal kind, connecting nodes with edges)
- This package is great and I like how it interacts with the Graphviz program
- One thing that I like to do in planning and organising data analysis projects is to make graphs and lists of the methods steps, inputs and Outputs
- A simple way to organise these things is in a dataframe (table) with a column for each step (node) and two others for inputs and outputs (edges)
- In my utilities R package &lt;code&gt;github.com/ivanhanigan/disentangle&lt;/code&gt; I have written functions that turn this table into a graphiviz DOT language script
- Recently I have needed to unpack the list for a more itemized view
- Both these functions are showcased below&lt;/p&gt;

&lt;h4&gt;Code: newnode&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# First create some test data, each step is a collection of edges 
# with inputs or outputs simple comma seperated lists
dat &amp;lt;- read.csv(textConnection('
cluster ,  step    , inputs                    , outputs                                , description                      
A  ,  siteIDs      , &quot;GPS, helicopter&quot;          , &quot;spatial, site doco&quot;                 , latitude and longitude of sites  
A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
B  ,  biomass      , spatial                   , biomass_g                              ,                                  
B  ,  correlations , &quot;exposures,trapped_no,biomass_g&quot; , report1                         , A study we published             
C  ,  paper1       , report1                   , &quot;open access repository, data package&quot; ,                                  
D  ,  biomass revision, new estimates          , biomass_g                              , this came late
'), stringsAsFactors = F, strip.white = T)    
str(dat)
# dat

# Now run the function and create a graph
nodes &amp;lt;- newnode(
  indat = dat,
  names_col = &quot;step&quot;,
  in_col = &quot;inputs&quot;,
  out_col = &quot;outputs&quot;,
  desc_col = &quot;description&quot;,
  clusters_col = &quot;cluster&quot;,
  nchar_to_snip = 40
  )  
sink(&quot;Transformations.dot&quot;)
cat(nodes)
sink()
#DiagrammeR::grViz(&quot;Transformations.dot&quot;)
system(&quot;dot -Tpng Transformations.dot -o Transformations.png&quot;)
browseURL(&quot;Transformations.png&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;

&lt;p&gt;That creates this diagram&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Transformations.png&quot; alt=&quot;/images/Transformations.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Now to showcase the tool that itemizes this list of inputs and outputs&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The original table has no capacity to add detail about each node as they are held as a list of inputs and outputs&lt;/li&gt;
&lt;li&gt;To add detail for each we need to unpack each list and create a new table with one row per node&lt;/li&gt;
&lt;li&gt;I decided to make this a long table with an identifier for each node about which step (edge) the node is an input or an output&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code: newnode_csv&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;nodes_graphy &amp;lt;- newnode_csv(
  indat = dat,
  names_col = &quot;step&quot;,
  in_col = &quot;inputs&quot;,
  out_col = &quot;outputs&quot;,
  clusters_col = 'cluster'
  ) 
# which creates this table
knitr::kable(nodes_graphy)
|cluster |name             |in_or_out |value                  |
|:-------|:----------------|:---------|:----------------------|
|A       |siteIDs          |input     |GPS                    |
|A       |siteIDs          |input     |helicopter             |
|A       |siteIDs          |output    |spatial                |
|A       |siteIDs          |output    |site doco              |
|A       |weather          |input     |BoM                    |
|A       |weather          |output    |exposures              |
|B       |trapped          |input     |spatial                |
|B       |trapped          |output    |trapped_no             |
|B       |biomass          |input     |spatial                |
|B       |biomass          |output    |biomass_g              |
|B       |correlations     |input     |exposures              |
|B       |correlations     |input     |trapped_no             |
|B       |correlations     |input     |biomass_g              |
|B       |correlations     |output    |report1                |
|C       |paper1           |input     |report1                |
|C       |paper1           |output    |open access repository |
|C       |paper1           |output    |data package           |
|D       |biomass revision |input     |new estimates          |
|D       |biomass revision |output    |biomass_g              |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This can now be useful for making a 'shopping list' of the data to aquire, transform, analyse or archive.&lt;/p&gt;
</description>
				<published>2015-09-23 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/organising-graph-nodes-and-edges-in-a-dataframe/</link>
			</item>
		
			<item>
				<title>Open Notebook Science, Jekyll Blogs, Github and Jerry Seinfeld's Secret to Productivity</title>
				<description>&lt;p&gt;The other day I reported that I've implemented a new open science task management regime &lt;a href=&quot;http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/&quot;&gt;http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This was instigated by my renewed enthusiasm for open science after a few high profile papers have come out in the last few months imploring scientists to take action on shonky statistics and the &quot;morass of poorly conducted data analyses, with errors ranging from trivial and strange to devastating&quot; (Peng 2015) &lt;a href=&quot;http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x&quot;&gt;http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I believe that making ones electronic notebook open is one of the most obvious and easily achieved things to do toward that ambition.  I also think that keeping the TODO-list in the forefront of ones mind and continuously checking things off the list is a great boost for productivity and keeping on track.  This culminates in the advice to keep momentum by doing something toward the plan on a daily basis, no matter how trivial.  This is sometimes called Jerry Seinfeld's secret to productivity: Just keep at it. Don't break the streak. &lt;a href=&quot;http://dirk.eddelbuettel.com/blog/2014/10/12/&quot;&gt;http://dirk.eddelbuettel.com/blog/2014/10/12/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So what was holding me back from a really useful daily publication of my open notebook?  I showed last post how I manage tasks in Emac Orgmode (a task organiser and calendar/agenda rolled up with code execution for running R scripts etc).  I also write my blog posts in orgmode.&lt;/p&gt;

&lt;p&gt;The only problem with that set up was that I was still using the code from Charlie Park &lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;http://charliepark.org/jekyll-with-plugins/&lt;/a&gt; which adds the inadequate commit description '&lt;code&gt;Latest build&lt;/code&gt;' every time.  What I needed was a way to actually log a summary of work each day, so I can look back over the history and know I actually did something everyday and was not just gaming the system by committing random little non-work additions (I want to balance this by doing &lt;em&gt;some&lt;/em&gt; work every day, but also take time off to read, exercise, socialize, and generally have fun).&lt;/p&gt;

&lt;p&gt;So anyway, the point of this post is to describe my revision to Charlie Park's code for building a jekyll blog:&lt;/p&gt;

&lt;h4&gt;Code: put in ~/.bash_profile&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;function bb() {
  cd ~/projects/ivanhanigan.github.com.raw &amp;amp;&amp;amp; jekyll b &amp;amp;&amp;amp; cp -r    
  ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com &amp;amp;&amp;amp; 
  cd ~/projects/ivanhanigan.github.com &amp;amp;&amp;amp; git add . -A  &amp;amp;&amp;amp; 
  git commit -m &quot;$*&quot; &amp;amp;&amp;amp; 
  git push
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;That bit about &lt;code&gt;$*&lt;/code&gt; was a bit difficult for me to get working as this is the first time I have written a bash script in anger.  The alternative was to use &lt;code&gt;$1&lt;/code&gt; and require the git commit message to be passed within quotes, which also makes sense but I did not do that.&lt;/li&gt;
&lt;li&gt;I also needed to change the terminal settings so that it always loads the bash_profile&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Bash terminal&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Edit &amp;gt; Profile preferences
Title and Command &amp;gt; Run command as a login shell 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;And so now I just have to deposit a markdown blog post into the jekyll &lt;code&gt;_posts&lt;/code&gt; folder and then&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Bash&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;bb Add a meaningful commit message about todays progress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;There you have it, a meaningful message regarding what I have been doing towards my scientific output every day.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seinfeld-streak-day9-1.png&quot; alt=&quot;/images/seinfeld-streak-day9.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seinfeld-streak-day9.png&quot; alt=&quot;/images/seinfeld-streak-day9.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Peng, R. (2015). The reproducibility crisis in science: 
A statistical counterattack. Significance, 12(3), 30–32. 
doi:10.1111/j.1740-9713.2015.00827.x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</description>
				<published>2015-09-22 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity/</link>
			</item>
		
			<item>
				<title>My Newnode R Function Useful For Causal Directed Acyclic Graphs (DAGs)</title>
				<description>&lt;h1&gt;Aims&lt;/h1&gt;

&lt;p&gt;I have worked on a function that turns a &lt;code&gt;data.frame&lt;/code&gt; into a graphviz code in the dot language, with some of my preferred settings.  I realised that it might be useful for causal directed acyclic graphs.&lt;/p&gt;

&lt;p&gt;Causal diagrams are useful for conceptualising the pathways of cause and effect.  These diagrams are sometimes simplly informal pictures but have also been developed in a more formal way to be used in modelling.  These formal developments use concepts derived from the mathmatical abstraction of Graphs (fundamentally Graphs  are networks of linked 'nodes', with the links being termed 'edges').  Causal diagrams can either be constructed to depict two things: first are feedback loops (a vexatious property of complex systems that confounds modelling) while second are more simple chain-of-events type pathways which proceed from an upstream cause to a downstream effect in a single direction, without cycles, called 'Directed Acyclic Graphs or DAGs.  The loop diagrams are out of the scope of this present blog post because the DAGs are much more easily addressed by the tool that I am describing.&lt;/p&gt;

&lt;p&gt;To begin I am going to build on this other guy's blog post on causal DAGs with R
  &lt;a href=&quot;http://donlelek.github.io/2015/03/31/dags-with-r/&quot;&gt;http://donlelek.github.io/2015/03/31/dags-with-r/&lt;/a&gt;
I wanted to add an interface for building these.&lt;/p&gt;

&lt;p&gt;Some background to the concepts that I use are provided in the references below.&lt;/p&gt;

&lt;h1&gt;Materials and Methods&lt;/h1&gt;

&lt;p&gt;The DiagrammeR package which has been integrated within R-Studio has made access to the graphing tool &lt;code&gt;graphviz&lt;/code&gt; much easier than it used to be.  My function &lt;code&gt;causal_dag&lt;/code&gt; (avaiable in my &lt;code&gt;disentangle&lt;/code&gt; github package) essentially constructs the required &lt;code&gt;nodes&lt;/code&gt; and &lt;code&gt;edges&lt;/code&gt; for that package to use.  Optionally we can also include &lt;code&gt;labels&lt;/code&gt; to indicate the direction of the effect.&lt;/p&gt;

&lt;p&gt;To use the tool all you need to do is create a list of &lt;code&gt;edges&lt;/code&gt; and their associated &lt;code&gt;inputs&lt;/code&gt; nodes and &lt;code&gt;outputs&lt;/code&gt; nodes (as a comma separated values string) shown in the picture below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/causal-ssheet.png&quot; alt=&quot;causal-ssheet.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# read in the sheet
library(disentangle)
library(stringr)
causes &amp;lt;- readxl::read_excel(&quot;causal-ssheet.xlsx&quot;)
causes
nodes &amp;lt;- newnode(causes, &quot;edges&quot;, &quot;inputs&quot;, &quot;outputs&quot;)
cat(nodes)
# The result is a formated graph in the dot language with some of my
# preferred settings such as edges showing as 'records' and a spot to
# write a description or include literature about each process
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;See the DOT code in the Appendix&lt;/li&gt;
&lt;li&gt;to render the graph now DiagrammeR can use this text string R object to render this to SVG&lt;/li&gt;
&lt;li&gt;I think it does not do PNG or PDF though so I still use graphviz and dot directly&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;grViz(nodes)

# But I also use graphviz directly to produce a publishable image in
# pdf or png
sink(&quot;reproduce-donlelek.dot&quot;)
cat(nodes)
sink()# If graphviz is installed and on linux call it with a shell command
#system(&quot;dot -Tpdf reproduce-donlelek.dot -o reproduce-donlelek.pdf&quot;)
system(&quot;dot -Tpng reproduce-donlelek.dot -o reproduce-donlelek.png&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Results&lt;/h1&gt;

&lt;p&gt;Here I have reproduced the work of donlelek&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/reproduce-donlelek.png&quot; alt=&quot;reproduce-donlelek.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Future directions&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I'd like to make the edges implicit, so that the spreadsheet keeps track of the information about the causal process, but the graph just shows the lines connecting the nodes&lt;/li&gt;
&lt;li&gt;The edges are where the action is, so I need to add a direction of effect.  This would be in a &lt;code&gt;label&lt;/code&gt; column and added in a [ label = 'abc' ] tag for each edge&lt;/li&gt;
&lt;li&gt;the rankdir option is LR to make this go sideways, which seems more the norm for causal DAGs, left to right.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;References&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Greenland, S., Pearl, J., &amp;amp; Robins, J. M. (1999). Causal diagrams for
epidemiologic research. Epidemiology (Cambridge, Mass.), 10(1),
37–48. doi:10.1097/00001648-199901000-00008

Reid, C. E., Snowden, J. M., Kontgis, C., &amp;amp; Tager, I. B. (2012). The
role of ambient ozone in epidemiologic studies of heat-related
mortality. Environmental Health Perspectives, 120(12),
1627–30. doi:10.1289/ehp.1205251

Newell, B., &amp;amp; Wasson, R. (2001). Social System vs Solar System: Why
Policy Makers Need History. In: Conflict and Cooperation related to
International Water Resources : Historical Perspectives. In World
Water (Vol. 2002).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Appendix&lt;/h1&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#####################################################################
# The following output is automatically created by newnode()
# NOTE for some reason, to show on the blog, I had to replace all { braces with normal (
#####################################################################
digraph transformations (

&quot;Metritis&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Cistic Ovarian Disease&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Age&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Fertility effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Fertility effects |  ) ))&quot;]
&quot;Fertility effects&quot; -&amp;gt; &quot;Fertility&quot;


&quot;Metritis&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Retained Placenta&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Age&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Cistic Ovarian effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Cistic Ovarian effects |  ) ))&quot;]
&quot;Cistic Ovarian effects&quot; -&amp;gt; &quot;Cistic Ovarian Disease&quot;


&quot;Retained Placenta&quot; -&amp;gt; &quot;Metritis effects&quot;
&quot;Metritis effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Metritis effects |  ) ))&quot;]
&quot;Metritis effects&quot; -&amp;gt; &quot;Metritis&quot;


 &quot;Age&quot; -&amp;gt; &quot;Retained Placenta effects&quot;
&quot;Retained Placenta effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Retained Placenta effects |  ) ))&quot;]
&quot;Retained Placenta effects&quot; -&amp;gt; &quot;Retained Placenta&quot;


 )
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-09-19 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/my-newnode-r-function-useful-for-causal-directed-acyclic-graphs/</link>
			</item>
		
			<item>
				<title>If You Don't Find A Solution In R, Keep Googling!</title>
				<description>&lt;p&gt;I've learnt this lesson multiple times. It happens like this.  A solution is not immediately obvious in R so you might think of writing your own function.  Generally there is a solution you just did not google enough.
This time I was tricked a little because the GIS functions have been bad for a long time but getting better very rapidly recently.  A little while ago I had a very successful
outcome from using the &lt;code&gt;raster::extract&lt;/code&gt; function on a large raster file
to get the attributes for a set of points.  I needed to do the same
thing but this time for a shapefile and points.  I looked at the
raster package and saw you can use the &lt;code&gt;raster::intersect&lt;/code&gt; function
here, and it worked on the small sample data I tested with but failed
with the big dataset as it ran out of memory.  I assumed that R had not caught up with the GIS world yet and so I came up with this workaround below by splitting the points data layer into chunks.&lt;/p&gt;

&lt;p&gt;I then got access to ArcMap and was wondering whether it could do it, and it DID!
So then I googled a bit and found the solution was simple:&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;sp::over()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Here is my hack in case I ever need to pull out the bit that does the splitting up of the points file, or the tryCatch():&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;big_pt_intersect &amp;lt;- function(pts, ply, chunks = 100){
  idx &amp;lt;- split(pts@data, 1:chunks)
  #str(idx)
  for(i in 1:length(idx)){
  #i = 1
  print(i)
    ids &amp;lt;- idx[ [i] ][,1]
  #str(pts@data)
  qc &amp;lt;- pts[pts@data[,1] %in% ids,]
  #str(qc)
  tryCatch(
    chunk &amp;lt;-  raster::intersect(qc, ply), 
    error = function(err){print(err)})
  if(!exists('chunk_out')){

    chunk_out &amp;lt;- chunk@data
  } else {
    chunk_out &amp;lt;- rbind(chunk_out, chunk@data)
  }
  rm(chunk)

  }
  #str(chunk_out)
  return(chunk_out)
}
# NB warning about split length multiple is not fatal, just due to nonequal chunks 
# (ie the geocodes are 2009/100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</description>
				<published>2015-09-17 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/if-you-dont-find-a-solution-in-r-keep-googling/</link>
			</item>
		
			<item>
				<title>Templates are Needed for Reproducible Research Reports (that Look Good)</title>
				<description>&lt;p&gt;I read with interest the the Transparency and Openness Promotion (TOP) Committee templates for guidelines to enhance transparency in the science that journals publish.&lt;/p&gt;

&lt;h4&gt;Citation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Supplementary Materials for Nosek, B. A., Alter, G., Banks, G. C.,
Borsboom, D., Bowman, S. D., Breckler, S. J., … Yarkoni,
T. (2015). Promoting an open research culture. Science, 348(6242),
1422–1425. doi:10.1126/science.aab2374
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I think though that guidelines like the suggestion to copy-paste bits of the manuscript leave a bit to be desired:&lt;/p&gt;

&lt;h4&gt;Quote;&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Authors document compliance by copy-pasting the relevant passages
in the paper that address the question into the form. For example,
when indicating how sample size was determined, authors copy paste
into the form the text in the paper that describes how sample size
was determined.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Reproducible Research Reports solve this problem by ensuring that the data preparation and analysis are executed in the same script that produces the manuscript, therefore a one-stop-shop for documentation of the entire study.&lt;/p&gt;

&lt;h2&gt;There is a need for Templates of Reproducible Research Reports (that look good!)&lt;/h2&gt;

&lt;p&gt;Rstudio provides very easy support for these documents if you use R.  In particular the option of a menu button to create a new report populates that report with the required header information and some example script to work off.  But the easiest option does not look so good.  This is the Rmarkdown option and it is very user friendly in terms of the markup language needed to write the descriptive language around your analysis (mostly plain text with a few simple options for heading styles etc) rather than the Sweave option which leads to the full blown LaTeX markup language that is a lot more complicated.&lt;/p&gt;

&lt;h4&gt;Boilerplate Rmarkdown header from Rstudio:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;---
title: &quot;Untitled&quot;
author: &quot;Ivan C. Hanigan&quot;
date: &quot;16 September 2015&quot;
output: html_document
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This is great for quick reporting of work as you go, but I  primarily write for output that will be printed (e.g. pdf docs). More specifically, I need the concept of a page, and to have full control over the placement of table and figure ‘environments’, stuff that is easy in LaTeX (once you figure out some of the esoteric parts of that language).&lt;/p&gt;

&lt;p&gt;To achieve a simple writing environment in Markdown but with the powerful layout options of LaTeX I reviewed this guys work but I think it takes it to an uneccessary level of complicated-ness
&lt;a href=&quot;https://github.com/jhollist/manuscriptPackage&quot;&gt;https://github.com/jhollist/manuscriptPackage&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So I went back to some of the old Sweave/Latex templates I had put together and ported it into a markdown header.&lt;/p&gt;

&lt;h4&gt;Boilerplate Rmarkdown header for pretty report&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;---
title: &quot;Untitled&quot;
author: &quot;Ivan C. Hanigan&quot;
date: &quot;16 September 2015&quot;
header-includes:
  - \usepackage{graphicx}
  - \usepackage{fancyhdr} 
  - \pagestyle{fancy} 
  - \usepackage{lastpage}
  - \usepackage{float} 
  - \floatstyle{boxed} 
  - \restylefloat{figure} 
  - \usepackage{url} 
  - \usepackage{color}
  - \lhead{Left Header}
  - \chead{Rmarkdown Rocks}
  - \rhead{\today}
  - \lfoot{Left Footer}
  - \cfoot{Centre Footer}
  - \rfoot{\thepage\ of \pageref{LastPage}}  
output: 
  pdf_document:
    toc: false
documentclass: article
classoption: a4paper
bibliography: references.bib
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Now the layout of tables and figures is done with latex&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Using the xtable package allows results to be displyed in tables
and has built in support for some R objects, so summrising the
linear fit above in ~\ref{ATable}

```{r, results='asis', type = 'tex'}
library(xtable)    
print(xtable(fit, caption=&quot;Example Table&quot;,
  digits=4,table.placement=&quot;ht&quot;,label=&quot;ATable&quot;), comment = F)    
```

## A Plot

Plots intergrate most easily if made seperately as can be seen in figure ~\ref{test}
```{r}
png(&quot;Rmarkdownfig.png&quot;)
plot(x,y,main=&quot;Example Plot&quot;,xlab=&quot;X Variable&quot;,ylab=&quot;Y Variable&quot;)
abline(fit,col=&quot;Red&quot;)
dev.off()
```
\begin{figure}[H]
\begin{center}
\includegraphics[width=.5\textwidth]{Rmarkdownfig.png}
\end{center}
\caption{Some Plot}
\label{test}
\end{figure}
\clearpage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I also realised that if this was to be a full report of a scientific study it would need to include some of the machinery needed for bibliographies.&lt;/p&gt;

&lt;h4&gt;Stuff for bibliographies&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;```{r, echo=F, results = 'hide', message = F, warning=F}
library(&quot;knitcitations&quot;)
library(&quot;bibtex&quot;)
cleanbib()
cite_options(citation_format = &quot;pandoc&quot;, check.entries = FALSE)

bib &amp;lt;- read.bibtex(&quot;C:/Users/Ivan/Dropbox/references/library.bib&quot;)

```

&amp;lt;!--Put data analysis and reporting here, then at the end of the doc--&amp;gt;

```{r, echo=F, message=F, eval=T}
write.bibtex(file=&quot;references.bib&quot;)
```

# References

&amp;lt;!--The bib will then be written following the final subheading--&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I hope this might help others develop their own templates for RRR that look great.&lt;/p&gt;
</description>
				<published>2015-09-16 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/templates-needed-for-reproducible-research-reports-that-look-good/</link>
			</item>
		
			<item>
				<title>task-management-like-an-open-science-hacker</title>
				<description>&lt;p&gt;I just read this impressive paper and it has really given me a push toward making this open lab notebook&lt;/p&gt;

&lt;h4&gt;Citation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Nosek, B. A., et al. (2015). Promoting an open research
culture. Science, 348(6242), 1422–1425. doi:10.1126/science.aab2374
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Quote&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The situation is a classic collective action problem. Many individual researchers lack
strong incentives to be more transparent, even though the credibility of science would 
benefit if everyone were more transparent.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;So I think I'll try to step up the pace of logging my daily scientific work.
One super easy thing to do is to publish my daily log from my task management in orgmode.
Indeed I am also reading at the moment this guy who says&lt;/p&gt;

&lt;h4&gt;Quote&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The core of your documentation is the research log.

Long, S. (2015). Reproducible Results and the Workflow of Data Analysis. 
Retrieved from http://www.indiana.edu/~jslsoc/ftp/WIM/wf wim 2015 2015-08-21@3.pdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Finally, I was struck by this reference &lt;a href=&quot;http://rich-iannone.github.io/about/2014/10/28/introduction.html&quot;&gt;http://rich-iannone.github.io/about/2014/10/28/introduction.html&lt;/a&gt; to something about 365+ day GitHub streaks. It was covered earlier by Geoff Greer, and by Dirk Eddelbuettel.&lt;/p&gt;

&lt;p&gt;It seems the basic concept is that you can leverage off an obsessive tendency by making sure you do something toward ticking off items from the task list every day.  The impulse to not breaking the chain is supposed to give you inspiration to keep going.  I think this might work well for my temperatment.&lt;/p&gt;

&lt;h2&gt;Emacs and orgmode&lt;/h2&gt;

&lt;p&gt;The set up of my daily log is pretty simple. After being set up by kjhealy's starter kit.
Then I modified the org-agenda-files which was set in the main el file that kjhealy provided  and then with the command C-c a a emacs will display my calendar.&lt;/p&gt;

&lt;p&gt;When I open emacs in the morning I  open the agenda and this also opens research-log file.  I move to that buffer, then I use this key command to insert a new entry for todays date&lt;/p&gt;

&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt; (define-skeleton org-journalentry
   &quot;Template for a journal entry.&quot;
   &quot;project:&quot;
   &quot;*** &quot; (format-time-string &quot;%Y-%m-%d %a&quot;) &quot; \n&quot;
   &quot;**** TODO-list \n&quot;
   &quot;***** TODO \n&quot;
   &quot;**** timesheet\n&quot;
   &quot;#+begin_src txt :tangle work-log.csv :eval no :padline no\n&quot;
   (format-time-string &quot;%Y-%m-%d %a&quot;) &quot;, &quot; str &quot;, 50\n&quot; 
   &quot;#+end_src\n&quot;
 )
 (global-set-key [C-S-f5] 'org-journalentry)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This creates a new date, a stub of a TODO for anything ad hoc and a entry into my timesheet.csv file.&lt;/p&gt;

&lt;p&gt;I then select from TODO items from a global list that I keep at the top of the file, and cut/paste them into todays list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/agenda.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Great so I just moved this research-log orgmode file into my blog github repo, and with the help of charlie park's bash script I am good to go&lt;/p&gt;

&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;alias build_blog=&quot;cd ~/projects/ivanhanigan.github.com.raw; jekyll b;
cp -r ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com;
cd ~/projects/ivanhanigan.github.com;git add .;git commit -am 'Latest build.';git push&quot;
alias bb=&quot;build_blog&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;So this will put the resulting changes onto my open lab book website here &lt;a href=&quot;https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org&quot;&gt;https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Things to note:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I found this list of tips &lt;a href=&quot;http://natashatherobot.com/streak-github-mistakes/&quot;&gt;http://natashatherobot.com/streak-github-mistakes/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In particular I realise I need to make my daily push by 4:50 PM in Canberra ACT as this is 11:50 PM the previous day for Github, Pacific Time (PT)&lt;/li&gt;
&lt;li&gt;I also will need to ensure I don't publish sensitive (or embarrasing entries).&lt;/li&gt;
&lt;li&gt;I'll try to keep the identity of my collaborators private as well, so just use their initials rather than names.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>2015-09-13 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/task-management-like-an-open-science-hacker/</link>
			</item>
		
			<item>
				<title>how-to-say-why-before-what</title>
				<description>&lt;p&gt;I have discovered a flaw in my writing style.&lt;br/&gt;
I often say what it it before I say why it is important.&lt;/p&gt;

&lt;h4&gt;Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Disentangling health effects of environmental from social factors
is difficult for a variety of reasons. The effort to examine and
to separate environmental and social causes is nevertheless
valuable. [WHY IS IT VALUABLE?] This is especially important to
policy makers and to others who seek to maximise the public
good. A greater understanding of their respective contributions
will lead to more rational, deep-seated, lasting and effective
interventions.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;The caps question is from someone reading my draft.  I need to start with the why.
Perhaps just turn the paragraph on its head?&lt;/p&gt;

&lt;h4&gt;Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;A greater understanding of the respective contributions from
environmental and social factors will lead to more rational,
deep-seated, lasting and effective interventions by policy makers
and to others who seek to maximise the public good.  Disentangling
health effects of environmental from social factors is difficult
for a variety of reasons. The effort to examine and to separate
environmental and social causes is nevertheless valuable.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</description>
				<published>2015-09-03 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/how-to-say-why-before-what/</link>
			</item>
		
			<item>
				<title>tracking-a-data-analysis-pipeline</title>
				<description>&lt;p&gt;I have just uploaded a new version of the windows build for my 'disentangle' package.  The blurb of the  draft vignette is below.&lt;/p&gt;

&lt;h1&gt;Introduction&lt;/h1&gt;

&lt;p&gt;It can be much easier to conceptually understand a complicated data
analysis pipeline than it is to implement that pipeline effectively.
This report outlines the use of the 'disentangle' R package, available from &lt;a href=&quot;http://ivanhanigan.github.io/projects.html&quot;&gt;http://ivanhanigan.github.io/projects.html&lt;/a&gt;.  This package contains functions that were developed to aid data
analysts to map out all the aspects of their work when planning and
conducting complicated data analyses using the pipeline concept.    There are often many steps in the design and analysis of a study and
when these are put together as a data analysis pipeline this addresses
the challenge of reproducibility (Peng 2006).  The
credibility of data analyses requires that every step is able to be
scrutinised (Leek 2015).&lt;/p&gt;

&lt;h2&gt;Motivating scientific questions&lt;/h2&gt;

&lt;p&gt;The type of data analysis that is
the focus of this work is more complicated than simply loading some
data that are already cleaned, fitting some models and reporting some
output.  Typically the type of data analysis projects that these tools
are aimed at involve attempts to control for a large number
of inter-relationships and associations between variables. It is
especially problematic that these variables need to have been selected
by the scientists from a multitude of possible variables and a
plethora of possible data sources, during a long process of data
collection, cleaning, exploration and decision making in preparation
for data analysis. There are also a multitude of steps and decision
points in the process of model building and model checking. The use of
statistical models involving many entangled environmental and social
variables can easily result in spurious association that may be
mistakenly interpreted as causation.  Projects that the author has
been involved in include explorations of hypotheses about health effects from
droughts, bushfire smoke, heat-waves and dust-storms which produced
novel findings, and informed controversial debates about the
implications of climate change. The requirement to adequately convey
the methods and results of this research was problematic and motivated
the work on effective use of reproducible research techniques and data
analysis pipelines.&lt;/p&gt;
</description>
				<published>2015-08-28 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/08/tracking-a-data-analysis-pipeline/</link>
			</item>
		
			<item>
				<title>Web Data, Climate Grids and THREDDS UPDATE</title>
				<description>&lt;h1&gt;Update&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;This is an update to my previous post:  &lt;a href=&quot;http://ivanhanigan.github.io/2014/12/climate-grids-and-thredds-server-experimenting/&quot;&gt;http://ivanhanigan.github.io/2014/12/climate-grids-and-thredds-server-experimenting/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I was worried about the available metadata with those netcdf data I was working with so contacted Dr Brad Evans at EMAST who set me straight:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;QUOTE:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;&quot;you need to use ncdf4  - not ncdf  ...
because they are netcdf4 files and not netcdf3 files. 
This is poorly explained in the R community.
All of the netcdf files from Australian providers (AusCover, BoM, etc...) 
have been in netcdf4 for a couple of years now.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;So here is the revised code using ncdf4 library:&lt;/h1&gt;

&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# sudo apt-get install r-cran-ncdf4 
library(ncdf4)
library(raster)
strt &amp;lt;-'2012-01-01'
end &amp;lt;- '2012-01-04'
dates &amp;lt;- seq(as.Date(strt),as.Date(end),1)          
dates
par(mfrow = c(2,2))
for(i in 1:length(dates)){
 # i=1
  date_i &amp;lt;- dates[i]
  infile &amp;lt;- sprintf(&quot;http://dapds00.nci.org.au/thredds/dodsC/rr9/Climate/eMAST/ANUClimate/0_01deg/v1m0_aus/day/land/tmin/e_01/2012/eMAST_ANUClimate_day_tmin_v1m0_%s.nc&quot;, gsub(&quot;-&quot;, &quot;&quot;, date_i))

  nc &amp;lt;- nc_open(infile)
  str(nc)
  print(nc)
  vals &amp;lt;- ncvar_get(nc, varid=&quot;air_temperature&quot;)
  str(vals)
  nc.att &amp;lt;-    nc$var$air_temperature
  xmin &amp;lt;- min(nc.att$dim[[1]]$vals)
  xmax &amp;lt;- max(nc.att$dim[[1]]$vals)
  ymin &amp;lt;- min(nc.att$dim[[2]]$vals)
  ymax &amp;lt;- max(nc.att$dim[[2]]$vals)

  print(c(xmin,xmax))
  print(c(ymin,ymax))

  r &amp;lt;- raster(t(vals),
              xmn=xmin, xmx=xmax,
              ymn=ymin, ymx=ymax)
  #str(r)
  plot(r)
  nc_close(nc)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;RESULTS 1: METADATA&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;the result is I now have a lot more metdata returned to my R workspace&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;EXERPT&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;[...]
[1] &quot;        licence_copyright: Copyright 2009-2013 ANU. Rights owned by The Australian National University (ANU). Rights licensed subject to TERN  Attribution (TERN-BY).&quot;
[1] &quot;        short_desc: Australian coverage, ANUClimate 1.0, 0.01 degree grid, 1970-2012&quot;
[1] &quot;        summary: Minimum daily temperature, for the Australian continent between 1970-2012. Daily temperature regulates rates of plant growth and determines critical conditions such as frost on flowering and fruiting. Modelled by expressing each daily value as a difference anomaly with respect to the gridded 1976-2005 mean daily minimum temperature for each month as provided by eMAST_ANUClimate_mmn_tmin_v1m0_1976_2005. The daily anomalies were interpolated by trivariate thin plate smoothing spline functions of longitude, latitude and vertically exaggerated elevation using ANUSPLIN Version 4.5. There was an average of 671 Bureau of Meteorology data points available for each day between 1970 and 2012. Automated quality assessment rejected on average 3 data values per day with extreme studentised residuals. These were commonly associated with days following missing observations. The root mean square of all individual cross validation residuals provided by the spline analysis is 1.5 degrees Celsius. A comprehensive assessment of the analysis and the factors contributing to the quality of the final interpolated daily minimum temperature grids is in preparation.&quot;
[1] &quot;        long_name: Daily minimum temperature&quot;
[1] &quot;        contact: Michael Hutchinson, Professor of spatial and temporal analysis, 3.23A, Fenner School of Environment &amp;amp; Society, College of Medicine, Biology &amp;amp; Environment, Frank Fenner Building 141, Australian National University, Canberra, Australian Capital Territory, 200, Australia, (+61) 2 6125 4783, Michael.Hutchinson@anu.edu.au, http://orcid.org/0000-0001-8205-6689&quot;
[1] &quot;        references: 1. Hutchinson, M.F., Mckenney, D.W., Lawrence, K., Pedlar, J., Hopkinson, R., Milewska, E. and Papadopol, P. 2009. Development and testing of Canada-wide interpolated spatial models of daily minimum/maximum temperature and precipitation for 1961-2003. Journal of Applied Meteorology and Climatology 48: 725�741. http://dx.doi.org/10.1175/2008JAMC1979.1 2. Hutchinson, M.F. and Xu, T. 2013. ANUSPLIN version 4.4 User Guide. Fenner School of Environment and Society, Australian National University, Canberra http://fennerschool.anu.edu.au/files/anusplin44.pdf&quot;
[1] &quot;        source: ANUClimate 1.0&quot;
[1] &quot;        keywords: EARTH SCIENCE &amp;gt; ATMOSPHERE &amp;gt; ATMOSPHERIC TEMPERATURE &amp;gt; MAXIMUM/MINIMUM TEMPERATURE&quot;
[1] &quot;        Conventions: CF-1.6&quot;
[1] &quot;        institution: Australian National University&quot;
[1] &quot;        geospatial_lat_min: -43.74&quot;
[1] &quot;        geospatial_lat_max: -9&quot;
[1] &quot;        geospatial_lat_units: degrees_north&quot;
[1] &quot;        geospatial_lat_resolution: -0.01&quot;
[1] &quot;        geospatial_lon_min: 112.9&quot;
[1] &quot;        geospatial_lon_max: 154&quot;
[1] &quot;        geospatial_lon_units: degrees_east&quot;
[1] &quot;        geospatial_lon_resolution: 0.01&quot;
[1] &quot;        keywords_vocabulary: Global Change Master Directory (http://gcmd.nasa.gov)&quot;
[1] &quot;        metadata_link: http://datamgt.nci.org.au:8080/geonetwork&quot;
[1] &quot;        standard_name_vocabulary: Climate and Forecast(CF) convention standard names (http://cf-pcmdi.llnl.gov/documents/cf-standard-names)&quot;
[1] &quot;        id: eMAST_ANUClimate_day_tmin_v1m0_1970_2012&quot;
[1] &quot;        DOI: To be added&quot;
[1] &quot;        cdm_data_type: grid&quot;
[1] &quot;        contributor_name: Michael Hutchinson, Jennnifer Kesteven, Tingbao Xu&quot;
[1] &quot;        contributor_role: principalInvestigator, author, author&quot;
[1] &quot;        creator_email: eMAST.data@mq.edu.au&quot;
[1] &quot;        creator_name: eMAST data manager&quot;
[1] &quot;        creator_url: http://www.emast.org.au/&quot;
[1] &quot;        Metadata_Conventions: Unidata Dataset Discovery v1.0&quot;
[1] &quot;        publisher_name: Ecosystem Modelling and Scaling Infrastructure (eMAST) Facility: Macquarie University&quot;
[1] &quot;        publisher_email: eMAST.data@mq.edu.au&quot;
[1] &quot;        publisher_url: http://www.emast.org.au/&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;RESULTS 2: Grid data&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I still get lots of good data&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/thredds2.png&quot; alt=&quot;/images/thredds2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;NOTE I still need that weird transpose&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I note that the weird hacky transpose is still required&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# NB weird hacky transpose still required or else you get this
r &amp;lt;- raster(vals,
            xmn=xmin, xmx=xmax,
            ymn=ymin, ymx=ymax)

#str(r)
plot(r)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/thredds2raw.png&quot; alt=&quot;/images/thredds2raw.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-07-27 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/07/climate-grids-and-thredds-server-experimenting-update/</link>
			</item>
		
			<item>
				<title>Using the R EML software to mitigate risks in Morpho and Metacat data publishing</title>
				<description>&lt;h1&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Over the last few months I have used software called Metacat as a Data Portal and Repository.  Metacat is server software which has been
developed by the Knowledge Network for Biocomplexity (KNB).&lt;/li&gt;
&lt;li&gt;Metacat
conforms to the Ecological Metadata Language (EML) Standard (&lt;a href=&quot;https://knb.ecoinformatics.org/#external//emlparser/docs/index.html&quot;&gt;https://knb.ecoinformatics.org/#external//emlparser/docs/index.html&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;KNB also develop another software package called Morpho to be used by Ecologists to document their data (&lt;a href=&quot;https://knb.ecoinformatics.org/#tools&quot;&gt;https://knb.ecoinformatics.org/#tools/morpho&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Morpho can be used to send the data and metadata documents to be published on a Metacat portal.&lt;/li&gt;
&lt;li&gt;KNB’s software is used internationally by the Data
Observation Network for Earth (DataONE) nodes, the United States Long
Term Ecological Research (US LTER) network and the International Long
Term Ecological Research (ILTER) network.&lt;/li&gt;
&lt;li&gt;Additionally, the Australian Long Term Ecological Research Network
Data Portal (&lt;a href=&quot;http://www.ltern.org.au/knb/&quot;&gt;www.ltern.org.au/knb/&lt;/a&gt;), Australian SuperSites Network and
Australian Centre for Ecological Analysis and Synthesis used
the same underlying technology to publish data packages.&lt;/li&gt;
&lt;li&gt;The Metacat system is great for a data repository but unfortunately (in my experience) the Morpho software package has repeatedly hampered data processing and increased risks of inadvertently publishing data with errors.&lt;/li&gt;
&lt;li&gt;My colleagues and I workaround these problems using a lot of different 'fixes' for the different problems.&lt;/li&gt;
&lt;li&gt;Fortunately there is an alternative to Morpho in the R statistical software environment called the R-EML package (&lt;a href=&quot;https://github.com/ropensci/EML&quot;&gt;https://github.com/ropensci/EML&lt;/a&gt;).  This provides a library of functions used in the R language to generate and parse EML files.&lt;/li&gt;
&lt;li&gt;This new workflow mitigates some of the risks of the Morpho software by ensuring the data related steps of the workflow are conducted in the R environment for statical computing.&lt;/li&gt;
&lt;li&gt;However, some Issues remain in that this requires a fairly specialised computing environment with various Linux libraries configured appropriately&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Results&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I generate EML metadata using REML in the workflow shown in the figure below.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/workflow-rmd-md.png&quot; alt=&quot;altext&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Image adapted from &lt;a href=&quot;http://kieranhealy.org/blog/archives/2014/01/23/plain-text/&quot;&gt;http://kieranhealy.org/blog/archives/2014/01/23/plain-text/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>2015-03-19 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/03/r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing/</link>
			</item>
		
	</channel>
</rss>