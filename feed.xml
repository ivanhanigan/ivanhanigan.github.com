<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>Recology</title>
		<description>An exploration of using R for ecology, evolution, and open science.</description>
		<link>http://schamberlain.github.com</link>
		
			<item>
				<title>Daintree Rainforest Observatory Climate Data from AWAP-GRIDS</title>
				<description>&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;AWAP GRIDS &lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Introduction and Methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 Material and Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 R-code-for-extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3&quot;&gt;3 Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-4&quot;&gt;4 R-code-for-comparison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-5&quot;&gt;5 Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; Introduction and Methods&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;


&lt;p&gt;
This is a work in progress.  It is a stub of an article I want to put together which shows how to use several online data repositories together as a showcase of the [Scientific Workflow and Integration Software for Health (SWISH) Climate Impact Assessments](&lt;a href=&quot;https://github.com/swish-climate-impact-assessment&quot;&gt;https://github.com/swish-climate-impact-assessment&lt;/a&gt;) project.
&lt;/p&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1&lt;/span&gt; Authors&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1&quot;&gt;

&lt;ul&gt;
&lt;li&gt;Ivan Hanigan and [Markus Nolf](&lt;a href=&quot;http://www.thinkoholic.com&quot;&gt;http://www.thinkoholic.com&lt;/a&gt;)
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2&lt;/span&gt; Background&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2&quot;&gt;


&lt;ul&gt;
&lt;li&gt;Markus Nolf offers this use case of the [SWISH EWEDB](&lt;a href=&quot;http://swish-climate-impact-assessment.github.io/&quot;&gt;http://swish-climate-impact-assessment.github.io/&lt;/a&gt;)
&lt;/li&gt;
&lt;li&gt;Markus is pulling together his  Daintree Rainforest Observatory (DRO) data into a manuscript for publication, and was looking for climate data from 2012 as well as long-term. 
&lt;/li&gt;
&lt;li&gt;More specifically, the annual precipitation and mean annual temperature for both 2012 and the 30-year mean.
&lt;/li&gt;
&lt;li&gt;The Australian Bureau of Meteorology has a nice rainfall dataset available at &lt;a href=&quot;http://www.bom.gov.au/climate/data/&quot;&gt;http://www.bom.gov.au/climate/data/&lt;/a&gt; (&quot;Cape Trib Store&quot; weather station), but it seems like the temperature records are patchy.
&lt;/li&gt;
&lt;li&gt;So it is advised to use the data the DRO collects its self
&lt;/li&gt;
&lt;li&gt;You need to apply through the [ASN SuperSite data portal](&lt;a href=&quot;http://www.tern-supersites.net.au/knb/&quot;&gt;http://www.tern-supersites.net.au/knb/&lt;/a&gt;) for access to the daily data for the DRO.
&lt;/li&gt;
&lt;li&gt;Note the use of the DRO met data will need to be properly cited as it is harder to keep
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;an AWS station running in the tropics for years than it is to collect most other data. 
The citation information is provided when you make a request to access the data.
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;The long term mean used by most DRO researchers is from the BOM station as we only have a short record from the station itself.  The offset is around 1000mm.
&lt;/li&gt;
&lt;li&gt;However what we want is mean annual temperatures but the BOM website seems to focus more on mean minimum and maximum temperatures.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3&lt;/span&gt; Material and Methods&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3&quot;&gt;


&lt;ul&gt;
&lt;li id=&quot;sec-1-3-1&quot;&gt;Baseline Climate Data 2012, Far North Queensland Rainforest Supersite, Cape Tribulation Node&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;We can use the data portal too see [the data file in question](&lt;a href=&quot;http://www.tern-supersites.net.au/knb/metacat/lloyd.238.13/html&quot;&gt;http://www.tern-supersites.net.au/knb/metacat/lloyd.238.13/html&lt;/a&gt;)
&lt;/li&gt;
&lt;li&gt;Application for access is via email
&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-3-2&quot;&gt;Extract mean annual temperatures at the BOM website&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;SWISH uses BoM data a fair bit and aims to streamline access to BoM data for extreme weather event analysis (which require long term average climatology to provide the baseline that extremes are measured against).
&lt;/li&gt;
&lt;li&gt;WRT to temperature most daily averages from BoM are calculated as average of maximum&lt;sub&gt;temperature&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;24&lt;/sub&gt;&lt;sub&gt;hours&lt;/sub&gt;&lt;sub&gt;after&lt;/sub&gt;&lt;sub&gt;9am&lt;/sub&gt;&lt;sub&gt;local&lt;/sub&gt;&lt;sub&gt;time&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;degrees&lt;/sub&gt; and minimum&lt;sub&gt;temperature&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;24&lt;/sub&gt;&lt;sub&gt;hours&lt;/sub&gt;&lt;sub&gt;before&lt;/sub&gt;&lt;sub&gt;9am&lt;/sub&gt;&lt;sub&gt;local&lt;/sub&gt;&lt;sub&gt;time&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;degree&lt;/sub&gt; (only couple of hundred AWS provide hourly data to get the proper mean of 24 obs).
&lt;/li&gt;
&lt;li&gt;The Bureau of Meteorology has generated a range of gridded meteorological datasets for Australia as a contribution to the Australian Water Availability Project (AWAP). These include daily max and min temperature which you could use to generate daily averages, then calculate your long term averages from those?  
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.bom.gov.au/jsp/awap/&quot;&gt;http://www.bom.gov.au/jsp/awap/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Documentation is at &lt;a href=&quot;http://www.bom.gov.au/amm/docs/2009/jones.pdf&quot;&gt;http://www.bom.gov.au/amm/docs/2009/jones.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-3-3&quot;&gt;A workflow to download and process the public BoM weather grids.&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;This workflow uses the open source R software with some of our custom written packages:
&lt;/li&gt;
&lt;/ul&gt;



&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;2&lt;/span&gt; R-code-for-extraction&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-2&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;name:r-code&lt;/span&gt;



&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;aim daily weather for any point location from online BoM weather grids&lt;/span&gt;

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;depends on some github packages&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(awaptools)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;http://swish-climate-impact-assessment.github.io/tools/awaptools/awaptools-downloads.html&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(swishdbtools)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools-downloads.html&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(gisviz)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;http://ivanhanigan.github.io/gisviz/&lt;/span&gt;

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;and this from CRAN&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(raster)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'raster'&lt;/span&gt;); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(raster)

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;get weather data, beware that each grid is a couple of megabytes&lt;/span&gt;
vars &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;maxave&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;minave&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;totals&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;vprph09&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;vprph15&quot;&lt;/span&gt;) &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;,&quot;solarave&quot;) &lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;solar only available after 1990&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(measure &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; vars)
{
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;measure &amp;lt;- vars[1]&lt;/span&gt;
  get_awap_data(start = &lt;span style=&quot;color: #2aa198;&quot;&gt;'1960-01-01'&lt;/span&gt;,end = &lt;span style=&quot;color: #2aa198;&quot;&gt;'1960-01-02'&lt;/span&gt;, measure)
}

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;get location&lt;/span&gt;
locn &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; geocode(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;daintree rainforest&quot;&lt;/span&gt;)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;this uses google maps API, better check this&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;lon       lat&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;1 145.4185 -16.17003&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;Treat data frame as spatial points&lt;/span&gt;
epsg &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; make_EPSG()
shp &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
                              proj4string=CRS(epsg$prj4[epsg$code %&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt;% &lt;span style=&quot;color: #2aa198;&quot;&gt;'4283'&lt;/span&gt;]))
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;now loop over grids and extract met data&lt;/span&gt;
cfiles &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt;  dir(pattern=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;grid$&quot;&lt;/span&gt;)

&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt; (i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; seq_len(length(cfiles))) {
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;i &amp;lt;- 1 ## for stepping thru&lt;/span&gt;
  gridname &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; cfiles[[i]]
  r &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; raster(gridname)
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;image(r) # plot to look at&lt;/span&gt;
  e &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; extract(r, shp, df=T)
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;str(e) ## print for debugging&lt;/span&gt;
  e1 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; shp
  e1@data$values &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; e[,2]
  e1@data$gridname &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; gridname
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;write to to target file&lt;/span&gt;
  write.table(e1@data,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;output.csv&quot;&lt;/span&gt;,
    col.names = i == 1, append = i&amp;gt;1 , sep = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;,&quot;&lt;/span&gt;, row.names = &lt;span style=&quot;color: #b58900;&quot;&gt;FALSE&lt;/span&gt;)
}

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;further work is required to format the column with the gridname to get out the date and weather paramaters.&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;3&lt;/span&gt; Results&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-3&quot;&gt;

&lt;ul&gt;
&lt;li id=&quot;sec-3-1&quot;&gt;Results&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;Markus reports:
&lt;/li&gt;
&lt;li&gt;&quot;The R-script worked great once i had set a working directory that did not include spaces. (It may have been a different problem that got solved by changing the wd, but the important thing is it's running now.)&quot;
&lt;/li&gt;
&lt;li&gt;Markus downloaded 70+ GB of gridded weather data from the BoM website to his local computer
&lt;/li&gt;
&lt;li&gt;Also note there is another set of gridded data available from the BOM, which contains pre-computed longterm mean temps, [ready to be extracted with the script](&lt;a href=&quot;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&quot;&gt;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&lt;/a&gt;)
&lt;/li&gt;
&lt;li&gt;&quot;Using this file, I only needed to get the 2012 temp grids for a comparison of 2012 vs. 30-year data. I'm going to run the extraction of 1961-1990 data, just to be sure.&quot;
&lt;/li&gt;
&lt;li&gt;&quot;When we finished analysis of the long-term temperature from daily means found:
&lt;/li&gt;
&lt;li&gt;While the official, pre-computed long-term mean (i.e. 30-year grid file, analysed with your script) was 22.29 °C for the DRO coordinates (145.4494, -16.1041), the new value from daily means (i.e. daily minave and maxave averaged) is 24.91 °C.
&lt;/li&gt;
&lt;li&gt;We're not sure what causes this discrepancy, but thought we'd note that there is one.
&lt;/li&gt;
&lt;li&gt;For Markus' manuscript, the fact that means from daily min/max were practially identical in 1961-1990 and 2012 is enough.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-3-2&quot;&gt;Dataset discrepancy&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;Following up on the interesting a difference between the two BoM datasets. 
&lt;/li&gt;
&lt;li&gt;One thing that might cause this might be if you are calculating the average of the annual averages ie sum(annavs)/30 or the average of all the daily averages as sum(dailyavs)/(30 * 365 or 366)?  the variance will differ by these methods.
&lt;/li&gt;
&lt;li&gt;looks like the 30 year dataset is the former:
&lt;/li&gt;
&lt;li&gt;&quot;Average annual temperatures (maximum, minimum or mean) are calculated by adding daily temperature values each year, dividing by the number of days in that year to get an average for that particular year. The average values for each year in a specified period (1961 to 1990) are added together and the final value is calculated by dividing by the number of years in the period (30 years in this case).&quot;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[metadata](&lt;a href=&quot;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&quot;&gt;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&lt;/a&gt;)
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Markus followed the BOM calculation method, and just compared it with two other approaches.
&lt;/li&gt;
&lt;li&gt;average of all 21914 values
&lt;/li&gt;
&lt;li&gt;average of yearly sum(min and max values per year)/(ndays*2)
&lt;/li&gt;
&lt;li&gt;average of yearly sum(daily average)/ndays)
&lt;/li&gt;
&lt;li&gt;where ndays = number of days per year.
&lt;/li&gt;
&lt;li&gt;Differences between these methods show only in the 6th decimal place, far from 2.62 degrees.
&lt;/li&gt;
&lt;/ul&gt;



&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-4&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;4&lt;/span&gt; R-code-for-comparison&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;This is Markus' comparison script &lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;also see the formatted table csv, as well as the SWISH script's raw output csv&lt;/span&gt;

setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;E:\\markus\\ph.d\\aus-daintree\\data-analysis\\climate&quot;&lt;/span&gt;)
climate &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;minmaxave-30year-daily.csv&quot;&lt;/span&gt;, sep=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;,&quot;&lt;/span&gt;, dec=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;.&quot;&lt;/span&gt;)

climate$year &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(climate$file,1,4)
climate$dailymean &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; (climate$maxave+climate$minave)/2
head(climate)


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;total average across all days and values&lt;/span&gt;
annmean &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; mean(c(climate$maxave,climate$minave))
annmean


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;daily means averaged by year, then total average&lt;/span&gt;
annmean1 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(1,2)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:30) {
        annmean1[i] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; mean(climate[climate$year==(i+1960),]$dailymean)
        &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;print(annmean1[i])&lt;/span&gt;
}
annmean1
mean(annmean1)


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;mean of all values per year, then total average&lt;/span&gt;
annmean2 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(1,2)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:30) {
        tmpdata &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; climate[climate$year==(i+1960),]
        annmean2[i] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; (sum(tmpdata$maxave) + sum(tmpdata$minave))/(length(tmpdata$maxave)+length(tmpdata$minave))
        &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;print(annmean2[i])&lt;/span&gt;
}
annmean2; mean(annmean2)


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;differences&lt;/span&gt;
annmean - mean(annmean1)
annmean - mean(annmean2)
mean(annmean1) - mean(annmean2)


&lt;/pre&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-5&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-5&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;5&lt;/span&gt; Discussion&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-5&quot;&gt;


&lt;ul&gt;
&lt;li&gt;Principal findings 
&lt;/li&gt;
&lt;li&gt;Strengths 
&lt;/li&gt;
&lt;li&gt;Weaknesses 
&lt;/li&gt;
&lt;li&gt;Comparison with other studies 
&lt;/li&gt;
&lt;li&gt;What do the results mean? 
&lt;/li&gt;
&lt;li&gt;What are the policy implications?
&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
&lt;li id=&quot;sec-5-1&quot;&gt;Conclusion&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;So what is the main 'take-home' message?
&lt;/li&gt;
&lt;li&gt;How do you rate the approach overall?
&lt;/li&gt;
&lt;/ul&gt;



&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</description>
				<published>Sat Oct 26 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/extract-weather-data-from-awap-grids/</link>
			</item>
		
			<item>
				<title>document-first-ask-questions-later</title>
				<description>&lt;p&gt;This post is just a short note about something I'm thinking of calling &quot;documentation-driven development&quot;.
It is based on the concept of &lt;a href=&quot;http://en.wikipedia.org/wiki/Test-driven_development&quot;&gt;&quot;test-driven development&quot;&lt;/a&gt;, and more recently:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://lamages.blogspot.in/2013/04/test-driven-analysis.html&quot;&gt;&quot;test-driven analysis&quot;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;or even &lt;a href=&quot;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&quot;&gt;&quot;Evidence-based Data Analysis&quot;&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;It is also a kind of a critique on the paradigm suggested by the BCCVL statement on &lt;a href=&quot;http://bccvl.org.au/blog/2013/08/20/just-in-time-metadata/&quot;&gt;&quot;Just-In-Time metadata&quot;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Anyway, it is a small thing but hopefully big things will grow.&lt;/p&gt;
</description>
				<published>Fri Oct 25 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/document-first-ask-questions-later/</link>
			</item>
		
			<item>
				<title>A Great Intro 2 Logistic Regression</title>
				<description>&lt;p&gt;This is a great example of logistic regression,  because it is pretty simple but covers good ground.  I got it from Peter Caley;s R tutorial workbook from Charles Darwin School of Environmental Research.&lt;/p&gt;

&lt;p&gt;It is also a tragic example of the impact weather can have on health.&lt;br/&gt;
The colder it is the more likely the shuttle is to explode.&lt;/p&gt;

&lt;p&gt;The problem was with the failure rate (and number of) O-rings that failed (n.fail) related to the temperature (temp).&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#Load the data
#The following R code will construct the dataset
n.fail &amp;lt;- c(2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0)
temp &amp;lt;- c(53, 66, 68, 70, 75, 78, 57, 67, 69, 70, 75, 79, 58, 67, 70, 72, 76, 81, 63, 67, 70, 73, 76)
# there were 6 o rings for each of 23 attempts
total &amp;lt;- rep(6,23)
# probability of fail
p.fail &amp;lt;- n.fail/total
# Response = resp column bind them together  
resp &amp;lt;- cbind(n.fail, total-n.fail)

###########################################################################
# we can write text files easily once the data frame or matrix is in shape
data &amp;lt;- as.data.frame(cbind(resp,temp))
names(data) &amp;lt;- c('nfail','totalMinusNfail', 'temp')
# write.csv(data, 'learnR-logistic-data.csv', row.names=F)

###########################################################################
# and read it in again 
# data2 &amp;lt;- read.csv('learnR-logistic-data.csv')

################################################################
# name:learnR-logistic
png('images/pfail.png')
plot(temp, p.fail, pch=16, xlim=c(40,100), ylim=c(0,0.4))
title('A plot of the proportion failed by temperature')
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/pfail.png&quot; alt=&quot;pfail.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;###########################################################################
# newnode: linear
linear &amp;lt;- glm(resp ~ 1 + temp, family=binomial(link=logit))
summary(linear)
linearoutput &amp;lt;- summary(linear)
linearoutput$coeff

###########################################################################
# newnode: learnR-logistic
cf &amp;lt;- linearoutput$coeff
signif(cf[which(row.names(cf) == 'temp'),'Estimate'],2)

###########################################################################
# newnode: learnR-logistic
# write.csv(linearoutput$coeff,&quot;challengerOfails.csv&quot;)

###########################################################################
# newnode: learnR-logistic
 png('images/challengerLogistic.png')
 par(mfrow=c(2,2))
 plot(linear)
 dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/challengerLogistic.png&quot; alt=&quot;challengerLogistic.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;####################################################################
# newnode: learnR-logistic
dummy &amp;lt;- data.frame(temp=seq(20,100,1))
pred.prob &amp;lt;- predict.glm(linear, newdata=dummy, type=&quot;resp&quot;)
png('images/pfailfit.png')
plot(temp, p.fail, xlab=&quot;Launch Temperature (F)&quot;,
 ylab=&quot;Proportion Failing&quot;, pch=16, xlim=c(20,100), ylim=c(0,1.0))
lines(dummy$temp, pred.prob)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/pfailfit.png&quot; alt=&quot;pfailfit.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;####################################################################
resp &amp;lt;- as.data.frame(resp)
resp$fail &amp;lt;- ifelse(resp$n.fail &amp;gt; 0, 1, 0)
resp$temp &amp;lt;- temp

png('images/fail.png')
with(resp, plot(temp, fail, xlab=&quot;Launch Temperature (F)&quot;,ylab=&quot;Joint damage&quot;, pch=16, xlim=c(50,80), ylim=c(0,1.0))
     )
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/fail.png&quot; alt=&quot;fail.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;chal.logit &amp;lt;- glm(fail~temp,family=binomial, data = resp)
summary(chal.logit)$coeff

png('images/pfailfit2.png')
cx &amp;lt;- c(50:80/1)
cyhat &amp;lt;- coefficients(chal.logit)[c(1)] +
coefficients(chal.logit)[c(2)]*cx
cpihat &amp;lt;- exp(cyhat)/(1+exp(cyhat))
with(resp,plot(temp,fail,xlab=&quot;Temperature&quot;,ylab=&quot;Damage&quot;,
main=&quot;Incidence of Booster Field Joint Damage vs. Temperature&quot;, xlim = c(50,80))
     )
lines(cx,cpihat)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/pfailfit2.png&quot; alt=&quot;pfailfit2.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>Fri Oct 18 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/challenger-logistic/</link>
			</item>
		
			<item>
				<title>spatially-structured-time-series-with-nmmaps</title>
				<description>&lt;p&gt;I will use the NMMAPSlite datasets for a simple example of what I
describe as &quot;Spatially Structured Timeseries&quot; as opposed to
&quot;Spatio-Temporal&quot; which I think more explicitly includes spatial
structure in the model.  &lt;a href=&quot;http://ivanhanigan.github.io/spatiotemporal-regression-models/&quot;&gt;See This Report&lt;/a&gt; for all the gory details.&lt;/p&gt;

&lt;h1&gt;R Codes&lt;/h1&gt;

&lt;!-- &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; --&gt;


&lt;!-- &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Strict//EN&quot; --&gt;


&lt;!--                &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd&quot;&gt; --&gt;


&lt;!-- &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;en&quot; xml:lang=&quot;en&quot;&gt; --&gt;


&lt;p&gt;&lt;head&gt;
&lt;title&gt;Spatiotemporal Regression Modelling&lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;Spatiotemporal Regression Modelling&quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-10-16T15:17+1100&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Spatiotemporal Regression Modelling&lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Core Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 Core Model Plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1&lt;/span&gt; Core Model&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #5F7F5F;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;name:core&lt;/span&gt;
&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;func&lt;/span&gt;
setwd(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;~/projects/spatiotemporal-regression-models/NMMAPS-example&quot;&lt;/span&gt;)
&lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;require&lt;/span&gt;(mgcv)
&lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;require&lt;/span&gt;(splines)

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;load&lt;/span&gt;
analyte &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;analyte.csv&quot;&lt;/span&gt;)

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;clean&lt;/span&gt;
analyte$yy &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(analyte$date,1,4)
numYears&lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt;length(names(table(analyte$yy)))
analyte$date &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; as.Date(analyte$date)
analyte$time &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; as.numeric(analyte$date)
analyte$agecat &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; factor(analyte$agecat,
                          levels = c(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;under65&quot;&lt;/span&gt;,
                              &lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;65to74&quot;&lt;/span&gt;, &lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;75p&quot;&lt;/span&gt;),
                          ordered = &lt;span style=&quot;color: #7CB8BB;&quot;&gt;TRUE&lt;/span&gt;
                          )

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;do&lt;/span&gt;
fit &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; gam(cvd ~ s(tmax) + s(dptp) +
           city + agecat +
           s(time, k= 7*numYears, fx=T) +
           offset(log(pop)),
           data = analyte, family = poisson
           )

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;plot of response functions&lt;/span&gt;
png(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;images/nmmaps-eg-core.png&quot;&lt;/span&gt;, width = 1000, height = 750, res = 150)
par(mfrow=c(2,3))
plot(fit, all.terms = &lt;span style=&quot;color: #7CB8BB;&quot;&gt;TRUE&lt;/span&gt;)
dev.off()


&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;2&lt;/span&gt; Core Model Plots&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-2&quot;&gt;

&lt;p&gt;&lt;img src=&quot;/images/nmmaps-eg-core.png&quot;  alt=&quot;/images/nmmaps-eg-core.png&quot; /&gt;
&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</description>
				<published>Wed Oct 16 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/spatially-structured-time-series-with-nmmaps/</link>
			</item>
		
			<item>
				<title>morpho-and-rfigshare</title>
				<description>&lt;p&gt;In this Case Study I will use Morpho to compare directly with reml.&lt;/p&gt;

&lt;h1&gt;Step one: Set up morpho&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Follow the instructions at the ASN SuperSite website and install Morpho 1.8 rather than latest version because it has technical issues that stop it from setting permissions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.tern-supersites.net.au/index.php/data/repository-tutorial&quot;&gt;Configure morpho&lt;/a&gt;.  (I will follow the ASN SuperSite instructions as a future Case Study will be to use their KNB Metacat service).&lt;/li&gt;
&lt;li&gt;Do not configure to connect to the Metacat repository, will need a password to be assigned by ASN data manager.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Step 2: Look at the REML created metadata using Morpho&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Morpho offers to open existing sets for modification.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code: get location of my example dataset&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(disentangle)
fpath &amp;lt;- system.file(file.path(&quot;extdata&quot;, &quot;civst_gend_sector.csv&quot;), package=&quot;disentangle&quot;)
fpath
dirname(fpath)
# [1] &quot;/home/ivan_hanigan/Rlibs/disentangle/extdata&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Morpho &gt; File &gt; import = civst_gend_sector_eml.xml&lt;/li&gt;
&lt;li&gt;(not the figshare_civst_gend_sector_eml.xml that was created when sending to figshare)&lt;/li&gt;
&lt;li&gt;Error encountered.  could not open metadata, open empty data package.  Offered to upgrade (unable to edit &gt; accepted)&lt;/li&gt;
&lt;li&gt;unable to display data, empty data package will be shown&lt;/li&gt;
&lt;li&gt;top menu &gt; Documentation &gt; Add/Edit ion

&lt;h1&gt;Step 3: Create new datasets with Morpho&lt;/h1&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Mon Oct 14 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/morpho-and-rfigshare/</link>
			</item>
		
			<item>
				<title>dc-uploader-and-ANU-DataCommons</title>
				<description>&lt;p&gt;In this post I use the tool produced at the ANU by the DataCommons team.  This requires Python3.&lt;/p&gt;

&lt;h1&gt;What does it do?&lt;/h1&gt;

&lt;p&gt;The script only creates new collection records. The functionality to edit records didn’t make it into the script as the expectation is that automated ingests will only require creation of new datasets to which files will be uploaded.&lt;/p&gt;

&lt;p&gt;Users can feel free to tweak the collection parameter file to their liking in the development environment until happy with the results.&lt;/p&gt;

&lt;h1&gt;Create the metadata.txt&lt;/h1&gt;

&lt;p&gt;You need to get the python scripts and conf file from the ANU DataCommons team.  Store these somewhere handy and move to that directory.&lt;/p&gt;

&lt;p&gt;change the anudc.conf: to test out the scripts by creating some sample records, please uncomment the “host” field in the file that points to dc7-dev2.anu.edu.au:8443 , and comment out the one that points to datacommons.anu.edu.au:8443.&lt;/p&gt;

&lt;p&gt;Also you get a different token in dev and prod servers for security reasons you cannot use the same token. Also, storing your username and password in plain text is not recommended and is to be used only for debugging purposes. Also, in my case I had to change the owner group to ‘5’ when creating records in dev. In prod, it’s 6.&lt;/p&gt;

&lt;p&gt;You can look int the &quot;Keys.txt&quot; file that contains the full list of values that can be specified in this metadata.txt file.&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;setwd(&quot;~/tools/dcupload&quot;)
sink(&quot;metadata.txt&quot;)
cat(&quot;
# This file, referred to as a collection parameter file, consists of
# data in key=value pairs. This data is sent to the ANU Data Commons
# to create a collection, establish relations with other records,
# and/or upload files to those collections.

# The metadata section consists of metadata for use in creation (not
# for modification) of record metadata in ANU Data Commons. The
# following fields are required for the creation of a record. The file
# Keys.txt contains the full list of values that can be specified in
# this file. Based on this information below, a collection record of
# type databaset with the title &quot;Test Collection 6/05/2013&quot; will be
# created owned by Meteorology and Health group.
[metadata]
type = Collection
subType = dataset
ownerGroup = 5
# 6 on production, 5 on dev
name = Civil Status, Gender and Activity Sector
briefDesc = An example, fictional dataset for Decision Tree Models
citationCreator = Ritschard, G. (2006). Computing and using the deviance with classification trees. In Compstat 2006 - Proceedings in Computational Statistics 17th Symposium Held in Rome, Italy, 2006.
email = ivan.hanigan@anu.edu.au
anzforSubject = 1601

# The relations section allows you to specify the relation this record
# has with other records in the system.  Currently relations with NLA
# identifiers is not supported.
[relations]
isOutputOf = anudc:123

# This section contains a line of the form 'pid = anudc:123' once a
# record has been created so executing the uploader script with the
# same collection parameter file doesnt create a new record with the
# same metadata.
[pid]
&quot;)
sink()

# run the dcload
system(&quot;python3 dcuploader.py -c metadata.txt&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;What happened?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Looking in the metadata.txt file it now has a pid like &quot;pid = test:3527&quot;&lt;/li&gt;
&lt;li&gt;And we have created a new record in our account on the DataCommons server.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;go to the website&lt;/h1&gt;

&lt;p&gt;Now go to &lt;a href=&quot;https://dc7-dev2.anu.edu.au:8443/DataCommons/&quot;&gt;the dev site&lt;/a&gt; and you can continue editing the record manually in the browser.&lt;/p&gt;

&lt;p&gt;Or if we have ironed out the wrinkles you could go straight to the production server at &lt;a href=&quot;https://datacommons.anu.edu.au:8443/DataCommons&quot;&gt;This Link&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Uploading the data&lt;/h1&gt;

&lt;p&gt;The dataset gets sent using a Java applet in the browser while you are manually editing the record using the browser.&lt;/p&gt;

&lt;h1&gt;Notes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After the records get created, the script tries to relate the record to other records as you’ve specified in the collection parameter file in the relations section. If you’re creating a record in dev2, you cannot relate it to a record in production because that record doesn’t exist in dev2. Remember that IDs for records in dev environments have the prefix “test:” while those in production have “anudc:”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, when you ran the script against production the created records were linked with the record with the ID anudc:123. I have now removed those relations. You might want to change that value in your metadata.txt file so the links are established to records that created records actually can be related to. Or for testing purposes, simply delete the entire [relations] section.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Sun Oct 13 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/dc-uploader-and-ANU-DataCommons/</link>
			</item>
		
			<item>
				<title>reml-and-rfigshare-part-2</title>
				<description>&lt;p&gt;In the last post I explored the functionality of reml.
This time I will try to send data to figshare.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First follow &lt;a href=&quot;https://github.com/ropensci/rfigshare&quot;&gt;These Instructions&lt;/a&gt; to get rfigshare set up.  In particular store your figshare credentials in ~/.Rprofile&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:reml-and-rfigshare-part-2&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
require(devtools)
install_github(&quot;reml&quot;, &quot;ropensci&quot;)
require(reml)
install_github(&quot;rfigshare&quot;, &quot;ropensci&quot;)
require(rfigshare)
install_github(&quot;disentangle&quot;, &quot;ivanhanigan&quot;)
require(disentangle)
# load
fpath &amp;lt;- system.file(file.path(&quot;extdata&quot;,&quot;civst_gend_sector_eml.xml&quot;), package = &quot;disentangle&quot;)
setwd(dirname(fpath))
obj &amp;lt;- eml_read(fpath)
# clean
obj
# do

## STEP 1: find one of the preset categories
# available. We can ask the API for
# a list of all the categories:
list &amp;lt;- fs_category_list()
list[grep(&quot;Survey&quot;, list)]

## STEP 2: PUBLISH TO FIGSHARE
id &amp;lt;- eml_publish(fname,
                  description=&quot;Example EML
                    A fictional dataset&quot;,
                  categories = &quot;Survey results&quot;,
                  tags = &quot;EML&quot;,
                  destination=&quot;figshare&quot;
                  )
# there are several warnings
# but go to figshare and it has sent the metadata and data OK

# make public using either the figshare web interface, the
# rfigshare package (using fs_make_public(id)) or just by adding
# the argument visibility = TRUE to the above eml_publish
fs_make_public(id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Now these data are on figshare&lt;/h1&gt;

&lt;p&gt;Now I have published the data they are visible and have a DOI&lt;/p&gt;

&lt;iframe src=&quot;http://wl.figshare.com/articles/820158/embed?show_title=1&quot; width=&quot;568&quot; height=&quot;157&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

</description>
				<published>Sat Oct 12 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/reml-and-rfigshare-part-2/</link>
			</item>
		
			<item>
				<title>data-documentation-case-study-reml-and-rfigshare</title>
				<description>&lt;h4&gt;Case Study: reml-and-rfigshare&lt;/h4&gt;

&lt;p&gt;First we will look at the work of the ROpenSci team and the reml
package.  In the vignette they show how to publish data to figshare
using rfigshare package.  &lt;a href=&quot;http://figshare.com/&quot;&gt;figshare&lt;/a&gt; is a site
where scientists can share datasets/figures/code. The goals are to
encourage researchers to share negative results and make reproducible
research efforts user-friendly. It also uses a tagging system for
scientific research discovery. They give you unlimited public space
and 1GB of private space.&lt;/p&gt;

&lt;p&gt;Start by getting the reml package.&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
require(devtools)
install_github(&quot;reml&quot;, &quot;ropensci&quot;)
require(reml)
?eml_write
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This is the Top-level API function for writing eml.  Help page is a bit sparse.  See &lt;a href=&quot;https://github.com/ropensci/reml&quot;&gt;This Link&lt;/a&gt; for more.  For eg &quot;for convenience, dat could simply be a data.frame and reml will launch it's metadata wizard to assist in constructing the metadata based on the data.frame provided. While this may be helpful starting out, regular users will find it faster to define the columns and units directly in the format above.&quot;&lt;/p&gt;

&lt;p&gt;Now load up the test data for classification trees I described in &lt;a href=&quot;/2013/10/test-data-for-classification-trees/&quot;&gt;This Post&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;install_github(&quot;disentangle&quot;, &quot;ivanhanigan&quot;) # for the data
                                             # described in prev post

# load
fpath &amp;lt;- system.file(file.path(&quot;extdata&quot;, &quot;civst_gend_sector.csv&quot;),
                     package = &quot;disentangle&quot;
                     )
civst_gend_sector &amp;lt;- read.csv(fpath)

# clean
str(civst_gend_sector)

# do
eml_write(civst_gend_sector,
          creator = &quot;Ivan Hanigan &amp;lt;ivanhanigan@gmail.com&amp;gt;&quot;)





# Starts up the wizard, a section is shown below.  The wizard
# prompts in the console and the user writes the answer.

# Enter description for column 'civil_status':
#  marriage status
# column civil_status appears to contain categorical data.
#  
# Categories are divorced/widowed, married, single
#  Please define each of the categories at the prompt
# define 'divorced/widowed':
# was once married
# define 'married':
# still married
# define 'single':
# never married

# TODO I don't really know what activity_sector is.  I assumed
# school because Categories are primary, secondary, tertiary.

# this created &quot;metadata.xml&quot; and &quot;metadata.csv&quot;
file.remove(c(&quot;metadata.xml&quot;,&quot;metadata.csv&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;
This was a very minimal data documentation effort.  A bit more detail would be better.  Because I would now need to re-write all that in the wizard I will take the advice of the help file that &quot;regular users will find it faster to define the columns and units directly in the format&quot;&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;ds &amp;lt;- data.set(civst_gend_sector,
               col.defs = c(&quot;Marriage status&quot;, &quot;sex&quot;, &quot;education&quot;, &quot;counts&quot;),
               unit.defs = list(c(&quot;was once married&quot;,&quot;still married&quot;,&quot;never married&quot;),
                   c(&quot;women&quot;, &quot;men&quot;),
                   c(&quot;primary school&quot;,&quot;secondary school&quot;,&quot;tertiary school&quot;),
                   c(&quot;persons&quot;))
               )
ds
# this prints the dataset and the metadata
# now run the EML function
eml_write(ds, 
          title = &quot;civst_gend_sector&quot;,  
          description = &quot;An example, fictional dataset for Decision Tree Models&quot;,
          creator = &quot;Ivan Hanigan &amp;lt;ivanhanigan@gmail.com&amp;gt;&quot;,
          file = &quot;inst/extdata/civst_gend_sector_eml.xml&quot;
          )
# this created the xml and csv with out asking anything
# but returned a
## Warning message:
## In `[&amp;lt;-.data.frame`(`*tmp*`, , value = list(civil_status = c(2L,  :
##   Setting class(x) to NULL;   result will no longer be an S4 object

# TODO investigate this?

# now we can access the local EML
obj &amp;lt;- eml_read(&quot;inst/extdata/civst_gend_sector_eml.xml&quot;)
obj 
str(dataTable(obj))
# returns an error
## Error in plyr::compact(lapply(slotNames(from), function(s) if (!isEmpty(slot(from,  (from attribute.R#300) : 
##   subscript out of bounds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;So this looks like a useful tool.  Next steps are to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;look at sending these data to figshare&lt;/li&gt;
&lt;li&gt;describe a really really REALLY simple workflow (3 lines? create metadata, eml_write, push to figshare)&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Sat Oct 12 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/data-documentation-case-study-reml-and-rfigshare/</link>
			</item>
		
			<item>
				<title>two-main-types-of-data-documentation-workflow</title>
				<description>&lt;p&gt;This post introduces a new series of blog posts in which I want to experiment with a few tools for data documentation, which I'll present as Case Studies.  This series of posts will be pitched to an audience mixture of data librarians and data analysts.&lt;/p&gt;

&lt;p&gt;Data documentation occurs in a spectrum from simple notes through to elaborate systems.  I've been working on a conceptual framework about how the actual process can be done in two distinct ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Graphical User Interface (GUI) solutions&lt;/li&gt;
&lt;li&gt;Programmatic (Scripted/Automagic) solutions&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think the GUI tools are in general pretty user friendly and useful
for simple projects with only a small number of datasets, but have a
major drawback for the challenge of heterogeneous data integration.  I
think the problem is expressed nicely &lt;a href=&quot;http://carlboettiger.info/2013/06/23/notes-on-leveraging-the-ecological-markup-language.html&quot;&gt;In This Post By Carl Boettiger&lt;/a&gt;  in reference to Morpho:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;looks like a rather useful if tedious tool for generating EML
files. Unfortunately, without the ability to script inputs or
automatically detect existing data structures, we are forced through
the rather arduous process of adding all metadata annotation each
time....&quot;&lt;/li&gt;
&lt;li&gt;&quot;...A package could also provide utilities to generate EML from R objects, leveraging the metadata implicit in R objects that is not present in a CSV (in which there is no built-in notion of whether  a column is numeric or character string, what missing value characters it uses, or really if it is consistent at all. Avoiding manual specification of these things makes the metadata annotation less tedious as well.&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Centralised Repository, Distributed Users&lt;/h1&gt;

&lt;p&gt;A key aspect of current approaches is the existence of a centralised data management system.  All the examples I consider include at least a metadata catalogue and some also include a data repository.  An additional feature sometimes exists for managing users permissions.&lt;/p&gt;

&lt;p&gt;The relationship between users and centralised services is a really complicated space, but essentially consists of the ability for users to create the documentation and push it (perhaps along with the data) to the metadata catalogue  and/or repository.  So given these assumptions I propose the following types of arrangement:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;user sends metadata to metadata catalogue&lt;/li&gt;
&lt;li&gt;user sends metadata and data to metadata catalogue and data repository&lt;/li&gt;
&lt;li&gt;user sends metadata and data and permissions information to metadata catalogue and data repository and permissions system.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The Case Studies I've identified that I want to explore are listed below, names follow the format 'client tool'-and-'data repository or metadata catalogue'-and-optionally-'permissions system':&lt;/p&gt;

&lt;h4&gt;Programmatic solutions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;reml-and-rfigshare&lt;/li&gt;
&lt;li&gt;reml-and-knb (when/if this becomes available)&lt;/li&gt;
&lt;li&gt;make_ddixml-and-ddiindex-and-orapus&lt;/li&gt;
&lt;li&gt;r2ddi-ddiindex&lt;/li&gt;
&lt;li&gt;dc-uploader-and-ANU-DataCommons&lt;/li&gt;
&lt;li&gt;dc-uploader-and-RDA&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Graphical User Interface solutions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;morpho-and-knb-metacat&lt;/li&gt;
&lt;li&gt;nesstar-publisher-and-nesstar-and-whatever-Steve-calls-the-ADA-permissions-system&lt;/li&gt;
&lt;li&gt;xmet-and-Australian-Spatial-Data-Directory&lt;/li&gt;
&lt;li&gt;sdmx-editor-and-sdmx-registry&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Fri Oct 11 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/two-main-types-of-data-documentation-workflow/</link>
			</item>
		
			<item>
				<title>wickhams-tidy-tools-only-get-you-90-pct-the-way</title>
				<description>&lt;h4&gt;Hadley Wickham's tidy tools&lt;/h4&gt;

&lt;p&gt;In this video at 8 mins 50 seconds he says &quot;these four tools do 90% of the job&quot;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;subset,&lt;/li&gt;
&lt;li&gt;transform,&lt;/li&gt;
&lt;li&gt;summarise, and&lt;/li&gt;
&lt;li&gt;arrange&lt;/li&gt;
&lt;li&gt;TODO I noticed &lt;a href=&quot;http://www.rstudio.com/training/curriculum/data-manipulation.html&quot;&gt;at the website for an Rstudio  course&lt;/a&gt; transform has been replaced by mutate as one of the &quot;four basic verbs of data manipulation&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;iframe src=&quot;//player.vimeo.com/video/33727555&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt; &lt;p&gt;&lt;a href=&quot;http://vimeo.com/33727555&quot;&gt;Tidy Data&lt;/a&gt; from &lt;a href=&quot;http://vimeo.com/user2150538&quot;&gt;Drew Conway&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;So I thought what's the other 10?  Here's a few contenders for my work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;merge&lt;/li&gt;
&lt;li&gt;reshape::cast and reshape::melt&lt;/li&gt;
&lt;li&gt;unlist&lt;/li&gt;
&lt;li&gt;t() transpose&lt;/li&gt;
&lt;li&gt;sprintf or paste&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;R-subset&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Filter rows by criteria
subset(airquality, Temp &amp;gt; 90, select = c(Ozone, Temp))

## NB This is a convenience function intended for use interactively.  For
## programming it is better to use the standard subsetting functions like
## ‘[’, and in particular the non-standard evaluation of argument
## ‘subset’ can have unanticipated consequences.

with(airquality,
     airquality[Temp &amp;gt; 90, c(&quot;Ozone&quot;, &quot;Temp&quot;)]
     )

# OR

airquality[airquality$Temp &amp;gt; 90,  c(&quot;Ozone&quot;, &quot;Temp&quot;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-transform&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# New columns that are functions of other columns       
df &amp;lt;- transform(airquality,
                new = -Ozone,
                Temp2 = (Temp-32)/1.8
                )
head(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-mutate&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(plyr)
# same thing as transform
df &amp;lt;- mutate(airquality, new = -Ozone, Temp = (Temp - 32) / 1.8)    
# Things transform can't do
df &amp;lt;- mutate(airquality, Temp = (Temp - 32) / 1.8, OzT = Ozone / Temp)

# mutate is rather faster than transform
system.time(transform(baseball, avg_ab = ab / g))
system.time(mutate(baseball, avg_ab = ab / g))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-summarise&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# New data.frame where columns are functions of existing columns
require(plyr)    
df &amp;lt;- ddply(.data = airquality,
            .variables = &quot;Month&quot;,
            .fun = summarise,
            tmax = max(Temp),
            tav = mean(Temp),
            ndays = length(unique(Day))
            )
head(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Passing variables to ddply for summary&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Notice how the name of the variable Temp doesn't need quotes?
# this means that you need to hard code the names
# But if you want to pass variables to this inside a function we need a
# different approach.

summarise_df  &amp;lt;- function(x, by, var1, var2, var3)
  {
    data_out &amp;lt;- ddply(x,
                      by,
                      function(df) return(
                        c(
                          tmax = max(df[,var1]),
                          tav = mean(df[,var2]),
                          ndays = length(unique(df[,var3]))
                          )
                        )
                      )
    return(data_out)
  }

df2 &amp;lt;- summarise_df(x = airquality, by = &quot;Month&quot;,
                   var1 = &quot;Temp&quot;, var2 = &quot;Temp&quot;, var3 = &quot;Day&quot;
                   )

head(df2)
all.equal(df,df2)
# TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Another alternative, if we want to pass the dataset as string too&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;summarise_df2  &amp;lt;- function(x, by, var1, var2, var3)
  {
    data_out &amp;lt;- eval(
      parse(
        text =
        sprintf(
          &quot;ddply(.data = %s,
            .variables = '%s',
            .fun = summarise,
            tmax = max(%s),
            tav = mean(%s),
            ndays = length(unique(%s))
            )&quot;, x, by, var1, var2, var3
          )
        )
      )
    return(data_out)
  }

df3 &amp;lt;- summarise_df2(x = &quot;airquality&quot;, by = &quot;Month&quot;,
                     var1 = &quot;Temp&quot;, var2 = &quot;Temp&quot;, var3 = &quot;Day&quot;
                     )
head(df3)
all.equal(df, df3)
# TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-arrange&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Re-order the rows of a data.frame
df &amp;lt;- arrange(airquality, Temp, Ozone)
head(df)
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>Thu Oct 10 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/wickhams-tidy-tools-only-get-you-90-pct-the-way/</link>
			</item>
		
	</channel>
</rss>