<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>disentangle</title>
		<description>Disentangle Things</description>
		<link>http://ivanhanigan.github.com</link>
		
			<item>
				<title>Climate grids thredds European data</title>
				<description>&lt;p&gt;I've got netCDF data regarding temperature calculated from the daily E-OBS gridded dataset which is based on observational data with a spatial resolution of 0.22Â° on a
rotated pole grid, with the north pole at 39.25N, 162W.  &lt;a href=&quot;http://www.ecad.eu&quot;&gt;http://www.ecad.eu&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The rotated grid is the same as used in many ENSEMBLES Regional Climate Models.  But I've never worked with the rotated grid projection before and so here is what I learnt.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;{r}&quot;&gt;#library(ncdf4)
library(ncdf)
library(raster)
projdir &amp;lt;- &quot;~/projects/weather_european_eobs/eobs_temperature_tg_dataset&quot;
outdir &amp;lt;- &quot;data_provided&quot;
outdir &amp;lt;- file.path(projdir, outdir)
#dir.create(outdir, recursive = T)
setwd(projdir)
dir()
# The URL below will get you the data, but the ECAD group do request you register your email address 
# they probably use this information to report some usage stats to their funders, so 
# Please do consider going to this webpage (http://www.ecad.eu/download/ensembles/ensembles.php)
# and register yourself.  Thanks!
webroot &amp;lt;- &quot;http://www.ecad.eu/download/ensembles/data/Grid_0.22deg_rot&quot;
# Create a list of netcdf files to download, we can loop over it
file_list &amp;lt;- c(&quot;tg_0.22deg_rot_1950-1964_v12.0.nc.gz&quot;,&quot;tg_0.22deg_rot_1965-1979_v12.0.nc.gz&quot;)

# only download if not already done
setwd(outdir)
for(i in 1:length(file_list))
    {
    dl &amp;lt;- file.path(webroot, file_list[i])
    dl
    infile &amp;lt;- basename(dl)
    exists  &amp;lt;- dir(pattern= infile)
    if(
      !exists(&quot;exists&quot;)
       )
    {
    download.file(dl, destfile = infile, mode = &quot;wb&quot;)
    system(sprintf(&quot;gunzip %s&quot;, infile))
    }
    &quot; (250MB)&quot;
    }
setwd(projdir)

# now we can go through all days, I set this up as a loop over ncdf files then days, 
# but I'll probably try to set this up to directly use the netCDF aggregation functions available and avoid looping 
# for(j in 1:length(file_list))
  #{
    j = 1
    infile &amp;lt;- file.path(outdir, gsub(&quot;.gz&quot;, &quot;&quot;, file_list[j]))
    nc &amp;lt;- open.ncdf(infile)
    #str(nc)
    print(nc)
    #?get.var.ncdf, following the example in the help file
    print(paste(&quot;The file has&quot;,nc$nvars,&quot;variables&quot;))
    var_i      &amp;lt;- nc$var[[4]]
    #var_i
    varsize &amp;lt;- var_i$varsize
    ndims   &amp;lt;- var_i$ndims
    nt      &amp;lt;- varsize[ndims]  # Remember timelike dim is always the LAST dimension!
    #nt 
    # we will set up to do a loop over days, I want to
    # a) read in the day grid
    # b) make a spatial points file with the appropriate projection
    # c) convert to geotiff and save to disk
    #for(i in 1:nt)
    #  {
        i = 1
       # Initialize start and count to read one timestep of the variable.
       start &amp;lt;- rep(1,ndims)   # begin with start=(1,1,1,...,1)
       start[ndims] &amp;lt;- i       # change to start=(1,1,1,...,i) to read timestep i
       count &amp;lt;- varsize        # begin w/count=(nx,ny,nz,...,nt), reads entire var
       count[ndims] &amp;lt;- 1       # change to count=(nx,ny,nz,...,1) to read 1 tstep
       data3 &amp;lt;- get.var.ncdf( nc, var_i, start=start, count=count )

       # Now read in the value of the timelike dimension
       timeval &amp;lt;- get.var.ncdf( nc, var_i$dim[[ndims]]$name, start=i, count=1 )

       print(paste(&quot;Data for variable&quot;,var_i$name,&quot;at timestep&quot;,i,
               &quot; (time value=&quot;,timeval,var_i$dim[[ndims]]$units,&quot;):&quot;))
       # [1] &quot;Data for variable tg at timestep 1  (time value= 0 days since 1950-01-01 00:00 ):&quot;      
       #print(data3)
       #image(data3)
       dat1 &amp;lt;- list()
       dat1$x &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_longitude&quot;)
       #dat1$x &amp;lt;- dat1$x[,1]       
       dat1$y &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_latitude&quot;)
       #dat1$y &amp;lt;- dat1$y[1,]              
       dat1$z &amp;lt;- get.var.ncdf(nc, var_i, start=start, count=count )
       str(dat1$z)
       #image(dat1$z)
       #map(&quot;world&quot;, xlim = c(xmin, xmax), ylim = c(ymin, ymax))
       #with(dat1, points(x, y, cex = .1, pch = 16))
       dat2 &amp;lt;- data.frame(
         x = as.vector(dat1$x),
         y = as.vector(dat1$y),
         z = as.vector(dat1$z)
         )
       #str(dat2)  
       #getwd()
       # I had the idea to save each day out to a file for looping over again and extracting spatially located data
       # say for a pixel, but I decided to try out the netCDF aggregation tools at a next step
       # save(dat2, file = sprintf(&quot;data_derived/eobs_tg_%s.RData&quot;,i))           
       #load(sprintf(&quot;eobs_tg_%s.RData&quot;,i))
       #This seemed like a good idea, but RData is more compressed
       # write.csv(dat2, sprintf(&quot;eobs_tg.csv&quot;,i), row.names = F, na = &quot;&quot;)       
       #}
#}

# load the data for a specific day as example
dir(&quot;data_derived&quot;)
infile  &amp;lt;- &quot;eobs_tg_1.RData&quot;
load(file.path(&quot;data_derived/&quot;, infile))
ls()
# Now this is able to be mapped, but have make sure of the projection
library(maptools)
library(scales) 
library(RColorBrewer)
library(rgdal)
# the following code was adapted from

# Bedia, J. (2012). R practice using data from the ENSEMBLES
# Project. Retrieved from
# http://www.value-cost.eu/sites/default/files/VALUE_TS1_D02_RIntro.pdf

data(wrld_simpl)
# loads the world map dataset 
wrl  &amp;lt;- as(wrld_simpl,&quot;SpatialLines&quot;) 
l1 &amp;lt;- list(&quot;sp.lines&quot;,wrl)
#x &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_longitude&quot;)
str(x)
x &amp;lt;- as.vector(x)
#y &amp;lt;- get.var.ncdf(nc, varid=&quot;Actual_latitude&quot;)
str(y)
y  &amp;lt;- as.vector(y)


coords &amp;lt;- cbind(x, y)
str(coords)
head(coords)
# so the actual lat lons are not regular grid in degrees
png(&quot;figures_and_tables/qc_actual_xy.png&quot;)
plot(coords, asp=1, cex=.4, col=&quot;grey&quot;, 
  pch=&quot;+&quot;, main=(&quot;actual lon-lat grid&quot;))
lines(wrl)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;This is the first graphic, it shows a kind of fan of locations&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_actual_xy.png&quot; alt=&quot;/images/qc_actual_xy.png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;{r}&quot;&gt;# so what does the rotated grid look like in its own universe?
str(dat1$z)
png(&quot;figures_and_tables/qc_rotated.png&quot;)
image(dat1$z)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;This is how it looks on rotated grid&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_rotated.png&quot; alt=&quot;images/qc_rotated.png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;{r}&quot;&gt;# so now add in the temperatures, use the wgs latlongs
t &amp;lt;- as.vector(dat1$z)
dat3 &amp;lt;- cbind.data.frame(coords, t)
summary(dat3)
coordinates(dat3) &amp;lt;- c(1,2)
#names(dat3@data)  &amp;lt;- c(&quot;z&quot;)
str(dat3)
summary(dat3@data)
color.palette &amp;lt;- rev(brewer.pal(11,&quot;Spectral&quot;))
getwd()
dir()
png(&quot;figures_and_tables/qc_tempmap_wgs.png&quot;)
spplot(dat3, scales=list(draw=TRUE), sp.layout=list(l1), 
       col.regions=alpha(color.palette,.2), cuts=10, 
       main=&quot;tg&quot;)
dev.off()

# write out the data to a shapefile
# we first have to make sure that the proj4 string is ok
str(dat3)
proj4string(dat3) &amp;lt;- &quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0&quot;
setwd(file.path(projdir, &quot;data_derived&quot;))
writeOGR(dat3, &quot;out2.shp&quot;, &quot;out2&quot;, &quot;ESRI Shapefile&quot;)
setwd(projdir)

# to look at the rotated project do the following
# need to look at the website to get this info
# http://opendap.knmi.nl/knmi/thredds/dodsC/e-obs_0.22rotated/tg_0.22deg_rot_v12.0.nc.html
print(nc)
# it doesn't seem like there is this info in the ncdf file?
get.var.ncdf(nc, &quot;projection&quot;)
rcm.lonlat.grid &amp;lt;- SpatialPoints(coords)
# now set this to wgs84
proj4string(rcm.lonlat.grid) &amp;lt;-&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0&quot;

# this is the proj from website
# +proj=ob_tran +o_proj=longlat +lon_0=18 +o_lat_p=39.25 +a=6367470 +e=0
rcm.lambert.proj4 &amp;lt;- CRS(&quot;+proj=ob_tran +o_proj=longlat +lon_0=18 +o_lat_p=39.25 +a=6367470 +e=0&quot;)
# do the transform
spTransform(rcm.lonlat.grid, rcm.lambert.proj4) -&amp;gt; rcm.lambert.grid
summary(rcm.lambert.grid)

world.trans &amp;lt;- spTransform(wrl, rcm.lambert.proj4)

png(&quot;figures_and_tables/qc_rotated_grid_and_countries.png&quot;)
plot(rcm.lambert.grid@coords, cex=.2, pch=3, asp=1, col=&quot;grey&quot;, 
  main=&quot;Projected RCM Grid - Lambert Conical Conformal&quot;)
lines(world.trans, col=&quot;red&quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;This shows the rotated world countries&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_rotated_grid_and_countries.png&quot; alt=&quot;images/qc_rotated_grid_and_countries.png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;{r&quot;&gt;pr.df &amp;lt;- cbind.data.frame(coordinates(rcm.lambert.grid), dat3@data)
coordinates(pr.df) &amp;lt;- c(1,2) 
l1  &amp;lt;- list(&quot;sp.lines&quot;, world.trans)
color.palette &amp;lt;- colorRampPalette(c(&quot;yellow&quot;,&quot;cyan&quot;, &quot;blue&quot;,&quot;purple&quot;))

# This graph shows this with temp
png(&quot;figures_and_tables/qc_rotated_grid_and_countries2.png&quot;)
spplot(pr.df, sp.layout=list(l1), cuts=7, cex=1.5, 
  col.regions=alpha(color.palette(7),.15), pch=rep(15,7),
  main=&quot;Temp&quot;)
dev.off()
# not much point writing this to shapefile?
#proj4string(pr.df)  &amp;lt;- rcm.lambert.proj4
#str(pr.df)
#setwd(file.path(projdir, &quot;data_derived&quot;))
#dir()
# writeOGR(pr.df, &quot;out.shp&quot;, &quot;out&quot;, &quot;ESRI Shapefile&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;this one has temperatures too&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/qc_rotated_grid_and_countries2.png&quot; alt=&quot;images/qc_rotated_grid_and_countries2.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;acknowledgement and citations.&lt;/h2&gt;

&lt;p&gt;E-OBS temperature and precipitation:&lt;/p&gt;

&lt;p&gt;We acknowledge the E-OBS dataset from the EU-FP6 project ENSEMBLES
(http://ensembles-eu.metoffice.com) and the data providers in the
ECA&amp;amp;D project (http://www.ecad.eu)&lt;/p&gt;

&lt;p&gt;Haylock, M.R., N. Hofstra, A.M.G. Klein Tank, E.J. Klok, P.D. Jones,
M. New. 2008: A European daily high-resolution gridded dataset of
surface temperature and precipitation. J. Geophys. Res (Atmospheres),
113, D20119, doi:10.1029/2008JD10201&quot;&lt;/p&gt;
</description>
				<published>2016-02-02 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/02/climate-grids-thredds-european-data/</link>
			</item>
		
			<item>
				<title>The perfect is the enemy of the good</title>
				<description>&lt;p&gt;According to Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good&quot;&gt;https://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good&lt;/a&gt;
this phrase was popularised by Voltaire, and Shakespeare via King Lear: &quot;striving to better, oft we mar what's well&quot;&lt;/p&gt;

&lt;p&gt;This phrase is a favourite of several people I respect and admire.  On the other hand it has always vaguely troubled me.
I think there may be two dimensions of this phrase that are worth pondering.&lt;/p&gt;

&lt;h2&gt;One: &quot;the good&quot; is a quality of the work&lt;/h2&gt;

&lt;p&gt;The first dimension is better expressed in King Lear, and on the wiki page where it is suggested the meaning is &quot;that we might never complete a task if we have decided not to stop until it is perfect.&quot;
This is correct, however not an absolutely faithful interpretation of the phrase &quot;striving to better&quot;.  To my mind this does not necessarily lead to the position &quot;will not stop until perfect&quot;.
If one strives for perfection but admits that this is beyond the scope of one's ambition then one will eventually stop when &quot;it&quot; is &quot;good enough&quot;, and that state will be better if one were striving for perfection than if one where aiming for &quot;satisfactory&quot;.&lt;/p&gt;

&lt;h2&gt;Two: &quot;the good&quot; is the impact the work may have on the world around us&lt;/h2&gt;

&lt;p&gt;The second dimension resonates more clearly with me and that is that &quot;the good&quot; is a public good, an impact that we feel would make the world a better place.
To aim for a satisfactory action or intervention on the world around is would then be the goal, and we should try to achieve this as soon as possible, without delaying our work with minutia or trivia that might be that extra 1 percent that we would hope elevates our work further than satisfactory.  One extra rung up the ladder toward perfection.&lt;/p&gt;

&lt;h2&gt;Stirzaker's &quot;simplicity cycle&quot;&lt;/h2&gt;

&lt;p&gt;On page 175 of Stirzaker, R. (2010). Out of the scientists garden: A story of water and food. Canberra, Australia: CSIRO.
I copied the image and show it below, but I have flipped it around so the axis are reversed from his original.
This shows a couple of trajectories we can head along in terms of striving for enhancements, and the effect this has on how &quot;good&quot; the results are.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/stirzacker.jpg&quot; alt=&quot;/images/stirzacker.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Seriousness aside&lt;/h2&gt;

&lt;p&gt;I used this image in my last project as a rallying call to the team to focus on both striving for the perfection while also balancing our aims to impact on the good.
My colleagues pointed out that the flipped over version is easily turned into a stickman, a la XKCD!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/stirzacker2.jpg&quot; alt=&quot;/images/stirzacker2.jpg&quot; /&gt;&lt;/p&gt;
</description>
				<published>2016-01-31 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/the-perfect-is-the-enemy-of-the-good/</link>
			</item>
		
			<item>
				<title>UPDATE to post 'GIS Issues when R is Used for Transforming Coordinate Systems'</title>
				<description>&lt;ul&gt;
&lt;li&gt;UPDATE to post &lt;a href=&quot;http://ivanhanigan.github.com//2015/10/gis-issues-when-r-used-transforming-coordinate-systems&quot;&gt;http://ivanhanigan.github.com//2015/10/gis-issues-when-r-used-transforming-coordinate-systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recall that RGDAL transforms and write out spatial data in GDA94 with a problem in that ArcGIS does not like the prj file and complains&lt;/li&gt;
&lt;li&gt;This seems an ok workaround&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;infile &amp;lt;- &quot;western_sydney_passive_samplers_site_details.csv&quot;
indir &amp;lt;- &quot;data_derived&quot;

outdir &amp;lt;- indir
outfile &amp;lt;- file.path(outdir, gsub(&quot;.csv&quot;,&quot;.shp&quot;,infile))

dat &amp;lt;- read.csv(file.path(indir, infile), as.is=T)
str(dat)

dat2 &amp;lt;- SpatialPointsDataFrame(data.frame(x=dat$long, y=dat$lat), dat,
                                proj4string=CRS(epsg$prj4[epsg$code %in% '4283'])
                                )

writeOGR(dat2, outfile,
         outdir, driver='ESRI Shapefile'
         )
# fix the prj file 
download.file(&quot;http://spatialreference.org/ref/epsg/4283/prj/&quot;,
              gsub(&quot;.shp&quot;, &quot;.prj&quot;, outfile), mode = &quot;wb&quot;
              )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/samplers_sydney.png&quot; alt=&quot;/images/samplers_sydney.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2016-01-29 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/gis-issues-when-r-used-transforming-coordinate-systems/</link>
			</item>
		
			<item>
				<title>Release checklist references</title>
				<description>&lt;h2&gt;Some good reading and best-practice advice&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Driessen, V. (2010). A successful Git branching model. Nvie.Com. Retrieved January 28, 2016, from &lt;a href=&quot;http://nvie.com/posts/a-successful-git-branching-model/&quot;&gt;http://nvie.com/posts/a-successful-git-branching-model/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;object data=&quot;/images/nvie2010-2.pdf&quot; type=&quot;application/pdf&quot; width=&quot;675&quot; height=&quot;800&quot;&gt;
  &lt;p&gt;Alternative text - include a link &lt;a href=&quot;images/nvie2010-2.pdf.pdf&quot;&gt;to the PDF!&lt;/a&gt;&lt;/p&gt;
&lt;/object&gt;


&lt;ul&gt;
&lt;li&gt;Huff, K. D. (n.d.). A Software Release Checklist. Retrieved January 28, 2016, from &lt;a href=&quot;http://bids.berkeley.edu/news/software-release-checklist&quot;&gt;http://bids.berkeley.edu/news/software-release-checklist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/Huff2016.png&quot; alt=&quot;/images/Huff2016.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stanisic, L., Legrand, A., &amp;amp; Danjean, V. (2015). An Effective Git And Org-Mode Based Workflow For Reproducible Research. ACM SIGOPS Operating Systems  â¦. Retrieved from &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2723881&quot;&gt;http://dl.acm.org/citation.cfm?id=2723881&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/Stanisic2015.png&quot; alt=&quot;/images/Stanisic2015.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2016-01-28 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/release-checklist-references/</link>
			</item>
		
			<item>
				<title>Repeatability vs replication - Definitional variations</title>
				<description>&lt;p&gt;In my previous post I bemoaned the variability in definitions of reproducibility and replication.&lt;br/&gt;
The definition of reproducibility in particular concerns me as I finalise my thesis on 'Reproducible Research Pipelines'.  However I also have noticed a confusion of Repeatability and Replication that is worth noting.  In various author's definitions.  In CASSEY, P., &amp;amp; BLACKBURN, T. M. (2006). Reproducibility and Repeatability in Ecology. BioScience, 56(12), 958. &lt;a href=&quot;http://bioscience.oxfordjournals.org/content/56/12/958.full&quot;&gt;http://bioscience.oxfordjournals.org/content/56/12/958.full&lt;/a&gt; they agree with the definition of Peng 2011 that Reproducibility is to re-calculate the same result from the same data and they use Repeatability as a synonym for Peng's Replication.&lt;/p&gt;

&lt;p&gt;According to Cassey and Blackburn (2006), reproducibility is the case that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from the information presented in the study, a third party could
replicate (sic) the reported results identically.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This definition distinguishes reproducibility from repeatability which is when&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a third party must be able to perform a study using identical methodological protocols 
and analyze the resulting data in an identical manner
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Which is what Peng terms 'replicability'.  This leads me to conclude that:&lt;/p&gt;

&lt;h3&gt;Cassey and Blackburn conflate Repeatability with Replicability.&lt;/h3&gt;

&lt;p&gt;However another author gives a very confused and overlapping view Ellison, A. (2010). Repeatability and Transparency in Ecological Research. Ecology.  &lt;a href=&quot;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&quot;&gt;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&lt;/a&gt; Accessed 12 Jan 2016.&lt;/p&gt;

&lt;p&gt;To add further confusion I have dug up the latest dictionary of epidemiology and find that whilst this agrees with Reproducibility and Replication, it treates Repeatability as a synonym of Reproducibility!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Porta, Miquel S. Dictionary of Epidemiology (6th Edition). New York, NY, USA: Oxford University Press, USA, 2014. ProQuest ebrary. Web. 24 January 2016.
Copyright Â© 2014. Oxford University Press, USA. All rights reserved.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;
Repeatability (Syn: reproducibility): The value below which the
absolute difference between two single test results may be expected to
lie with a probability of 95%, when the results are obtained by the
same method and equipment from identical test material in the same
setting by the same operator within short intervals of time. A test or
measurement is repeatable if the results are identical or closely
similar each time it is conducted. 1-3,5-9,91 See also measurement,
terminology of; reliability.

Replication: The execution of an observational or experimental study
more than once so as to confirm the findings, increase precision, and
obtain a closer estimation of sampling error. Exact replication should
be distinguished from consistency of results on replication. Exact
replication is often possible in the physical sciences, but in the
health, life, and social sciences consistency of results on
replication is often the best that can be
attained. 1,2,6,25,39,42,91,206-208,270,273,533 Consistency of results
on replication is perhaps the most important consideration in
judgements of CAUSALITY.

Reproducibility: see REPEATABILITY.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Whatever happens, never cite Drummond 2009!&lt;/h2&gt;

&lt;p&gt;It is unclear whether Drummond's self-published conference paper was peer reviewed (or reviewed by the conference committee) but the following quote is unsupported assertion and should be ignored.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reproducibility requires changes; replicability avoids them.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Drummond, C. (2009). Replicability is not Reproducibility: Nor is it Good Science. Proceedings of the Evaluation Methods for Machine Learning Workshop 26th International Conference for Machine Learning. Retrieved January 25, 2016, from &lt;a href=&quot;http://cogprints.org/7691/7/icmlws09.pdf&quot;&gt;http://cogprints.org/7691/7/icmlws09.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Indeed in 2012 Drummond (in another self-published, working paper without evidence of peer review) did a backflip and said:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reproducibility requires that the experiment originally carried out be duplicated 
as far as is reasonably possible. The aim is to minimize the difference from 
the first experiment including its flaws, to produce
independent verification of the result as reported
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Drummond, C. (2012). Reproducible research: A dissenting opinion. Unpublished draft. Retrieved October 9, 2015, from &lt;a href=&quot;http://cogprints.org/8675/&quot;&gt;http://cogprints.org/8675/&lt;/a&gt;&lt;/p&gt;
</description>
				<published>2016-01-25 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/repeatability-vs-replication-definitional-variations/</link>
			</item>
		
			<item>
				<title>Reproducibility vs replication - Definitional variations</title>
				<description>&lt;p&gt;There is confusion between the definitions of reproducibility, repeatability and replicability.
I strongly feel we need to tackle that head on and come to an agreed definition.
I prefer Peng 2011:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reproduciblity is using the same data and getting the exact same result.&lt;/li&gt;
&lt;li&gt;Replication is getting a new sample and doing the analysis again and getting a similar result.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;Peng, R. D. (2011). Reproducible research in computational
science. Science, 334(6060), 1226â1227. doi:10.1126/science.1213847
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;There are many people using these interchangeably or around the opposite way. For example Drummond got it round the wrong way in 'Drummond, C., 2009. Replicability is not reproducibility: nor is it good science' &lt;a href=&quot;http://cogprints.org/7691/7/icmlws09.pdf&quot;&gt;http://cogprints.org/7691/7/icmlws09.pdf&lt;/a&gt;
and then reverted it in 'Reproducible Research: a Dissenting Opinion'. &lt;a href=&quot;http://cogprints.org/8675/1/ReproducibleResearch.pdf&quot;&gt;http://cogprints.org/8675/1/ReproducibleResearch.pdf&lt;/a&gt;
(Check out Peng's reaction: &lt;a href=&quot;http://simplystatistics.org/2012/11/15/reproducible-research-with-us-or-against-us-3/&quot;&gt;http://simplystatistics.org/2012/11/15/reproducible-research-with-us-or-against-us-3/&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;And this blog but also gets the definitions around the wrong way &lt;a href=&quot;http://jermdemo.blogspot.com.au/2012/12/the-reproducible-research-guilt-trip.html&quot;&gt;http://jermdemo.blogspot.com.au/2012/12/the-reproducible-research-guilt-trip.html&lt;/a&gt; (even tho being quite entertaining to read and had this great picture.... not sure what the picture means???)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/thinker2.jpg&quot; alt=&quot;/images/thinker2.jpg&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Sure, OK, it is fine that people define things differently to one another but:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The single biggest problem in communication 
is the illusion that it has taken place.
George Bernard Shaw quotes from BrainyQuote.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;We rely on a common defintion to ensure we are talking about the same thing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/communication-bnewell.png&quot; alt=&quot;/images/communication-bnewell.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Source: Newell, B. (2012). Simple models, powerful ideas: Towards effective integrative practice. Global Environmental Change, 22(3), 776â783. &lt;a href=&quot;http://dx.doi.org/10.1016/j.gloenvcha.2012.03.006&quot;&gt;http://dx.doi.org/10.1016/j.gloenvcha.2012.03.006&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is regrettable that in Ecology (my favourite discipline) there seems to be quite a wide gap between various author's definitions.  In CASSEY, P., &amp;amp; BLACKBURN, T. M. (2006). Reproducibility and
Repeatability in Ecology. BioScience, 56(12), 958. &lt;a href=&quot;http://bioscience.oxfordjournals.org/content/56/12/958.full&quot;&gt;http://bioscience.oxfordjournals.org/content/56/12/958.full&lt;/a&gt; they agree with the definition of Peng 2011. However another author gives a very confused and overlapping view:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
because that context changes through time and space, it is virtually
impossible to reproduce precisely or quantitatively any single
experimental or observational field study in ecology. Yet many
ecological studies can be repeated. In particular, ecological
synthesis â the assembly of derived datasets and their subsequent
analysis, re-analysis, and meta-analysis â should be easy to repeat
and reproduce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Ellison, A. (2010). Repeatability and Transparency in Ecological Research.
Ecology.  &lt;a href=&quot;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&quot;&gt;https://dash.harvard.edu/bitstream/handle/1/3123279/Ellison_Repeatability.pdf?sequence=2&lt;/a&gt; Accessed 12 Jan 16&lt;/p&gt;

&lt;p&gt;In another interesting approach Freedman, L. P., Cockburn, I. M., &amp;amp; Simcoe, T. S. (2015). The Economics of Reproducibility in Preclinical Research. PLOS Biology, 13(6), e1002165. &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pbio.1002165&quot;&gt;http://dx.doi.org/10.1371/journal.pbio.1002165&lt;/a&gt; chose instead to define &lt;em&gt;irreproducibility&lt;/em&gt; such that it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;that encompasses the existence and propagation of one or more errors,
flaws, inadequacies, or omissions (collectively referred to as errors) 
that prevent replication of results
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Leaving us to assume that the opposite of this is therefore &lt;em&gt;reproducibility&lt;/em&gt;, although avoiding defining this themselves.  Looking back at the two heads in the picture above... it is interesting to ponder how some people would receive the signal of Freedman et al, having defined the opposite of the thing that is the object of their discussion, rather than the thing itself!&lt;/p&gt;

&lt;p&gt;Let's all agree with Peng and Cassey/Blackburn and move on already!&lt;/p&gt;
</description>
				<published>2016-01-22 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/reproducibility-vs-replication-definitional-variations/</link>
			</item>
		
			<item>
				<title>Validity of measurement</title>
				<description>&lt;p&gt;I have needed to describe validity recently and found it useful to paraphrase some of this statistics blog post: &lt;a href=&quot;&quot;&gt;http://andrewgelman.com/2015/04/28/whats-important-thing-statistics-thats-not-textbooks/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I've been working a lot on air pollution modelling recently, where 'validation' is used to assess how well the modelled pollution predicted values represent the actual pollution observed. I tend to think of validity as formalised in statistical terms, i.e. as correlations between different measurements of the same thing, or between measurement and 'truth', and statistics are used for assessing and calibrating measurements.&lt;/p&gt;

&lt;p&gt;I am guessing that when applied to the validity behind a research proposal, the issue might be whether the measurement is suitable for addressing the issue the researcher (and research question) is interested in, and therefore supporting the researchers to make valid inferences from the outcome of statistical methods. I have heard lots of anecdotes from statisticians in which when they are asked to help with analysing data, their advice was essentially that in order for a valid analysis that addresses the research question one would rather need to have collected different measurements.&lt;/p&gt;
</description>
				<published>2016-01-18 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/validity-of-measurement/</link>
			</item>
		
			<item>
				<title>We have a statistically rigorous and scientifically meaningful definition of replication. Let's use it</title>
				<description>&lt;p&gt;Researchers writing about the 'Reproducibility Crisis' often conflate
the terms reproducibility, repeatability and replicability, but it is
quite important to distinguish these.  There is a great discussion of
the distinction in the SimplyStatistics blog post titled: &lt;a href=&quot;http://simplystatistics.org/2015/10/20/we-need-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication&quot;&gt;We need a statistically rigorous and
scientifically meaningful definition of
replication&lt;/a&gt;.
But I actually now think &lt;em&gt;we DO have that definition!&lt;/em&gt; It is the
that repeatability is the same
as replication and involves a new sample with new measurement errors
while reproducibility uses the same data to recalculate the result.&lt;/p&gt;

&lt;p&gt;I think it is vital that we labour the point so that the
distinction between repeatability and reproducibility is made clear.&lt;/p&gt;

&lt;p&gt;I follow the definition that 'reproducible' is exact re-computation
whereas repeatable/replicable is a new analysis, of a new sample,
yielding a new result (plus or minus some variance from measurement
error), and if the original study is replicated then the same
conclusions are reached from the new analysis.&lt;/p&gt;

&lt;p&gt;I used this paragraph and recent reference in my thesis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reproducibility is defined as âthe ability to recompute data analytic
results given an observed dataset and knowledge of the data analysis
pipelineâ (Leek &amp;amp; Peng 2015). This definition distinguishes
reproducibility from replicability which is âthe chance that an
independent experiment targeting the same scientific question will
produce a consistent resultâ (Leek &amp;amp; Peng 2015).  

Leek, J.T. &amp;amp; Peng, R.D. (2015). Opinion: Reproducible research can
still be wrong: Adopting a prevention approach. Proceedings of the
National Academy of Sciences of the United States of America, 112(6),
1645â1646.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;But I am also a big fan of the the definitions in Peng 2011 and Cassey 2006.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Peng 2011:&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;With replication, independent investigators address a scientific
hypothesis and build up evidence for or against it...

Reproducibility calls for the data and computer code used to analyze
the data be made available to others. This standard falls short of
full replication because the same data are analysed again, rather than
analysing independently collected data.

Peng, R. D. (2011). Reproducible research in computational
science. Science, 334(6060), 1226â1227. doi:10.1126/science.1213847
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Cassey 2006:&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;[For a repeatable study] a third party must be able to perform a study
using identical methodological proto- cols and analyze the resulting
data in an identical manner... [Further] a published result
must be presented in a manner that allows for a quantitative
comparison in a later study...

We consider a study reproducible if, from the information presented in
the study, a third party could replicate the re- ported results
identically. 

CASSEY, P., &amp;amp; BLACKBURN, T. M. (2006). Reproducibility and
Repeatability in Ecology. BioScience,
56(12), 958. doi:10.1641/0006-3568
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;To fully endorse any scientific claims, the experimental findings should be completely repeatable by many independent investigators who âaddress areplicated hypothesis and build up evidence for or against itâ (Peng, 2011). It is important to note that the exact results need not be computed in a repeatable study. This is because experimentation involves probability, and if performed again, with a different sample and new set of measurement errors some variance between experiments is to be expected.&lt;/p&gt;
</description>
				<published>2016-01-12 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/we-have-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication/</link>
			</item>
		
			<item>
				<title>Exemplars of distributing data and code - rOpenSci</title>
				<description>&lt;p&gt;The practice of distributing data and code has a wide variety of possible approaches.  There are many resources available to be used to post data and code to the internet for dissemination, and it is very easy to access these resources.  It is more difficult to find exemplars of how data and code are easily and effectively distributed.  I am conducting a review of some of the resources that describe procedures for this and present exemplars in this and following notes.&lt;/p&gt;

&lt;p&gt;A paper that describes the rOpenSci project's approach is &lt;a href=&quot;http://dx.doi.org/10.5334/jors.bu&quot;&gt;http://dx.doi.org/10.5334/jors.bu&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;Boettiger, C., Chamberlain, S., Hart, E., &amp;amp; Ram,
K. (2015). Building Software, Building Community: Lessons from the
rOpenSci Project. Journal of Open Research Software,
3(1). 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This paper is focused on the way the community development and capacity building part of the project was conducted.  The diagram shown below is introduced as an example of the style that rOpenSci recommend a data analysis workflow be constructed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/workflow-ropensci.png&quot; alt=&quot;/images/workflow-ropensci.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The focus on publishing data to a public repository so early in the project (prior to final analysis and manuscript) seems premature to me. But then, I do feel that I am somewhat more concerned with vexatious activity by climate skeptics than the rOpenSci team.&lt;/p&gt;
</description>
				<published>2016-01-03 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2016/01/exemplars-of-distributing-data-and-code-ropensci/</link>
			</item>
		
			<item>
				<title>My framework of scientific workflow and integration software for holistic data analysis</title>
				<description>&lt;p&gt;Scientific workflow and integration software for holistic data analysis (SWISH) is a
title I have given to describe the area of my research that focuses on the tools and techniques
of reproducible data analysis.&lt;/p&gt;

&lt;p&gt;Reproducibility is the ability to recompute the results of a data
analysis with the original data.  It is possible to have analyses that
are reproducible with varying degrees of difficulty. A data
analysis might be reproducible but require thousands of hours of work to
piece together the datasets, transformations, manipulations, calculations and interpretations of computational results.
A primary challenge to reproducible data analysis is to make analyses
that are &lt;em&gt;easy&lt;/em&gt; to reproduce.&lt;/p&gt;

&lt;p&gt;To achieve this, a guiding principle is that analysts should
effectively implement 'pipelines' of method steps and tools.  Data
analysts should employ standardised and evidence-based methods based
on conventions developed from many data analysts approaching the
problems in a similar way, rather than each analyst configuring
pipelines to suit particular individual or domain-specific
preferences.&lt;/p&gt;

&lt;h2&gt;Planning and implementing a pipeline&lt;/h2&gt;

&lt;p&gt;It can be much easier to conceptualise a complicated data analysis
method than to implement this as a reproducible research pipeline. The
most effective way to implement a pipeline is by methodically tracking
each of the steps taken, the data inputs needed and all the outputs of
the step.  If done in a disciplined way then the analyst or some other
person could 'audit' the procedure easily and access the details of
the pipeline they need to scrutinise.&lt;/p&gt;

&lt;h3&gt;Toward a standardised data analysis pipeline framework&lt;/h3&gt;

&lt;p&gt;In my own work I have tried a diverse variety of configurations based on
things I have read and discussions I have had.  Coming to the end of
my PhD project I have reflected on the framework that I have arrived at and
present this below as a schematic overview.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*   /home/
**    /overview.org 
           - summary data_inventory
           - DMP
**    /worklog.org    
           - YYYY-MM-DD
*   /projects/
**    /project1_data_analysis_project_health_research
***       /dataset1_merged_health_outcomes_and_exposures
             - index.org
             - git (local private, gitignore all subfolders)
             - workplan
             - worklog
             - workflow
             - main.Rmd
****         /data1_provided
****         /data2_derived
*****            - workflow script
****         /code
****         /results/  (this has all the pathways explored)
*****           - README.md
                 - git (public Github)
                 /YYYY-MM-DD-shortname (i.e. EDA, prelim, model-selection, sensitivity)
                     /main.Rmd
                     /code/
                     /data/
****         /report/
                   /manuscript.Rmd
                     - main results recomputed in production/publication quality
                     - supporting_information (but also can refer to github/results)
                 /figures_and_tables/
                     - png
                     - csv
*****           /journal_submission/
                     - cover letter
                     - approval signatures
                     - submitted manuscript
*****           /journal_revision/
                     - response.org
**    /project2_data_analysis_project_exposure_assessment
           - index.org
           - git
***       /dataset2.1_monitored_data
              - workplan
              - worklog
              - workflow
****         /data1_provided
****         /data2_derived 
                 - stored here or
                 - web2py crud or
                 - geoserver
              /data1_and_data2_backups
              /reports/
                 - manuscript.Rmd -&amp;gt; publish with the data somehow
              /tools (R package)
                 - git/master -&amp;gt; Github
****      /dataset2.2_GIS_layers 
**    /methods_or_literature_review_project
*  /tools/
         /web2py
             /applications
                 /data_inventory
                     - holdings
                     - prospective
                 /database_crud
          /disentangle (R package)
          /pipeline_templates
**   /data/
         /postgis_hanigan
         /postgis_anu_gislibrary
         /geoserver_anu_gislibrary
**   /references/
         - mendeley
         - bib
         - PDFs annotated
**   /KeplerData/workflows/MyWorkflows/
***      /data_analysis_workflow_using_kepler (implemented as an R package)
****         /inst/doc/A01_load.R
***      /data_analysis_workflow_using_kepler (implemented as an R LCFD workflow)
             - main.Rmd (raw R version)
             - main.xml (this is kepler)
****         /data/
                 - file1.csv
                 - file2.csv
****         /code/
                 - load.R
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-12-22 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/12/my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis/</link>
			</item>
		
	</channel>
</rss>