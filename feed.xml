<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>Recology</title>
		<description>An exploration of using R for ecology, evolution, and open science.</description>
		<link>http://schamberlain.github.com</link>
		
			<item>
				<title>git-can-be-simple-or-very-complicated</title>
				<description>&lt;ul&gt;
&lt;li&gt;Git is a Distributed Version Control System.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;centerforopenscience.org&quot;&gt;centerforopenscience.org&lt;/a&gt; has developed the Open Science Framework  which is they say &quot;a simplified front end to the powerful and popular version control system Git&quot;.&lt;/li&gt;
&lt;li&gt;I use Github a lot for extending the local features into an online space&lt;/li&gt;
&lt;li&gt;So I finally got around to poking the open science framework with the &lt;a href=&quot;https://openscienceframework.org/project/pyts3/&quot;&gt;Hutchinson Drought Index project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It turns out to be too simplified, and not have very many of the feature I love about Git and GitHub :-(&lt;/li&gt;
&lt;li&gt;For eg it is not really distributed in that you don't get to sync your local repo with the onlin version&lt;/li&gt;
&lt;li&gt;you upload a script or dataset, then you continue editing locally until you want to commit and then you have to upload again, one file at a time with the GUI rather than &quot;git add .&quot; and &quot;git push&quot;.&lt;/li&gt;
&lt;li&gt;I recommend having a look, it might work for you, but if you want more power checkout Yihui's suggestions for using github &lt;a href=&quot;http://yihui.name/en/2011/12/how-to-become-an-efficient-and-collaborative-r-programmer/&quot;&gt;http://yihui.name/en/2011/12/how-to-become-an-efficient-and-collaborative-r-programmer/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and &lt;a href=&quot;http://yihui.name/en/2013/06/fix-typo-in-documentation/&quot;&gt;http://yihui.name/en/2013/06/fix-typo-in-documentation/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;In general I don't think simple front-end's should be a barrier to accessing a sophisticated back-end!&lt;/h4&gt;
</description>
				<published>Tue Nov 19 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/git-can-be-simple-or-very-complicated/</link>
			</item>
		
			<item>
				<title>nectar cloud pumilio build got bogged down</title>
				<description>&lt;p&gt;I've been trying to build pumilio bioacoustics server on a Aust Nectar Research Cloud VM, but hit various roadblocks.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;built with NeCTAR Ubuntu 12.04.2 (Precise)&lt;/li&gt;
&lt;li&gt;these issues (especially the apt-get install r-cran-mysql etc ) might not occure with later Ubuntu?&lt;/li&gt;
&lt;li&gt;test NeCTAR Ubuntu 12.10 (Quantal) or&lt;/li&gt;
&lt;li&gt;NeCTAR Ubuntu 13.04 (Raring) ??&lt;/li&gt;
&lt;li&gt;TODO FIX issues with python audio lab&lt;/li&gt;
&lt;li&gt;TODO swapfile&lt;/li&gt;
&lt;li&gt;TODO Mount the larger storage&lt;/li&gt;
&lt;li&gt;TODO add swapfile&lt;/li&gt;
&lt;li&gt;TODO fix R install etc&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The Github code (in the gh-pages branch) and published as &lt;a href=&quot;http://ivanhanigan.github.io/pumilio-bushfm/&quot;&gt;a report at this link&lt;/a&gt;.  The Source Code is available to Clone or Fork at &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm&quot;&gt;This Github Repo&lt;/a&gt; and is at a stage that most of the installation and configuration is documented to a point where a test sound file has successfully been uploaded.&lt;/p&gt;

&lt;h4&gt;HELP WANTED&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Any researcher at an Australia university could follow the instructions on their own Nectar Cloud VM.&lt;/li&gt;
&lt;li&gt;If anybody out there is interested in bioacoustics, R, Linux or web data archives please help&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Background to pumilio, bushfm and this project&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/birdcombined.png&quot; alt=&quot;birdcombined.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&quot;pumilio%20project&quot;&gt;http://pumilio.sourceforge.net/&lt;/a&gt; is by Luis J. Villanueva-Rivera and Bryan C. Pijanowski. (2012. Pumilio: A Web-Based Management System for Ecological Recordings. Bulletin of the Ecological Society of America 93: 71-81. doi: 10.1890/0012-9623-93.1.71)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;Bush-fm%20project&quot;&gt;http://www.bush.fm/&lt;/a&gt; aims to provide the research community with a portal to national scale acoustic sensor data repositories, and a suite of tools to perform analysis and reporting on these data.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;(Images from http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0000M4)&lt;/p&gt;
</description>
				<published>Mon Nov 18 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/nectar-cloud-pumilio-build-got-bogged-down/</link>
			</item>
		
			<item>
				<title>really-useful-r-upcase-string</title>
				<description>&lt;p&gt;Here is a really useful R snippet from  &lt;a href=&quot;http://stackoverflow.com/a/6364905&quot;&gt;http://stackoverflow.com/a/6364905&lt;/a&gt; with a minor modification to allow differnt splits&lt;/p&gt;

&lt;h4&gt;Code:r-upcase-string&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(&quot;The&quot;, &quot;quick&quot;, &quot;Brown&quot;, &quot;fox/lazy dog&quot;)

simpleCap &amp;lt;- function(x, tosplit = &quot; &quot;) {
  s &amp;lt;- strsplit(x, tosplit)[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep=&quot;&quot;, collapse=tosplit)
}
sapply(x, simpleCap)
sapply(x, simpleCap, tosplit = &quot;/&quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>Sun Nov 17 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/really-useful-r-upcase-string/</link>
			</item>
		
			<item>
				<title>pumilio-bushfm-test-dev-prod</title>
				<description>&lt;h1&gt;Testing the pumilio-bushfm-test-dev-prod build process, in an Open Notebook&lt;/h1&gt;

&lt;h4&gt;Aims:&lt;/h4&gt;

&lt;p&gt;It was suggested I could document the pumilio test build as an OpenNotebook.
I imagined that I could link this blog to github repo and doco hosted on gh-pages.&lt;/p&gt;

&lt;h4&gt;Methods:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;For ie the emacs html output now goes to &lt;a href=&quot;http://ivanhanigan.github.io/pumilio-bushfm/&quot;&gt;http://ivanhanigan.github.io/pumilio-bushfm/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;then collaborators can clone/fork &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm&quot;&gt;https://github.com/ivanhanigan/pumilio-bushfm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and comment/complain at &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm/wiki&quot;&gt;https://github.com/ivanhanigan/pumilio-bushfm/wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Results:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I did a test build a month ago on an old laptop sitting around, then rebuilt on the Nectar cloud&lt;/li&gt;
&lt;li&gt;Unfortunately I didn't realise that the Nectar VM had mounted /var on to the smaller root partition (the 40GB 2nd disk is on /mnt).&lt;/li&gt;
&lt;li&gt;then when I tried to upload a big sound file it broke :-(&lt;/li&gt;
&lt;li&gt;I did a bit of reading and whilst I began thinking I'd just need to move the mysql datadir via&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;sudo nano /etc/mysql/my.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;BUT&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;it actually looks like there is a WAV and MP3 file under /var/www/pumilio-2...&lt;/li&gt;
&lt;li&gt;so I think I can &lt;a href=&quot;http://askubuntu.com/questions/39536/how-can-i-store-var-on-a-separate-partition&quot;&gt;just remount /var&lt;/a&gt; onto the larger /mnt secondary disk.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Fri Nov 15 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/pumilio-bushfm-test-dev-prod/</link>
			</item>
		
			<item>
				<title>What Do Scientists Who Write Metadata Use To Do It? And Why?</title>
				<description>&lt;ul&gt;
&lt;li&gt;The extent to which scientists write metadata is probably lower than it ought to be&lt;/li&gt;
&lt;li&gt;The level of metadata written during science projects is probably described generally as 'bare-minimum' and &quot;the minimum needed for one-self to come back to and understand what one did&quot;&lt;/li&gt;
&lt;li&gt;It sometimes seems that even the bare minimum for one-self is not being kept very often&lt;/li&gt;
&lt;li&gt;I argue that the reasons for less-than-adequate metadata can be understood by looking at&lt;/li&gt;
&lt;li&gt;1) the culture of the scienctists displinary background via training&lt;/li&gt;
&lt;li&gt;2) the tools available and&lt;/li&gt;
&lt;li&gt;3) institutional  requirements to produce metadata (both about data or access to data)&lt;/li&gt;
&lt;li&gt;In my ongoing &lt;a href=&quot;http://ivanhanigan.github.io/2013/10/two-main-types-of-data-documentation-workflow/&quot;&gt;series of blog posts I am exploring the tools available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In this post I just wanted to start the discussion about discipline culture and institutional requirements.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Discipline Culture&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I trained in Geography in the age of GIS and this community uses metadata a lot&lt;/li&gt;
&lt;li&gt;Due to the prevalance of the digital map (collection of layers) which is a derivative data output&lt;/li&gt;
&lt;li&gt;Need to know the source of all the layers&lt;/li&gt;
&lt;li&gt;first law of GIS is &quot;garbage in, garbage out&quot;&lt;/li&gt;
&lt;li&gt;I was trained in the ANSLIC standard from the start&lt;/li&gt;
&lt;li&gt;ArcGIS has a tool called ArcCatalog which makes metadata easy to create and view&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Institutional Requirements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The ARC and NHMRC say they are going to require more metadata (and even data deposit)&lt;/li&gt;
&lt;li&gt;Restrictions on data access make it necessary to describe at least the metadata around provision agreements, licence, allowable access&lt;/li&gt;
&lt;li&gt;A supporting management level who value the metadata as research output (alongside a peer reviewed paper metadata pales in comparison)&lt;/li&gt;
&lt;li&gt;My old boss used to say &quot;Work Not Published Is Work Not Done&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;This reminds me of Approaches and Barriers to Reproducible Research&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;In 2011 BiostatMatt (Matt Shotwell) published &lt;a href=&quot;http://biostatmatt.com/uploads/shotwell-interface-2011.pdf&quot;&gt;a survey of biostatisticians&lt;/a&gt;
VUMC Dept. of Biostatistics to assess:&lt;/li&gt;
&lt;li&gt;the prevalence of fully scripted data analyses&lt;/li&gt;
&lt;li&gt;the prevalence of literate programming practices&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To assess the perceived barriers to reproducible research the also asked:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;What The biggest obstacle to always reproducibly scripting your work?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;pre&gt;&lt;code&gt;| Barrier                                                  | Staff | Faculty |
|----------------------------------------------------------+-------+---------|
| No signifcant obstacles.                                 |     8 |      10 |
| I havent learned how.                                    |     0 |       0 |
| It takes more time.                                      |     7 |       7 |
| It makes collaboration difficult (eg. file compatibility)|     4 |       2 |
| The software I use doesnt facilitate reproducibility.    |     0 |       0 |
| Its not always necessary for my work to be reproducible. |     2 |       0 |
| Other                                                    |     2 |       1 |
|----------------------------------------------------------+-------+---------|
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;So what about the Approaches and Barriers to Me Writing Metadata?&lt;/h3&gt;

&lt;p&gt;With a sample size of one I asked myself these questions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;| Q                                                  | A                                                                    |
|----------------------------------------------------+----------------------------------------------------------------------|
| Do I fully document data (to a metadata standard?) | Occasionally, using DDI for high value raw inputs and final products |
| Do I employ data documentation practices           | I use a tool I created to write minimal metadata occasionally        |
| What are the main barriers?                        | takes more time, The software doesnt facilitate, not always necessary|
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The tools need to help write metadata

&lt;ul&gt;
&lt;li&gt;the Institution needs to require metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Shotwell, M.S. and Alvarez, J.M. 2011. Approaches and Barriers to Reproducible Practices in Biostatistics.
http://biostatmatt.com/uploads/shotwell-interface-2011.pdf&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/what-do-scientists-who-write-metadata-use-to-do-it-and-why/</link>
			</item>
		
			<item>
				<title>librarians-and-python</title>
				<description>&lt;p&gt;I stumbled on these posts by &quot;Data Scientist Training for Librarians&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://altbibl.io/dst4l/exploratory-data-analysis-and-statistics-using-pandas-and-matplotlib/&quot;&gt;http://altbibl.io/dst4l/exploratory-data-analysis-and-statistics-using-pandas-and-matplotlib/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://altbibl.io/dst4l/pandas-munging-stats-and-visualization/&quot;&gt;http://altbibl.io/dst4l/pandas-munging-stats-and-visualization/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I;ve been wanting to learn more python.  I don't think it'll be ready for statistical modelling for a while, but I a want to be ready when it is.&lt;/p&gt;

&lt;h4&gt;R Code: get the olive oil dataset&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&quot;pgmm&quot;)
require(pgmm)
data(olive)
write.csv(olive, &quot;~/olive.csv&quot;, row.names = F)
olive &amp;lt;- read.csv(&quot;~/olive.csv&quot;)
names(olive) &amp;lt;- tolower(names(olive))
str(olive)
write.csv(olive, &quot;~/olive.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;OK now reproduce the example&lt;/h3&gt;

&lt;p&gt;I quite like the histograms from the second example.&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;%pylab inline
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.colors as colors

acidlist=['palmitic', 'palmitoleic', 'stearic', 'oleic', 'linoleic', 'linolenic', 'arachidic', 'eicosenoic']
dfsub=df[acidlist].apply(lambda x: x/100.0)
dfsub.head()

rkeys=[1,2,3]
rvals=['South','Sardinia','North']
rmap={e[0]:e[1] for e in zip(rkeys,rvals)}
rmap

fig, axes=plt.subplots(figsize=(10,20), nrows=len(acidlist), ncols=1) # sets up the framework for the graphs. Acidlist is defined elsewhere, and is a list of the acids we’re interested in.
i=0 # Sets a counter to 0

for ax in axes.flatten(): # Starts a loop to go through our plot and render each row

    acid=acidlist[i]
    seriesacid=df[acid] # creates seriesacid and sets it to df[acid], a list of the percent composition of the acid in the current iteration that’s in each olive oil.

    minmax=[seriesacid.min(), seriesacid.max()] # the minimum and maximum values plotted will be the minimum and maximum percentages that we find in the data

    for k,g in df.groupby('region'):  # starts a loop in the loop to plot the values by region
        style = {'histtype':'stepfilled', 'alpha':0.5, 'label':rmap[k], 'ax':ax}
        g[acid].hist(**style)

        ax.set_xlim(minmax)

        ax.set_title(acid)

        ax.grid(False)

        #construct legend
        ax.legend()
    i=i+1 # increments the counter, to move the loop on to the next acid.

    fig.tight_layout()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/acid.png&quot; alt=&quot;acid.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;I am not sure how to do the transparency but the rest of it would make more sense to me with R&lt;/p&gt;

&lt;p&gt;Will try to reproduce in R for head-to-head shoot out.&lt;/p&gt;
</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/librarians-and-python/</link>
			</item>
		
			<item>
				<title>handling-survey-data-with-r</title>
				<description>&lt;p&gt;R is generally very good for handling many different data types but&lt;/p&gt;

&lt;h3&gt;R has problems with survey data&lt;/h3&gt;

&lt;p&gt;This post is a stub about what packages Ive found with methods allowing to handle efficiently survey data: handle variable labels, values labels, and retrieve information about missing values&lt;/p&gt;

&lt;h4&gt;Base R:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;## Not run:
require(foreign)
analyte  &amp;lt;- read.spss(filename, to.data.frame=T) 
varslist &amp;lt;- as.data.frame(attributes(analyte)$variable.labels)
# this gives a pretty useful thing to use
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;While I was digging around in &lt;a href=&quot;http://mephisto.unige.ch/traminer&quot;&gt;TraMineR&lt;/a&gt; I found this link to Dataset, Emmanuel Rousseaux's package for handling, documenting and describing data sets of survey data.&lt;/p&gt;

&lt;h4&gt;Code:Dataset, a package for handling-survey-data-with-r&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;if(!require(Dataset)) install.packages(&quot;Dataset&quot;, repos=&quot;http://R-Forge.R-project.org&quot;);
require(Dataset)
data(dds)
str(dds)
# cool
description(dds$sexe)
# excellent!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;I'm sure there are plenty of other approaches.  I'll add them as I find them'&lt;/p&gt;
</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/handling-survey-data-with-r/</link>
			</item>
		
			<item>
				<title>Github And Reproducible Research Report Casestudy Hutchinson Drought Index</title>
				<description>&lt;h3&gt;Background&lt;/h3&gt;

&lt;p&gt;This is an example of using github for a Reproducible Research Report (RRR) using a casestudy of the  Hutchinson Drought Index.  The project is available under GNU licence at &lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex&quot;&gt;https://github.com/ivanhanigan/HutchinsonDroughtIndex&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Aims&lt;/h3&gt;

&lt;p&gt;This example will clone the repository from github and run the replication codes using the replication datasets.&lt;/p&gt;

&lt;h3&gt;Materials and Methods&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;assumes you are connected to the internet&lt;/li&gt;
&lt;li&gt;assumes you have git, a github account and ssh key set up&lt;/li&gt;
&lt;li&gt;assumes you have R and GDAL configured&lt;/li&gt;
&lt;li&gt;assumes BoM and ABS have kept their data in the locations I specified to download from&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Hutchinsons Drought Index&lt;/h3&gt;

&lt;p&gt;The Hutchinson Drought Index (or Drought Severity Index) is a climatic drought index
that was designed to reflect agricultural droughts using only rainfall data.&lt;/p&gt;

&lt;p&gt;The index was invented by Professor Mike Hutchinson at the ANU in 1992 (1) and
this project includes R codes written to describe the calculations
and also to download data (2,3) to play with.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;h4&gt;Replication Codes to run in the terminal&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:ivanhanigan/HutchinsonDroughtIndex.git ~/data/HutchinsonDroughtIndex
R
setwd(&quot;~/data/HutchinsonDroughtIndex&quot;)
source(&quot;HutchinsonDroughtIndex_go.r&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;WARNING this downloads 5.4MB from BoM and 19.9MB from ABS&lt;/li&gt;
&lt;li&gt;This runs the codes and produces graphs&lt;/li&gt;
&lt;li&gt;an alternative workflow would be to run the sweave file in the reports directory but I dont like that method as much and have set all the evaluation commands to false.&lt;/li&gt;
&lt;li&gt;it will run now that we have the graphs though, to create a report:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;setwd(&quot;reports&quot;)
Sweave(&quot;HutchinsonDroughtIndex_transformations_doc.Rnw&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;which creates a tex file that can create &lt;a href=&quot;/pdfs/HutchinsonDroughtIndex_transformations_doc.pdf&quot;&gt;this pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;but the simplest example I gave above will compute the index and create the graphs (one is shown below)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/CentralWestDrought8283.png&quot; alt=&quot;CentralWestDrought8283.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;So what actually happened?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;This project adheres to the &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;Reichian LCFD model of writing R code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In this the code is bundled into modules Func (tools), Load, Do, Clean (check).  These live in the src directory (following teh White ProjectTemplate model - well almost, He would put tools/func into lib)&lt;/li&gt;
&lt;li&gt;then a main.r (or go.r) script calls these modules&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;go.r&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt; source('src/HutchinsonDroughtIndex_tools.r')
 source('src/HutchinsonDroughtIndex_load.r')
 source('src/HutchinsonDroughtIndex_do.r')
 source('src/HutchinsonDroughtIndex_check.r')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The really interesting bit is &lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex/blob/master/src/HutchinsonDroughtIndex_tools_droughtIndex.r&quot;&gt;a tool written for this project&lt;/a&gt; that is called from within tools.r&lt;/p&gt;

&lt;p&gt;   source('src/HutchinsonDroughtIndex_tools_droughtIndex.r')&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;A keen reader would look in that script to find out exactly what the function does&lt;/li&gt;
&lt;li&gt;A keen author would push that function to an R package (a super keen bean would publish this on CRAN)&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Discussion&lt;/h3&gt;

&lt;h4&gt;Strengths:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;full codes are provided to reproduce the data access, manipulation and analysis&lt;/li&gt;
&lt;li&gt;the gory details are hidden from the casual user&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Weaknesses:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;the important details are hidden from the technical user&lt;/li&gt;
&lt;li&gt;the script depends on a bunch of things that might not be true&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;This is an example of a self-contained RRR&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Smith, D. I, Hutchinson, M. F, &amp;amp; McArthur, R. J. (1992) Climatic and
Agricultural Drought: Payments and Policy. (Centre for Resource and Environmental
Studies, Australian National University, Canberra, Australia).&lt;br/&gt;
http://fennerschool-research.anu.edu.au/spatio-temporal/publications/cres_paper1992.pdf&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bureau of Meteorology High Quality Monthly precipitation data
downloaded on 2012-01-09
from ftp://ftp.bom.gov.au/anon/home/ncc/www/change/HQmonthlyR/HQ_monthly_prcp_txt.tar&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Australian Bureau of Statistics Statistical Divisions 2006
downloaded on 2012-01-09
from http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1259.0.30.0022006?OpenDocument&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Financial support was provided by Professor Tony McMichael's
&quot;Australia Fellowship&quot; from the the National Health and Medical Research Council, via the
National Centre for Epidemiology and Population Health, Australian National University.&lt;/p&gt;
</description>
				<published>Wed Nov 06 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/github-and-reproducible-research-report-casestudy-hutchinson-drought-index/</link>
			</item>
		
			<item>
				<title>climatic-drought-guestpost-by-luciana-porfirio</title>
				<description>&lt;h3&gt;Guest post by Luciana Porfirio with code contributions by Francis Markham&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Recently I encountered a student doing an analysis who was doing everything in excel, and I couldn't contain my mouth and say R would be better... but it isn't a simple task. So any here are some tips.&lt;/li&gt;
&lt;li&gt;There is a table with 62 years * 12 months of rain data in mm. We calculated the cumulative distribution using ecdf: empirical cumulative distribution function. So the table looks like this: Year Jan Feb Mach (...) each cell contains the cum dist.&lt;/li&gt;
&lt;li&gt;We also got the number of months per year with less than X mm of rain. But he needs to know how many months  are between the drought months, regardless the year. So, I thought that converting the data into a ts was going to facilitate the task, but it doesn't, because now I don't' have columns and rows any longer.&lt;/li&gt;
&lt;li&gt;But I struggled to handle a ts object to do for example an ifelse.&lt;/li&gt;
&lt;li&gt;Luckily there are many many tricks about R, and with the code below we solved all his problems (and many months of work in excel).&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/data/raj_rain_data.csv&quot;&gt;Get the example data here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is how it looks like:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;###############################################
require(reshape2)

###############################################
#read csv file with rain BoM data
dat = read.csv('raj_rain_data.csv')

fn &amp;lt;- apply(dat[,2:13], 2, ecdf) # equivalent to Excel's percentrank function, only for cols 2 to 13 but you need to apply the function to each month

#this fun does all the months at once
fn2 = data.frame(t(do.call(&quot;rbind&quot;, sapply(1:12, FUN = function(i) fn[[i]](dat[,i+1]), simplify = FALSE))))
colnames(fn2) = colnames(dat[,2:13])
fn2$year = dat$Year

#if the value is lower than 0.4, retrieves a 1, otherwise 0
fn2$drought = rowSums(ifelse(apply(fn2[,1:12], 2, FUN= function (x) x &amp;lt; 0.4)==TRUE, 1,0))

#if the value is lower than 0.1, retrieves a 1, otherwise 0
fn2$extreme = rowSums(ifelse(apply(fn2[,1:12], 2, FUN= function (x) x &amp;lt; 0.1)==TRUE, 1,0))

str(fn2)
names(fn2)

#ts didn't work - Francis suggested to melt the data.frame # Melt
#fn2 to tall data set
fn2.tall &amp;lt;- melt(fn2, id.vars=&quot;year&quot;)

# Get the year-month into date format
fn2.tall$date &amp;lt;- with(fn2.tall,
                             as.Date(paste(&quot;1&quot;, variable, year), &quot;%d %b %Y&quot;))

# Convert dates into months since 0 BC/AD (arbitrary, but doesn't
# matter)
#I'm not using the month.idx with Ivan's solution (remove)
fn2.tall$mnth.idx &amp;lt;- sapply(fn2.tall$date, function(x){
  12*as.integer(format(x, &quot;%Y&quot;)) + (as.integer(format(x, &quot;%m&quot;)) - 1)
})

# Sort by date
fn2.tall &amp;lt;- fn2.tall[order(fn2.tall$date),]


#Ivan's solution

x&amp;lt;-fn2.tall$value&amp;lt;0.4
xx &amp;lt;- (cumsum(!x) + 1) * x
x2&amp;lt;-(seq_along(x) - match(xx, xx) + 1) * x
fn2.tall$count&amp;lt;-x2

#counts the number of cases of drought
as.data.frame(table(fn2.tall$count))
###############################################
###############################################
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>Fri Nov 01 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/11/climatic-drought-guestpost-by-luciana-porfirio/</link>
			</item>
		
			<item>
				<title>quantum-gis-visualisations</title>
				<description>&lt;ul&gt;
&lt;li&gt;This is a quick post on Quantum GIS for spatial data visualisation&lt;/li&gt;
&lt;li&gt;it is also a follow up on &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/06/test-gislibrary/&quot;&gt;this post about area concordance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quantum GIS is getting pretty good but is still a bit tricky to make good looking maps&lt;/li&gt;
&lt;li&gt;QGIS can use &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/04/quantumgis-and-postgis/&quot;&gt;remote PostGIS geodatabases on the Cloud as the backend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code: use postgis to create area-concordance&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(devtools)
install_github(&quot;gisviz&quot;, &quot;ivanhanigan&quot;)
require(gisviz)
require(swishdbtools)
ch &amp;lt;- connect2postgres2(&quot;gislibrary&quot;)
# make a temporary tablename, to avoid clobbering
temp_table &amp;lt;- swish_temptable(&quot;gislibrary&quot;)
temp_table &amp;lt;- paste(&quot;public&quot;, temp_table$table, sep = &quot;.&quot;)
temp_table
# this is going to be public.foo11c7673416ea

sql &amp;lt;- postgis_concordance(conn = ch, source_table = &quot;abs_sla.nswsla91&quot;,
   source_zones_code = 'sla_id', target_table = &quot;abs_sla.nswsla01&quot;,
   target_zones_code = &quot;sla_code&quot;,
   into = temp_table, tolerance = 0.01,
   subset_target_table = &quot;cast(sla_code as text) like '105%'&quot;, 
   eval = F) 
cat(sql)
dbSendQuery(ch, sql)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;now connect to PostGIS using QGIS &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/04/quantumgis-and-postgis/&quot;&gt;as described in this tute&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and add the layer to the map&lt;/li&gt;
&lt;li&gt;Style it how you like, I also added NSWSLA1991 over the top&lt;/li&gt;
&lt;li&gt;go into the &quot;new print composer&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/qgis-new-print-composer.png&quot; alt=&quot;qgis-new-print-composer.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/qgis-add-new-map.png&quot; alt=&quot;qgis-add-new-map.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;hit the export to image and viola&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/qgis-export-image.png&quot; alt=&quot;qgis-export-image.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Don't forget to clean up the database!&lt;/h3&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;dbSendQuery(ch, sprintf(&quot;drop table %s&quot;, temp_table))
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>Thu Oct 31 00:00:00 +1100 2013</published>
				<link>http://schamberlain.github.com/2013/10/quantum-gis-visualisations/</link>
			</item>
		
	</channel>
</rss>