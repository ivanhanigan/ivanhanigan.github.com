<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>disentangle</title>
		<description>Disentangle Things</description>
		<link>http://ivanhanigan.github.com</link>
		
			<item>
				<title>Reproducible Research And Managing Digital Assets Part 2</title>
				<description>&lt;p&gt;This post is about an effective and simple data management framework for analysis projects.
This post introduces Josh Reich's LCFD framework, originally introduced in this answer on the stack overflow website here &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;http://stackoverflow.com/a/1434424&lt;/a&gt;, and encoded into the makeProject R package &lt;a href=&quot;http://cran.r-project.org/web/packages/makeProject/makeProject.pdf&quot;&gt;http://cran.r-project.org/web/packages/makeProject/makeProject.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Literature Review Approach&lt;/h2&gt;

&lt;p&gt;This series of three posts is a summary of some of the most useful advice I have found based on my experience having implemented in my own work.&lt;/p&gt;

&lt;p&gt;This is the second post in a series of three entries regarding some evidence-based best practice approaches I have reviewed.  I have read many website articles and blog posts on a variety of approaches to the organisation of digital assets in a reporoducible research pipeline.
The material I've gathered in my ongoing search and opportunistic readings regarding best practice in this area have been recommended by practitioners which provides some weight of evidence.  In addition I have implemented some aspects of the many techniques and the reproducibility of my own work has improved greatly.&lt;/p&gt;

&lt;h2&gt;Digital Assets Management for Reproducible Research&lt;/h2&gt;

&lt;p&gt;The digital assets in a reproducible research pipeline include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Publication material (documents, figures, tables, literature)&lt;/li&gt;
&lt;li&gt;Data (raw measurements, data provided, data derived)&lt;/li&gt;
&lt;li&gt;Code (pre-processing, analysis and presentation)&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;How to use the &lt;code&gt;makeProject&lt;/code&gt; package&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;makeProject&lt;/code&gt; R package is designed to create a folder and some R scripts that are useful for generic workflow tasks.&lt;/li&gt;
&lt;li&gt;The theory is very similar to the approach described in the previous post about Scott Long's batch script: wfsetupsingle.bat &lt;a href=&quot;https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets&quot;&gt;https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# choose your project dir
setwd(&quot;~/projects&quot;)   
library(makeProject)
makeProject(&quot;makeProjectDemo&quot;)
#returns
&quot;Creating Directories ...
Creating Code Files ...
Complete ...&quot;
matrix(dir(&quot;makeProjectDemo&quot;))
#[1,] &quot;code&quot;       
#[2,] &quot;data&quot;       
#[3,] &quot;DESCRIPTION&quot;
#[4,] &quot;main.R&quot;     
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;This has set up some simple and sensible tools for a data analysis.&lt;/li&gt;
&lt;li&gt;Let's have a look at the &lt;code&gt;main.R&lt;/code&gt; script. This is the one file that is used to run all the modules of the project, found in the R scripts in the &lt;code&gt;code&lt;/code&gt; folder.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Project: makeProjectDemo
# Author: Your Name
# Maintainer: Who to complain to &amp;lt;yourfault@somewhere.net&amp;gt;

# This is the main file for the project
# It should do very little except call the other files

### Set the working directory
setwd(&quot;/home/ivan_hanigan/projects/makeProjectDemo&quot;)


### Set any global variables here
####################



####################


### Run the code
source(&quot;code/load.R&quot;)
source(&quot;code/clean.R&quot;)
source(&quot;code/func.R&quot;)
source(&quot;code/do.R&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I think that is very self-explanatory, but it does need some demonstration.  The next instalment in this three part blog post will describe the ProjectTemplate approach.  After that I will demonstrate ways that each of the three approaches can be used.&lt;/p&gt;
</description>
				<published>2015-09-26 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets-part-2/</link>
			</item>
		
			<item>
				<title>Reproducible Research And Managing Digital Assets (1/3)</title>
				<description>&lt;p&gt;This will be a series of three posts that describe some key evidence based best practice methods that have helped me plan and organise files and folders for data analysis.  I have found these via books and on websites.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scott Long's Workflow for Data Analysis with Stata&lt;/li&gt;
&lt;li&gt;Josh Reich's Least Commonly Fouled up Data analysis (LCFD) framework&lt;/li&gt;
&lt;li&gt;John Myles White's ProjectTemplate&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Toward evidence based best-practice data management systems&lt;/h2&gt;

&lt;p&gt;It is important for open science to have effective management of digital assets across the different phases of the research pipeline.  The traditional research pipeline moves from steps of hypothesis and design, measured data, analytic data, computational results (for figures, tables and numerical results), and reports (text and formatted manuscript).  Reproducible research pipelines extend traditional research by encoding the steps in code from a computer ‘scripting’ language, and distributing the data and code with publications.&lt;/p&gt;

&lt;p&gt;In this research pipeline context there are a large number of potential ways to manage digital assets (documents, data and code).  There are also many different motivating drivers that will affect the way that a scientist or group of scientists choose to manage their data and code.&lt;/p&gt;

&lt;p&gt;To deal with in house data management issues before starting and during analysis/reporting is critical for reproducible research.&lt;br/&gt;
I argue that more effective research pipelines can be achieved if scientists adopt the 'convention over configuration' paradigm and adopt best-practice systems based on evidence.&lt;/p&gt;

&lt;h3&gt;Long, S. (2015). Workflow for Reproducible Results.&lt;/h3&gt;

&lt;p&gt;For ages I was aware of the book from the Stata statistical program publishers:&lt;/p&gt;

&lt;h4&gt;Citation:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Long, J. S. (2008). The Workflow of Data Analysis: 
Principles and Practice. Stata publishing.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Recently I stumbled across more recent workshop slides and tutorial material which I will discuss briefly.&lt;/p&gt;

&lt;h4&gt;Citation:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Long, S. (2015). Workflow for Reproducible Results. 
IV : Managing digital assets Workflow for Tools for your WF. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Retrieved from &lt;a href=&quot;http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf&quot;&gt;http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Long suggests a lot of practical things to do, but I will just focus here on the recommended file and folder structure:&lt;/p&gt;

&lt;h4&gt;Recommended project directory structure:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;\ProjectAcronym
    \- History starting YYYY-MM-DD
    \- Hold then delete 
    \Admin
    \Documentation 
    \Posted
         \Paper 1
             \Correspondence 
             \Text
             \Analysis
    \PrePosted 
    \Resources 
    \Write 
    \Work
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In another workshop report Long provides a useful tool to automatically create this structure on windows&lt;/li&gt;
&lt;li&gt;Long, S. (2012). Principles of Workflow in Data Analysis.
Retrieved from &lt;a href=&quot;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&quot;&gt;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;a bash version would be useful for linux and mac users, but also the R language can do this on all platforms with the &lt;code&gt;dir.create&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code: wfsetupsingle.bat&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# wfsetupsingle.bat 
REM workflow talk 2 \ wfsetupsingle.bat jsl 2009-07-12 
REM directory structure for single person.
FOR /F &quot;tokens=2,3,4 delims=/- &quot; %%a in (&quot;%DATE%&quot;) do set CDATE=%%c-%%a-%%b 
md &quot;- History starting \%cdate%&quot; 
md &quot;- Hold then delete &quot; 
md &quot;- Pre posted &quot; 
md &quot;- To clean&quot; 
md &quot;Documentation&quot; 
md &quot;Posted&quot; 
md &quot;Resources&quot;
md &quot;Text\- Versions\&quot; 
md &quot;Work\- To do&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Critical reflections&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This recommendation is very sensible, especially the suggestion of moving things through the pipeline as they evolve from things being worked on (Write/Work) to later phases when they have been polished to a point that they can be put down while preparations for distrubuting them are made (Preposted) and then once they are sent off into downstream publication phases (Posted) they are locked for ever in a archival state.&lt;/li&gt;
&lt;li&gt;I am not particularly keen on the names that have been chosen (Resources, Write and Work are quite ambiguous terms).&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>2015-09-25 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets/</link>
			</item>
		
			<item>
				<title>Organising Graph Nodes And Edges In A Dataframe</title>
				<description>&lt;p&gt;I use the R package &lt;code&gt;DiagrammeR&lt;/code&gt; for creating graphs (the formal kind, connecting nodes with edges)
- This package is great and I like how it interacts with the Graphviz program
- One thing that I like to do in planning and organising data analysis projects is to make graphs and lists of the methods steps, inputs and Outputs
- A simple way to organise these things is in a dataframe (table) with a column for each step (node) and two others for inputs and outputs (edges)
- In my utilities R package &lt;code&gt;github.com/ivanhanigan/disentangle&lt;/code&gt; I have written functions that turn this table into a graphiviz DOT language script
- Recently I have needed to unpack the list for a more itemized view
- Both these functions are showcased below&lt;/p&gt;

&lt;h4&gt;Code: newnode&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# First create some test data, each step is a collection of edges 
# with inputs or outputs simple comma seperated lists
dat &amp;lt;- read.csv(textConnection('
cluster ,  step    , inputs                    , outputs                                , description                      
A  ,  siteIDs      , &quot;GPS, helicopter&quot;          , &quot;spatial, site doco&quot;                 , latitude and longitude of sites  
A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
B  ,  biomass      , spatial                   , biomass_g                              ,                                  
B  ,  correlations , &quot;exposures,trapped_no,biomass_g&quot; , report1                         , A study we published             
C  ,  paper1       , report1                   , &quot;open access repository, data package&quot; ,                                  
D  ,  biomass revision, new estimates          , biomass_g                              , this came late
'), stringsAsFactors = F, strip.white = T)    
str(dat)
# dat

# Now run the function and create a graph
nodes &amp;lt;- newnode(
  indat = dat,
  names_col = &quot;step&quot;,
  in_col = &quot;inputs&quot;,
  out_col = &quot;outputs&quot;,
  desc_col = &quot;description&quot;,
  clusters_col = &quot;cluster&quot;,
  nchar_to_snip = 40
  )  
sink(&quot;Transformations.dot&quot;)
cat(nodes)
sink()
#DiagrammeR::grViz(&quot;Transformations.dot&quot;)
system(&quot;dot -Tpng Transformations.dot -o Transformations.png&quot;)
browseURL(&quot;Transformations.png&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;

&lt;p&gt;That creates this diagram&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Transformations.png&quot; alt=&quot;/images/Transformations.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Now to showcase the tool that itemizes this list of inputs and outputs&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The original table has no capacity to add detail about each node as they are held as a list of inputs and outputs&lt;/li&gt;
&lt;li&gt;To add detail for each we need to unpack each list and create a new table with one row per node&lt;/li&gt;
&lt;li&gt;I decided to make this a long table with an identifier for each node about which step (edge) the node is an input or an output&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code: newnode_csv&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;nodes_graphy &amp;lt;- newnode_csv(
  indat = dat,
  names_col = &quot;step&quot;,
  in_col = &quot;inputs&quot;,
  out_col = &quot;outputs&quot;,
  clusters_col = 'cluster'
  ) 
# which creates this table
knitr::kable(nodes_graphy)
|cluster |name             |in_or_out |value                  |
|:-------|:----------------|:---------|:----------------------|
|A       |siteIDs          |input     |GPS                    |
|A       |siteIDs          |input     |helicopter             |
|A       |siteIDs          |output    |spatial                |
|A       |siteIDs          |output    |site doco              |
|A       |weather          |input     |BoM                    |
|A       |weather          |output    |exposures              |
|B       |trapped          |input     |spatial                |
|B       |trapped          |output    |trapped_no             |
|B       |biomass          |input     |spatial                |
|B       |biomass          |output    |biomass_g              |
|B       |correlations     |input     |exposures              |
|B       |correlations     |input     |trapped_no             |
|B       |correlations     |input     |biomass_g              |
|B       |correlations     |output    |report1                |
|C       |paper1           |input     |report1                |
|C       |paper1           |output    |open access repository |
|C       |paper1           |output    |data package           |
|D       |biomass revision |input     |new estimates          |
|D       |biomass revision |output    |biomass_g              |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This can now be useful for making a 'shopping list' of the data to aquire, transform, analyse or archive.&lt;/p&gt;
</description>
				<published>2015-09-23 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/organising-graph-nodes-and-edges-in-a-dataframe/</link>
			</item>
		
			<item>
				<title>Open Notebook Science, Jekyll Blogs, Github and Jerry Seinfeld's Secret to Productivity</title>
				<description>&lt;p&gt;The other day I reported that I've implemented a new open science task management regime &lt;a href=&quot;http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/&quot;&gt;http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This was instigated by my renewed enthusiasm for open science after a few high profile papers have come out in the last few months imploring scientists to take action on shonky statistics and the &quot;morass of poorly conducted data analyses, with errors ranging from trivial and strange to devastating&quot; (Peng 2015) &lt;a href=&quot;http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x&quot;&gt;http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I believe that making ones electronic notebook open is one of the most obvious and easily achieved things to do toward that ambition.  I also think that keeping the TODO-list in the forefront of ones mind and continuously checking things off the list is a great boost for productivity and keeping on track.  This culminates in the advice to keep momentum by doing something toward the plan on a daily basis, no matter how trivial.  This is sometimes called Jerry Seinfeld's secret to productivity: Just keep at it. Don't break the streak. &lt;a href=&quot;http://dirk.eddelbuettel.com/blog/2014/10/12/&quot;&gt;http://dirk.eddelbuettel.com/blog/2014/10/12/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So what was holding me back from a really useful daily publication of my open notebook?  I showed last post how I manage tasks in Emac Orgmode (a task organiser and calendar/agenda rolled up with code execution for running R scripts etc).  I also write my blog posts in orgmode.&lt;/p&gt;

&lt;p&gt;The only problem with that set up was that I was still using the code from Charlie Park &lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;http://charliepark.org/jekyll-with-plugins/&lt;/a&gt; which adds the inadequate commit description '&lt;code&gt;Latest build&lt;/code&gt;' every time.  What I needed was a way to actually log a summary of work each day, so I can look back over the history and know I actually did something everyday and was not just gaming the system by committing random little non-work additions (I want to balance this by doing &lt;em&gt;some&lt;/em&gt; work every day, but also take time off to read, exercise, socialize, and generally have fun).&lt;/p&gt;

&lt;p&gt;So anyway, the point of this post is to describe my revision to Charlie Park's code for building a jekyll blog:&lt;/p&gt;

&lt;h4&gt;Code: put in ~/.bash_profile&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;function bb() {
  cd ~/projects/ivanhanigan.github.com.raw &amp;amp;&amp;amp; jekyll b &amp;amp;&amp;amp; cp -r    
  ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com &amp;amp;&amp;amp; 
  cd ~/projects/ivanhanigan.github.com &amp;amp;&amp;amp; git add . -A  &amp;amp;&amp;amp; 
  git commit -m &quot;$*&quot; &amp;amp;&amp;amp; 
  git push
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;That bit about &lt;code&gt;$*&lt;/code&gt; was a bit difficult for me to get working as this is the first time I have written a bash script in anger.  The alternative was to use &lt;code&gt;$1&lt;/code&gt; and require the git commit message to be passed within quotes, which also makes sense but I did not do that.&lt;/li&gt;
&lt;li&gt;I also needed to change the terminal settings so that it always loads the bash_profile&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Bash terminal&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Edit &amp;gt; Profile preferences
Title and Command &amp;gt; Run command as a login shell 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;And so now I just have to deposit a markdown blog post into the jekyll &lt;code&gt;_posts&lt;/code&gt; folder and then&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Bash&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;bb Add a meaningful commit message about todays progress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;There you have it, a meaningful message regarding what I have been doing towards my scientific output every day.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seinfeld-streak-day9-1.png&quot; alt=&quot;/images/seinfeld-streak-day9.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seinfeld-streak-day9.png&quot; alt=&quot;/images/seinfeld-streak-day9.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Peng, R. (2015). The reproducibility crisis in science: 
A statistical counterattack. Significance, 12(3), 30–32. 
doi:10.1111/j.1740-9713.2015.00827.x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</description>
				<published>2015-09-22 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity/</link>
			</item>
		
			<item>
				<title>My Newnode R Function Useful For Causal Directed Acyclic Graphs (DAGs)</title>
				<description>&lt;h1&gt;Aims&lt;/h1&gt;

&lt;p&gt;I have worked on a function that turns a &lt;code&gt;data.frame&lt;/code&gt; into a graphviz code in the dot language, with some of my preferred settings.  I realised that it might be useful for causal directed acyclic graphs.&lt;/p&gt;

&lt;p&gt;Causal diagrams are useful for conceptualising the pathways of cause and effect.  These diagrams are sometimes simplly informal pictures but have also been developed in a more formal way to be used in modelling.  These formal developments use concepts derived from the mathmatical abstraction of Graphs (fundamentally Graphs  are networks of linked 'nodes', with the links being termed 'edges').  Causal diagrams can either be constructed to depict two things: first are feedback loops (a vexatious property of complex systems that confounds modelling) while second are more simple chain-of-events type pathways which proceed from an upstream cause to a downstream effect in a single direction, without cycles, called 'Directed Acyclic Graphs or DAGs.  The loop diagrams are out of the scope of this present blog post because the DAGs are much more easily addressed by the tool that I am describing.&lt;/p&gt;

&lt;p&gt;To begin I am going to build on this other guy's blog post on causal DAGs with R
  &lt;a href=&quot;http://donlelek.github.io/2015/03/31/dags-with-r/&quot;&gt;http://donlelek.github.io/2015/03/31/dags-with-r/&lt;/a&gt;
I wanted to add an interface for building these.&lt;/p&gt;

&lt;p&gt;Some background to the concepts that I use are provided in the references below.&lt;/p&gt;

&lt;h1&gt;Materials and Methods&lt;/h1&gt;

&lt;p&gt;The DiagrammeR package which has been integrated within R-Studio has made access to the graphing tool &lt;code&gt;graphviz&lt;/code&gt; much easier than it used to be.  My function &lt;code&gt;causal_dag&lt;/code&gt; (avaiable in my &lt;code&gt;disentangle&lt;/code&gt; github package) essentially constructs the required &lt;code&gt;nodes&lt;/code&gt; and &lt;code&gt;edges&lt;/code&gt; for that package to use.  Optionally we can also include &lt;code&gt;labels&lt;/code&gt; to indicate the direction of the effect.&lt;/p&gt;

&lt;p&gt;To use the tool all you need to do is create a list of &lt;code&gt;edges&lt;/code&gt; and their associated &lt;code&gt;inputs&lt;/code&gt; nodes and &lt;code&gt;outputs&lt;/code&gt; nodes (as a comma separated values string) shown in the picture below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/causal-ssheet.png&quot; alt=&quot;causal-ssheet.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# read in the sheet
library(disentangle)
library(stringr)
causes &amp;lt;- readxl::read_excel(&quot;causal-ssheet.xlsx&quot;)
causes
nodes &amp;lt;- newnode(causes, &quot;edges&quot;, &quot;inputs&quot;, &quot;outputs&quot;)
cat(nodes)
# The result is a formated graph in the dot language with some of my
# preferred settings such as edges showing as 'records' and a spot to
# write a description or include literature about each process
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;See the DOT code in the Appendix&lt;/li&gt;
&lt;li&gt;to render the graph now DiagrammeR can use this text string R object to render this to SVG&lt;/li&gt;
&lt;li&gt;I think it does not do PNG or PDF though so I still use graphviz and dot directly&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;grViz(nodes)

# But I also use graphviz directly to produce a publishable image in
# pdf or png
sink(&quot;reproduce-donlelek.dot&quot;)
cat(nodes)
sink()# If graphviz is installed and on linux call it with a shell command
#system(&quot;dot -Tpdf reproduce-donlelek.dot -o reproduce-donlelek.pdf&quot;)
system(&quot;dot -Tpng reproduce-donlelek.dot -o reproduce-donlelek.png&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Results&lt;/h1&gt;

&lt;p&gt;Here I have reproduced the work of donlelek&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/reproduce-donlelek.png&quot; alt=&quot;reproduce-donlelek.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Future directions&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I'd like to make the edges implicit, so that the spreadsheet keeps track of the information about the causal process, but the graph just shows the lines connecting the nodes&lt;/li&gt;
&lt;li&gt;The edges are where the action is, so I need to add a direction of effect.  This would be in a &lt;code&gt;label&lt;/code&gt; column and added in a [ label = 'abc' ] tag for each edge&lt;/li&gt;
&lt;li&gt;the rankdir option is LR to make this go sideways, which seems more the norm for causal DAGs, left to right.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;References&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Greenland, S., Pearl, J., &amp;amp; Robins, J. M. (1999). Causal diagrams for
epidemiologic research. Epidemiology (Cambridge, Mass.), 10(1),
37–48. doi:10.1097/00001648-199901000-00008

Reid, C. E., Snowden, J. M., Kontgis, C., &amp;amp; Tager, I. B. (2012). The
role of ambient ozone in epidemiologic studies of heat-related
mortality. Environmental Health Perspectives, 120(12),
1627–30. doi:10.1289/ehp.1205251

Newell, B., &amp;amp; Wasson, R. (2001). Social System vs Solar System: Why
Policy Makers Need History. In: Conflict and Cooperation related to
International Water Resources : Historical Perspectives. In World
Water (Vol. 2002).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Appendix&lt;/h1&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#####################################################################
# The following output is automatically created by newnode()
# NOTE for some reason, to show on the blog, I had to replace all { braces with normal (
#####################################################################
digraph transformations (

&quot;Metritis&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Cistic Ovarian Disease&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Age&quot; -&amp;gt; &quot;Fertility effects&quot;
&quot;Fertility effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Fertility effects |  ) ))&quot;]
&quot;Fertility effects&quot; -&amp;gt; &quot;Fertility&quot;


&quot;Metritis&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Retained Placenta&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Age&quot; -&amp;gt; &quot;Cistic Ovarian effects&quot;
&quot;Cistic Ovarian effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Cistic Ovarian effects |  ) ))&quot;]
&quot;Cistic Ovarian effects&quot; -&amp;gt; &quot;Cistic Ovarian Disease&quot;


&quot;Retained Placenta&quot; -&amp;gt; &quot;Metritis effects&quot;
&quot;Metritis effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Metritis effects |  ) ))&quot;]
&quot;Metritis effects&quot; -&amp;gt; &quot;Metritis&quot;


 &quot;Age&quot; -&amp;gt; &quot;Retained Placenta effects&quot;
&quot;Retained Placenta effects&quot;  [ shape=record, label=&quot;(( ( Name | Description ) | ( Retained Placenta effects |  ) ))&quot;]
&quot;Retained Placenta effects&quot; -&amp;gt; &quot;Retained Placenta&quot;


 )
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-09-19 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/my-newnode-r-function-useful-for-causal-directed-acyclic-graphs/</link>
			</item>
		
			<item>
				<title>If You Don't Find A Solution In R, Keep Googling!</title>
				<description>&lt;p&gt;I've learnt this lesson multiple times. It happens like this.  A solution is not immediately obvious in R so you might think of writing your own function.  Generally there is a solution you just did not google enough.
This time I was tricked a little because the GIS functions have been bad for a long time but getting better very rapidly recently.  A little while ago I had a very successful
outcome from using the &lt;code&gt;raster::extract&lt;/code&gt; function on a large raster file
to get the attributes for a set of points.  I needed to do the same
thing but this time for a shapefile and points.  I looked at the
raster package and saw you can use the &lt;code&gt;raster::intersect&lt;/code&gt; function
here, and it worked on the small sample data I tested with but failed
with the big dataset as it ran out of memory.  I assumed that R had not caught up with the GIS world yet and so I came up with this workaround below by splitting the points data layer into chunks.&lt;/p&gt;

&lt;p&gt;I then got access to ArcMap and was wondering whether it could do it, and it DID!
So then I googled a bit and found the solution was simple:&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;sp::over()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Here is my hack in case I ever need to pull out the bit that does the splitting up of the points file, or the tryCatch():&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;big_pt_intersect &amp;lt;- function(pts, ply, chunks = 100){
  idx &amp;lt;- split(pts@data, 1:chunks)
  #str(idx)
  for(i in 1:length(idx)){
  #i = 1
  print(i)
    ids &amp;lt;- idx[ [i] ][,1]
  #str(pts@data)
  qc &amp;lt;- pts[pts@data[,1] %in% ids,]
  #str(qc)
  tryCatch(
    chunk &amp;lt;-  raster::intersect(qc, ply), 
    error = function(err){print(err)})
  if(!exists('chunk_out')){

    chunk_out &amp;lt;- chunk@data
  } else {
    chunk_out &amp;lt;- rbind(chunk_out, chunk@data)
  }
  rm(chunk)

  }
  #str(chunk_out)
  return(chunk_out)
}
# NB warning about split length multiple is not fatal, just due to nonequal chunks 
# (ie the geocodes are 2009/100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</description>
				<published>2015-09-17 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/if-you-dont-find-a-solution-in-r-keep-googling/</link>
			</item>
		
			<item>
				<title>Templates are Needed for Reproducible Research Reports (that Look Good)</title>
				<description>&lt;p&gt;I read with interest the the Transparency and Openness Promotion (TOP) Committee templates for guidelines to enhance transparency in the science that journals publish.&lt;/p&gt;

&lt;h4&gt;Citation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Supplementary Materials for Nosek, B. A., Alter, G., Banks, G. C.,
Borsboom, D., Bowman, S. D., Breckler, S. J., … Yarkoni,
T. (2015). Promoting an open research culture. Science, 348(6242),
1422–1425. doi:10.1126/science.aab2374
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I think though that guidelines like the suggestion to copy-paste bits of the manuscript leave a bit to be desired:&lt;/p&gt;

&lt;h4&gt;Quote;&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Authors document compliance by copy-pasting the relevant passages
in the paper that address the question into the form. For example,
when indicating how sample size was determined, authors copy paste
into the form the text in the paper that describes how sample size
was determined.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Reproducible Research Reports solve this problem by ensuring that the data preparation and analysis are executed in the same script that produces the manuscript, therefore a one-stop-shop for documentation of the entire study.&lt;/p&gt;

&lt;h2&gt;There is a need for Templates of Reproducible Research Reports (that look good!)&lt;/h2&gt;

&lt;p&gt;Rstudio provides very easy support for these documents if you use R.  In particular the option of a menu button to create a new report populates that report with the required header information and some example script to work off.  But the easiest option does not look so good.  This is the Rmarkdown option and it is very user friendly in terms of the markup language needed to write the descriptive language around your analysis (mostly plain text with a few simple options for heading styles etc) rather than the Sweave option which leads to the full blown LaTeX markup language that is a lot more complicated.&lt;/p&gt;

&lt;h4&gt;Boilerplate Rmarkdown header from Rstudio:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;---
title: &quot;Untitled&quot;
author: &quot;Ivan C. Hanigan&quot;
date: &quot;16 September 2015&quot;
output: html_document
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This is great for quick reporting of work as you go, but I  primarily write for output that will be printed (e.g. pdf docs). More specifically, I need the concept of a page, and to have full control over the placement of table and figure ‘environments’, stuff that is easy in LaTeX (once you figure out some of the esoteric parts of that language).&lt;/p&gt;

&lt;p&gt;To achieve a simple writing environment in Markdown but with the powerful layout options of LaTeX I reviewed this guys work but I think it takes it to an uneccessary level of complicated-ness
&lt;a href=&quot;https://github.com/jhollist/manuscriptPackage&quot;&gt;https://github.com/jhollist/manuscriptPackage&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So I went back to some of the old Sweave/Latex templates I had put together and ported it into a markdown header.&lt;/p&gt;

&lt;h4&gt;Boilerplate Rmarkdown header for pretty report&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;---
title: &quot;Untitled&quot;
author: &quot;Ivan C. Hanigan&quot;
date: &quot;16 September 2015&quot;
header-includes:
  - \usepackage{graphicx}
  - \usepackage{fancyhdr} 
  - \pagestyle{fancy} 
  - \usepackage{lastpage}
  - \usepackage{float} 
  - \floatstyle{boxed} 
  - \restylefloat{figure} 
  - \usepackage{url} 
  - \usepackage{color}
  - \lhead{Left Header}
  - \chead{Rmarkdown Rocks}
  - \rhead{\today}
  - \lfoot{Left Footer}
  - \cfoot{Centre Footer}
  - \rfoot{\thepage\ of \pageref{LastPage}}  
output: 
  pdf_document:
    toc: false
documentclass: article
classoption: a4paper
bibliography: references.bib
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Now the layout of tables and figures is done with latex&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Using the xtable package allows results to be displyed in tables
and has built in support for some R objects, so summrising the
linear fit above in ~\ref{ATable}

```{r, results='asis', type = 'tex'}
library(xtable)    
print(xtable(fit, caption=&quot;Example Table&quot;,
  digits=4,table.placement=&quot;ht&quot;,label=&quot;ATable&quot;), comment = F)    
```

## A Plot

Plots intergrate most easily if made seperately as can be seen in figure ~\ref{test}
```{r}
png(&quot;Rmarkdownfig.png&quot;)
plot(x,y,main=&quot;Example Plot&quot;,xlab=&quot;X Variable&quot;,ylab=&quot;Y Variable&quot;)
abline(fit,col=&quot;Red&quot;)
dev.off()
```
\begin{figure}[H]
\begin{center}
\includegraphics[width=.5\textwidth]{Rmarkdownfig.png}
\end{center}
\caption{Some Plot}
\label{test}
\end{figure}
\clearpage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I also realised that if this was to be a full report of a scientific study it would need to include some of the machinery needed for bibliographies.&lt;/p&gt;

&lt;h4&gt;Stuff for bibliographies&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;```{r, echo=F, results = 'hide', message = F, warning=F}
library(&quot;knitcitations&quot;)
library(&quot;bibtex&quot;)
cleanbib()
cite_options(citation_format = &quot;pandoc&quot;, check.entries = FALSE)

bib &amp;lt;- read.bibtex(&quot;C:/Users/Ivan/Dropbox/references/library.bib&quot;)

```

&amp;lt;!--Put data analysis and reporting here, then at the end of the doc--&amp;gt;

```{r, echo=F, message=F, eval=T}
write.bibtex(file=&quot;references.bib&quot;)
```

# References

&amp;lt;!--The bib will then be written following the final subheading--&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I hope this might help others develop their own templates for RRR that look great.&lt;/p&gt;
</description>
				<published>2015-09-16 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/templates-needed-for-reproducible-research-reports-that-look-good/</link>
			</item>
		
			<item>
				<title>task-management-like-an-open-science-hacker</title>
				<description>&lt;p&gt;I just read this impressive paper and it has really given me a push toward making this open lab notebook&lt;/p&gt;

&lt;h4&gt;Citation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Nosek, B. A., et al. (2015). Promoting an open research
culture. Science, 348(6242), 1422–1425. doi:10.1126/science.aab2374
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Quote&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The situation is a classic collective action problem. Many individual researchers lack
strong incentives to be more transparent, even though the credibility of science would 
benefit if everyone were more transparent.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;So I think I'll try to step up the pace of logging my daily scientific work.
One super easy thing to do is to publish my daily log from my task management in orgmode.
Indeed I am also reading at the moment this guy who says&lt;/p&gt;

&lt;h4&gt;Quote&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;The core of your documentation is the research log.

Long, S. (2015). Reproducible Results and the Workflow of Data Analysis. 
Retrieved from http://www.indiana.edu/~jslsoc/ftp/WIM/wf wim 2015 2015-08-21@3.pdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Finally, I was struck by this reference &lt;a href=&quot;http://rich-iannone.github.io/about/2014/10/28/introduction.html&quot;&gt;http://rich-iannone.github.io/about/2014/10/28/introduction.html&lt;/a&gt; to something about 365+ day GitHub streaks. It was covered earlier by Geoff Greer, and by Dirk Eddelbuettel.&lt;/p&gt;

&lt;p&gt;It seems the basic concept is that you can leverage off an obsessive tendency by making sure you do something toward ticking off items from the task list every day.  The impulse to not breaking the chain is supposed to give you inspiration to keep going.  I think this might work well for my temperatment.&lt;/p&gt;

&lt;h2&gt;Emacs and orgmode&lt;/h2&gt;

&lt;p&gt;The set up of my daily log is pretty simple. After being set up by kjhealy's starter kit.
Then I modified the org-agenda-files which was set in the main el file that kjhealy provided  and then with the command C-c a a emacs will display my calendar.&lt;/p&gt;

&lt;p&gt;When I open emacs in the morning I  open the agenda and this also opens research-log file.  I move to that buffer, then I use this key command to insert a new entry for todays date&lt;/p&gt;

&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt; (define-skeleton org-journalentry
   &quot;Template for a journal entry.&quot;
   &quot;project:&quot;
   &quot;*** &quot; (format-time-string &quot;%Y-%m-%d %a&quot;) &quot; \n&quot;
   &quot;**** TODO-list \n&quot;
   &quot;***** TODO \n&quot;
   &quot;**** timesheet\n&quot;
   &quot;#+begin_src txt :tangle work-log.csv :eval no :padline no\n&quot;
   (format-time-string &quot;%Y-%m-%d %a&quot;) &quot;, &quot; str &quot;, 50\n&quot; 
   &quot;#+end_src\n&quot;
 )
 (global-set-key [C-S-f5] 'org-journalentry)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This creates a new date, a stub of a TODO for anything ad hoc and a entry into my timesheet.csv file.&lt;/p&gt;

&lt;p&gt;I then select from TODO items from a global list that I keep at the top of the file, and cut/paste them into todays list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/agenda.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Great so I just moved this research-log orgmode file into my blog github repo, and with the help of charlie park's bash script I am good to go&lt;/p&gt;

&lt;h4&gt;CODE&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;alias build_blog=&quot;cd ~/projects/ivanhanigan.github.com.raw; jekyll b;
cp -r ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com;
cd ~/projects/ivanhanigan.github.com;git add .;git commit -am 'Latest build.';git push&quot;
alias bb=&quot;build_blog&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;So this will put the resulting changes onto my open lab book website here &lt;a href=&quot;https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org&quot;&gt;https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Things to note:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I found this list of tips &lt;a href=&quot;http://natashatherobot.com/streak-github-mistakes/&quot;&gt;http://natashatherobot.com/streak-github-mistakes/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In particular I realise I need to make my daily push by 4:50 PM in Canberra ACT as this is 11:50 PM the previous day for Github, Pacific Time (PT)&lt;/li&gt;
&lt;li&gt;I also will need to ensure I don't publish sensitive (or embarrasing entries).&lt;/li&gt;
&lt;li&gt;I'll try to keep the identity of my collaborators private as well, so just use their initials rather than names.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>2015-09-13 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/task-management-like-an-open-science-hacker/</link>
			</item>
		
			<item>
				<title>how-to-say-why-before-what</title>
				<description>&lt;p&gt;I have discovered a flaw in my writing style.&lt;br/&gt;
I often say what it it before I say why it is important.&lt;/p&gt;

&lt;h4&gt;Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Disentangling health effects of environmental from social factors
is difficult for a variety of reasons. The effort to examine and
to separate environmental and social causes is nevertheless
valuable. [WHY IS IT VALUABLE?] This is especially important to
policy makers and to others who seek to maximise the public
good. A greater understanding of their respective contributions
will lead to more rational, deep-seated, lasting and effective
interventions.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;The caps question is from someone reading my draft.  I need to start with the why.
Perhaps just turn the paragraph on its head?&lt;/p&gt;

&lt;h4&gt;Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;A greater understanding of the respective contributions from
environmental and social factors will lead to more rational,
deep-seated, lasting and effective interventions by policy makers
and to others who seek to maximise the public good.  Disentangling
health effects of environmental from social factors is difficult
for a variety of reasons. The effort to examine and to separate
environmental and social causes is nevertheless valuable.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

</description>
				<published>2015-09-03 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/09/how-to-say-why-before-what/</link>
			</item>
		
			<item>
				<title>tracking-a-data-analysis-pipeline</title>
				<description>&lt;p&gt;I have just uploaded a new version of the windows build for my 'disentangle' package.  The blurb of the  draft vignette is below.&lt;/p&gt;

&lt;h1&gt;Introduction&lt;/h1&gt;

&lt;p&gt;It can be much easier to conceptually understand a complicated data
analysis pipeline than it is to implement that pipeline effectively.
This report outlines the use of the 'disentangle' R package, available from &lt;a href=&quot;http://ivanhanigan.github.io/projects.html&quot;&gt;http://ivanhanigan.github.io/projects.html&lt;/a&gt;.  This package contains functions that were developed to aid data
analysts to map out all the aspects of their work when planning and
conducting complicated data analyses using the pipeline concept.    There are often many steps in the design and analysis of a study and
when these are put together as a data analysis pipeline this addresses
the challenge of reproducibility (Peng 2006).  The
credibility of data analyses requires that every step is able to be
scrutinised (Leek 2015).&lt;/p&gt;

&lt;h2&gt;Motivating scientific questions&lt;/h2&gt;

&lt;p&gt;The type of data analysis that is
the focus of this work is more complicated than simply loading some
data that are already cleaned, fitting some models and reporting some
output.  Typically the type of data analysis projects that these tools
are aimed at involve attempts to control for a large number
of inter-relationships and associations between variables. It is
especially problematic that these variables need to have been selected
by the scientists from a multitude of possible variables and a
plethora of possible data sources, during a long process of data
collection, cleaning, exploration and decision making in preparation
for data analysis. There are also a multitude of steps and decision
points in the process of model building and model checking. The use of
statistical models involving many entangled environmental and social
variables can easily result in spurious association that may be
mistakenly interpreted as causation.  Projects that the author has
been involved in include explorations of hypotheses about health effects from
droughts, bushfire smoke, heat-waves and dust-storms which produced
novel findings, and informed controversial debates about the
implications of climate change. The requirement to adequately convey
the methods and results of this research was problematic and motivated
the work on effective use of reproducible research techniques and data
analysis pipelines.&lt;/p&gt;
</description>
				<published>2015-08-28 00:00:00 +1000</published>
				<link>http://ivanhanigan.github.com/2015/08/tracking-a-data-analysis-pipeline/</link>
			</item>
		
	</channel>
</rss>