<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>disentangle</title>
		<description>Disentangle Things</description>
		<link>http://ivanhanigan.github.com</link>
		
			<item>
				<title>Adopting a bullet point style</title>
				<description>&lt;p&gt;With respect to bullet points:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;there are a range of styles accepted&lt;/li&gt;
&lt;li&gt;you just have to be consistent&lt;/li&gt;
&lt;li&gt;I have decided I like the bullet point style from
&lt;a href=&quot;http://www.monash.edu/about/editorialstyle/editing/punctuation&quot;&gt;http://www.monash.edu/about/editorialstyle/editing/punctuation&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;End the introductory phrase preceding a list of bullet points in a
colon. If the individual bullet points are sentence fragments, don't
use a full stop, comma or semi-colon. Leave it bare until the last
bullet point, and then use a full stop. Don't use capitals. Use full
stops if each bullet point is a complete sentence.
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-11-12 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/adopting-a-bullet-point-style/</link>
			</item>
		
			<item>
				<title>Decisions to make when modelling interactions</title>
				<description>&lt;p&gt;This post focuses on decision making during statistical
modelling.  The case of investigating effect modification is used as
an example.  The analyst has several options available to them when
constructing the model parametrisation for adjustment to explain
modification effectively.  Unlike confounding (in which the criterion
of substantial magnitude change of the effect estimate when
controlling for a third variable can be easily assessed), models
including effect modification can be hard to interpret.  Taking
account of effect modification becomes increasingly important when
modelling complex interactions.&lt;/p&gt;

&lt;p&gt;If an effect moderator is present then the relationship between
exposure and response varies between different levels of the
moderator. An example is provided by a paper we published, in which the
relationship between proximity to wetlands and Ross River virus
infection was found to be different for towns and rural areas, where the
'urban' variable is the 'effect moderator'.&lt;/p&gt;

&lt;p&gt;There are three common ways for data analysts to address this question
in statistical models:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;multiple models for each group,&lt;/li&gt;
&lt;li&gt;interaction terms or&lt;/li&gt;
&lt;li&gt;re-parametrisation to explicitly depict interactions.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The first way is
to split apart the dataset and conduct separate analyses of multiple
groups. For example, one could run the regression in the urban zone
and the rural zone seperately and see if there were different exposure
response functions in the two models.  This was the approach of our paper.
This approach has the strength that it is simple to do and yeilds
results that are easy to interpret.  A limitation of this method is
that by splitting the dataset one loses degrees of freedom and
therefore statistical power.&lt;/p&gt;

&lt;p&gt;The second approach (which removes that limitation of the first option) is to
fit interaction terms.  The example shown in Figure 1 is a multiplicative interaction model (Brambor 2006).
This  approach was not taken in the models reported in our paper, but can easily be implemented and shows that the function of distance was
estimated to have different 'slopes' in each of the dichotomous urban
groups.&lt;/p&gt;

&lt;p&gt;The statistical method can be easily implemented in
software by including a multiplicative term between two variables,
however in practice the resulting post estimation regression outputs
can be difficult to interpret.  For example, say one wants to calculate
the effect and standard error for exposure X on health outcome Y with
the interaction of the effect modifier Z.  The form of this model can
be written as:&lt;/p&gt;

&lt;p&gt;$$ Y ~ \beta&lt;em&gt;{1}X + \beta&lt;/em&gt;{2}Z + \beta_{3}XZ $$&lt;/p&gt;

&lt;p&gt;where B1, B2 and B3 are the regression coefficients estimated and  the term XZ is the interaction between exposure and effect modifier.&lt;/p&gt;

&lt;p&gt;The difficulty for interpretation comes when using this method for calculating the marginal effect of X on Y and the conditional standard error.  The specific method described in Brambor et al. is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calculate the coefficients and the variance-covariance matrix from the regression model&lt;/li&gt;
&lt;li&gt;The marginal effect is $\beta&lt;em&gt;{1} + \beta&lt;/em&gt;{3}XZ$ where Z is the level of the
modifying factor (0 or 1 in the dichotomous effect modifier case)&lt;/li&gt;
&lt;li&gt;The conditional standard error is:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;$$\sqrt{var(\beta&lt;em&gt;{1}) + Z&lt;sup&gt;2&lt;/sup&gt; var(\beta&lt;/em&gt;{3}) + 2Zcov(\beta&lt;em&gt;{1}\beta&lt;/em&gt;{3})}$$&lt;/p&gt;

&lt;p&gt;Therefore the strengths of this approach is that it does not reduce
degrees of freedom and is straightforward to specify the model in
standard statistical software packages. The limitations are related to
interpretation of the resulting coefficients for both the main effects and the marginal effects, and standard errors for these.&lt;/p&gt;

&lt;p&gt;The third approach available to analysts makes it easier to access the
resulting regression output.  This method was employed in Paper 1 in
the final modelling phase in which estimates were calculated for the
different drought exposure-response funcitons in each of the
sub-groups. In the terms of Figure \ref{fig:effectmod.png} it is simple to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calculate X1 = X * Z (i.e = exposure for condition is met, zero otherwise)&lt;/li&gt;
&lt;li&gt;Calculate X0 = X * (1-Z) (i.e. = exposure for NON-condition, zero for condition is TRUE)&lt;/li&gt;
&lt;li&gt;Instead of X, Z and XZ, fit X1, X0 and Z.  This model also contains three parameters and captures the same interactions as it is the same model with a different parametrisation.  The standard errors for the X1 and X0 are calculated directly from the regression.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This method is much easier to implement and interpret.  This is also
considerably more flexible than the other two approaches.  A
limitation remains for this method in that the pre-processing steps
required are more complicated, and there are inherently more
possibilities for the data analyst to make errors in writing their
code as they make these changes to the analytical data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Demonstrating this with the data from the paper (available in table 2)
## the following code shows the different parametrisations for the effect modification by urban
## we show that the coeffs and se are equivalent but that the psuedo-R
## squared will be better when including all our data in stratified analysis

# model 0 effect in eastern
d_eastern
fit &amp;lt;- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_eastern )
summa &amp;lt;- summary(fit)
summa
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept) -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer      -0.24702    0.07921  -3.119  0.00182 **

# model 1 effect in urban
fit1 &amp;lt;- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_urban )
summa &amp;lt;- summary(fit1)
summa
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept) -5.52363    0.33354 -16.561   &amp;lt;2e-16 ***
## buffer       0.03853    0.06352   0.607    0.544

# step 1, combine the urban and rural data
d_eastern$urban &amp;lt;- 0
d_urban$urban &amp;lt;- 1
dat2 &amp;lt;- rbind(d_eastern, d_urban)
str(dat2)
dat2

# model 2 is a multiplicative term
fit2 &amp;lt;- glm(cases ~ buffer * urban + offset(log(1+pops)), family = 'poisson', data = dat2)
summa &amp;lt;- summary(fit2)
summa
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer       -0.24702    0.07921  -3.119  0.00182 **
## urban        -0.69938    0.36350  -1.924  0.05435 .
## buffer:urban  0.28555    0.10153   2.812  0.00492 **

# the coeff on buffer is for urban = 0 is main effect
# the coeff on buffer:urban is for urban = 1 is the marginal effect
b1 &amp;lt;- summa$coeff[2,1]
b3 &amp;lt;- summa$coeff[4,1]
b1 + b3
# 0.0385268
# but what about that p-value?  and the se?
str(fit2)
fit2_vcov &amp;lt;- vcov(fit2)
fit2_vcov
# now calculate the conditional standard error for the marginal effect of buffer for the value of the modifying variable (Z, urban =1)
varb1&amp;lt;-fit2_vcov[2,2]
varb3&amp;lt;-fit2_vcov[4,4]
covarb1b3&amp;lt;-fit2_vcov[2,4]
Z&amp;lt;-1
conditional_se &amp;lt;- sqrt(varb1+varb3*(Z^2)+2*Z*covarb1b3)
conditional_se



# model 3 is the re-parametrisation
dat2$buffer_urban &amp;lt;- dat2$buffer * dat2$urban
dat2$buffer_eastern &amp;lt;- dat2$buffer * (1-dat2$urban)

fit3 &amp;lt;- glm(cases ~ buffer_urban + buffer_eastern + urban + offset(log(1+pops)), family = 'poisson', data = dat2)
summa &amp;lt;- summary(fit3)
summa

## Coefficients:
##                Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)    -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer_urban    0.03853    0.06352   0.607  0.54416
## buffer_eastern -0.24702    0.07921  -3.119  0.00182 **
## urban          -0.69938    0.36350  -1.924  0.05435 .
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-10-31 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/decisions-to-make-when-modelling-interactions/</link>
			</item>
		
			<item>
				<title>Show missingness in large dataframes, version 2</title>
				<description>&lt;ul&gt;
&lt;li&gt;UPDATE: the other day I blogged this but I needed to tweak things, so this is a re-post with extra&lt;/li&gt;
&lt;li&gt;Sometime ago I saw this example of a method for assessing missing data in a large data frame &lt;a href=&quot;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&quot;&gt;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I asked my colleague Grant about doing this in R and he whipped up the following code to generate such an image:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/bankstown_traffic_counts_full_listing_june_2014.svg&quot; alt=&quot;/images/bankstown_traffic_counts_full_listing_june_2014.svg&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;misstable &amp;lt;- function(atable){
 op &amp;lt;- par(bg = &quot;white&quot;)
 plot(c(0, 400), c(0, 1000), type = &quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;,
     main = &quot;Missing Data Table&quot;)


 pmin=000
 pmax=400
 stre=pmax-pmin
 lnames=length(atable)
 cstep = (stre/lnames)
 for(titles in 1:lnames){
 text((titles-1) * cstep+pmin+cstep/2,1000,colnames(atable)[titles])
 }

 gmax=900
 gmin=0
 gstre=gmax-gmin
 rvec = as.vector(atable[ [ 1 ] ])
 dnames=length(rvec)
 step = gstre / dnames
 for(rows in 1:dnames){
 text(30,gmax - (rows-1)*step-step/2,rvec[rows])
 ymax=gmax - (rows-1)*step
 ymin=gmax - (rows)*step
 for(col in 2:lnames-1){
 if(atable[rows,col+1] == F){
 tcolor = &quot;red&quot;
 }
 if(atable[rows,col+1] == T){
 tcolor = &quot;white&quot;
 }
 rect((col) * (stre/lnames)+pmin, ymin, (col+1) * (stre/lnames)+pmin,
 ymax,col=tcolor,lty=&quot;blank&quot;)
 }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Now things to note are that the function expects the data to be TRUE if Not NA and  FALSE if is NA&lt;/li&gt;
&lt;li&gt;so might need to massage things a bit first&lt;/li&gt;
&lt;li&gt;here is the small test Grant supplied&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(grDevices)

# Make a quick dataframe with true/false representing data availability
locs=c(&quot;Australia&quot;,&quot;India&quot;,&quot;New Zealand&quot;,&quot;Sri Lanka&quot;,&quot;Uruguay&quot;,&quot;Somalia&quot;)
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable
#Draw the table.
misstable(atable)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;here is the one I worked on today&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# having defined the input dir and input file tried reading the excel sheet (without head 3 rows)
#dat &amp;lt;- readxl::read_excel(file.path(indir, infile), skip =3)
# got lots of warnings()
## 50: In read_xlsx_(path, sheet, col_names = col_names, col_types = col_types,  ... :
##   [1278, 4]: expecting date: got '[NULL]'
# I always worry about using excel connections so open in excel (in windows) 
# and save as to convert to CSV
dat &amp;lt;- read.csv(file.path(indir, gsub(&quot;.xlsx&quot;, &quot;.csv&quot;, infile)), skip =3, stringsAsFactor = F)
str(dat)
# 'data.frame':     1396 obs. of  167 variables:
# but most of the cols and a third of the rows are empty!
# check missings
dat2 &amp;lt;- data.frame(id = 1:nrow(dat), dat)
str(dat2)
# first if they are empty strings
dat2[dat2 == &quot;&quot;] &amp;lt;- NA
# now if NA
dat2[,2:ncol(dat2)] &amp;lt;- !is.na(dat2[,2:ncol(dat2)])

# Truncate the hundreds of empty cols
str(dat2[,1:18])
tail(dat2[,1:18])
svg(file.path(outdir, gsub(&quot;.csv&quot;, &quot;.svg&quot;, outfile))    )
misstable(dat2[,1:18])
dev.off()
browseURL(file.path(outdir, gsub(&quot;.csv&quot;, &quot;.svg&quot;, outfile))    )

# cool, that is an effective way to look at the data
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-10-28 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/show-missingness-in-large-dataframes-v2/</link>
			</item>
		
			<item>
				<title>show-missingness-in-large-dataframes</title>
				<description>&lt;p&gt;Sometime ago I saw this example of a method for assessing missing data in a large data frame &lt;a href=&quot;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&quot;&gt;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I asked my colleague Grant about doing this in R and he whipped up the following code to generate such an image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/misstable.png&quot; alt=&quot;/images/misstable.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;misstable &amp;lt;- function(atable){
 op &amp;lt;- par(bg = &quot;white&quot;)
 plot(c(0, 400), c(0, 1000), type = &quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;,
     main = &quot;Missing Data Table&quot;)


 pmin=000
 pmax=400
 stre=pmax-pmin
 lnames=length(atable)
 cstep = (stre/lnames)
 for(titles in 1:lnames){
 text((titles-1) * cstep+pmin+cstep/2,1000,colnames(atable)[titles])
 }

 gmax=900
 gmin=0
 gstre=gmax-gmin
 rvec = as.vector(atable[ [ 1 ] ])
 dnames=length(rvec)
 step = gstre / dnames
 for(rows in 1:dnames){
 text(30,gmax - (rows-1)*step-step/2,rvec[rows])
 ymax=gmax - (rows-1)*step
 ymin=gmax - (rows)*step
 for(col in 2:lnames-1){
 if(atable[rows,col+1] == F){
 tcolor = &quot;red&quot;
 }
 if(atable[rows,col+1] == T){
 tcolor = &quot;white&quot;
 }
 rect((col) * (stre/lnames)+pmin, ymin, (col+1) * (stre/lnames)+pmin,
 ymax,col=tcolor,lty=&quot;blank&quot;)
 }
 }
}

require(grDevices)

# Make a quick dataframe with true/false representing data availability
locs=c(&quot;Australia&quot;,&quot;India&quot;,&quot;New Zealand&quot;,&quot;Sri Lanka&quot;,&quot;Uruguay&quot;,&quot;Somalia&quot;)
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable
#Draw the table.
misstable(atable)
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-10-26 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/show-missingness-in-large-dataframes/</link>
			</item>
		
			<item>
				<title>Sanitize mendeley references in r markdown reporting</title>
				<description>&lt;p&gt;A key challenge for Reproducible Research Reports in Rmarkdown remains adequate scholarly citation management; the machinery of scholarly citations and references.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;knitcitations&lt;/code&gt; R package does a great job of working with bibtex bibliography files, however the bibtex manager that I use is Mendeley and it has implemented some rules on the way it handles special characters that forces the bibtex references into a state with some 'escaped' elements that breaks their presentation via &lt;code&gt;knitcitations&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is a post from my open notebook that shows the workaround I am using for sanitizing the Mendeley escaped underscores in URLS.&lt;/p&gt;

&lt;p&gt;Here is an example:&lt;/p&gt;

&lt;p&gt;There is considerable public health impact from the effects on mental of drought &lt;span class=&quot;citation&quot;&gt;(&lt;span&gt;Sarathi Biswas&lt;/span&gt; 2012)&lt;/span&gt;. It is proposed that the best method to disentangle the multifactorial nature of this causal mechanism is the ‘five-capitals’ framework, indeed this method may even enable understanding the human carrying capacity of ecosystems &lt;span class=&quot;citation&quot;&gt;(McMichael &amp;amp; Butler 2002)&lt;/span&gt;.&lt;/p&gt;




&lt;p&gt;McMichael, A.J. &amp;amp; Butler, C.D. (2002). Global health trends: Evidence for and against sustainable progress. &lt;em&gt;International Union for the Scientific Study of Population Committee on Emerging Health Threats&lt;/em&gt;. &lt;a href=&quot;http://www.demogr.mpg.de/papers/workshops/020619{\_}paper25.pdf&quot;&gt;http://www.demogr.mpg.de/papers/workshops/020619{\_}paper25.pdf&lt;/a&gt; [21 Sep. 2003]&lt;/p&gt;


&lt;p&gt;&lt;span&gt;Sarathi Biswas&lt;/span&gt;, P. (2012). Alcohol, drought lead to farmer’s suicide. &lt;em&gt;Daily News and Analysis&lt;/em&gt;. &lt;a href=&quot;http://www.dnaindia.com/pune/report{\_}alcohol-drought-lead-to-farmers-suicide{\_}1688976&quot;&gt;http://www.dnaindia.com/pune/report{\_}alcohol-drought-lead-to-farmers-suicide{\_}1688976&lt;/a&gt; [17 May 2012]&lt;/p&gt;




&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;See those pesky curly braces &lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt; around the underscores?&lt;/h2&gt;

&lt;h2&gt;The fix&lt;/h2&gt;

&lt;p&gt;The fix I am using is to sanitize each record where this is an issue as I build my document, so the mendeley version stays as-is, while the R version has been sanitized by removing the escape characters.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# read mendeley bibtex file
bib &amp;lt;- read.bibtex(&quot;~/references/library.bib&quot;)
# ad hoc fix
for(bibkey in c(&quot;SarathiBiswas2012&quot;, &quot;Mcmichael2002a&quot;)){
  bib[ [ bibkey ] ]$url &amp;lt;- gsub(&quot;\\{\\\\_\\}&quot;,&quot;_&quot;, bib[ [ bibkey ] ]$url)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the result:&lt;/p&gt;

&lt;p&gt;McMichael, A.J. &amp;amp; Butler, C.D. (2002). Global health trends: Evidence for and against sustainable progress. &lt;em&gt;International Union for the Scientific Study of Population Committee on Emerging Health Threats&lt;/em&gt;. &lt;a href=&quot;http://www.demogr.mpg.de/papers/workshops/020619_paper25.pdf&quot;&gt;http://www.demogr.mpg.de/papers/workshops/020619_paper25.pdf&lt;/a&gt; [21 Sep. 2003]&lt;/p&gt;


&lt;p&gt;&lt;span&gt;Sarathi Biswas&lt;/span&gt;, P. (2012). Alcohol, drought lead to farmer’s suicide. &lt;em&gt;Daily News and Analysis&lt;/em&gt;. &lt;a href=&quot;http://www.dnaindia.com/pune/report_alcohol-drought-lead-to-farmers-suicide_1688976&quot;&gt;http://www.dnaindia.com/pune/report_alcohol-drought-lead-to-farmers-suicide_1688976&lt;/a&gt; [17 May 2012]&lt;/p&gt;




&lt;p&gt;&lt;/p&gt;

</description>
				<published>2015-10-24 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/sanitize-mendeley-references-in-r-markdown-reporting/</link>
			</item>
		
			<item>
				<title>Keeping an electronic lab notebook for computational statistics projects</title>
				<description>&lt;ul&gt;
&lt;li&gt;In my previous post on this topic &lt;a href=&quot;http://ivanhanigan.github.io/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/&quot;&gt;http://ivanhanigan.github.io/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/&lt;/a&gt; I summarised some recommendations for electronic lab notebooks&lt;/li&gt;
&lt;li&gt;I've collated from a variety of sources for managing computational statistics projects in a reproducible research Pipelines&lt;/li&gt;
&lt;li&gt;One thing I found while reading the literature around this topic is that the concepts are difficult to really grasp until I see them being used&lt;/li&gt;
&lt;li&gt;The example worklog from Scott Long was a good insight into his method, that I really got more out of the figure below than from descriptions of the concept&lt;/li&gt;
&lt;li&gt;screen shot taken from Long, S. (2012). Principles of Workflow in Data Analysis. Retrieved from &lt;a href=&quot;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&quot;&gt;http://www.indiana.edu/~wim/docs/2012-long-slides.pdf&lt;/a&gt; (accessed 2015-10-23).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/worklog-long.png&quot; alt=&quot;/images/worklog-long.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;I added a red box around an important aspect of this example, the communication of gory details, that are often difficult to track if not using a notebook to log our work&lt;/li&gt;
&lt;li&gt;ARGH! indeed.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Replication from Noble's description&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;That looks good, but I also really liked the description in Noble's paper where scripts that do computations are linked to log entries&lt;/li&gt;
&lt;li&gt;This was something that I felt I wanted to see in action&lt;/li&gt;
&lt;li&gt;Without an example online, I had to have a go at creating one from the instructions&lt;/li&gt;
&lt;li&gt;I also had to make some modifications to the method because I want to have this set up work in a multidisciplinary team with some users on windows and others on linux, sharing the project on a network folder&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;My paraphrasing of Noble's description&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Worklog: this is the main file, like a 'lab notebook' for the analyst to track their work.  This  document resides in the root of the project directory and records your progress in detail. Entries in the notebook should be dated, and they should be relatively verbose, with links or embedded images or tables displaying the results of the experiments that you performed. In addition to describing precisely what you did, the notebook should record your observations, conclusions, and ideas for future work&lt;/li&gt;
&lt;li&gt;For group work, this can also contain a 'working' folder for each person to store their messy day-to-day files that we don't want to clutter up the main folder (eg 'working_ivan')&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Conventions I used for writing the worklog entries are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Names follow this structure [**] [date in ISO 8601 YYYY-MM-DD] [meeting/notes/results] [with/from UserName] [Re: topic shortname]&lt;/li&gt;
&lt;li&gt;'meetings' are for both agenda preparation and also notes of discussion&lt;/li&gt;
&lt;li&gt;'notes' are such things as emailed information or ad hoc Discovery&lt;/li&gt;
&lt;li&gt;'results' are entries related to a section of the 'results' folder. That is, this kind of entry is in parallel to the results entry (see below), however the log contains a prose description of the experiment, whereas the results folder contains scripts etc of all the gory details.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Tests&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I use Emacs on linux for most of my work but I need to share with windows users so tested out keeping the log in a MS word doc.  This got corrupted quickly I think because I edited in Libre office.&lt;/li&gt;
&lt;li&gt;I decided to try and just use a plain text format&lt;/li&gt;
&lt;li&gt;text files created in Ubuntu are so difficult to understand (read) when opened in Windows' Notepad. No matter how many lines have been used, all the lines appear in the same one line.
To set the buffer coding to DOS style issue:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;code&gt;M-x set-buffer-file-coding-system utf-8-dos&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;a couple of examples&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;As this is a plain text document opening it in emacs will not automatically render it in the Orgmode fashion&lt;/li&gt;
&lt;li&gt;To achieve this the command is &lt;code&gt;M-x org-mode&lt;/code&gt; and the file looks like below&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/worklog-ivan1.png&quot; alt=&quot;/images/worklog-ivan1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;From here I can keep adding new entries at the bottom, and have a section for URGENT ACTION a the top&lt;/li&gt;
&lt;li&gt;Orgmode can expand the entries by moving to that line and hitting TAB, or use the command &lt;code&gt;C-u C-u C-u TAB&lt;/code&gt; to expand all branches&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/worklog-ivan2.png&quot; alt=&quot;/images/worklog-ivan2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;the 'Experiment Results' level is about work you might do on a single day, or over a week or two&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Each results subfolder would have workflow scripts that does the work&lt;/li&gt;
&lt;li&gt;At this level each 'experiment' is written up in chronological order&lt;/li&gt;
&lt;li&gt;It is recommended to store every command used while performing the experiment preferably as an executable script that carries out the entire experiment automatically&lt;/li&gt;
&lt;li&gt;you should end up with a file that is parallel to the worklog entry&lt;/li&gt;
&lt;li&gt;The worklog contains a prose description of the experiment, whereas the driver script contains all the gory details&lt;/li&gt;
&lt;li&gt;this is the level I usually think of the distribution side of things&lt;/li&gt;
&lt;li&gt;You may want to pack up the results from one of these folders and email it to the collaborators, or decide on the one set of tables and figures to write into the manuscript for submission to a journal&lt;/li&gt;
&lt;li&gt;If this is accepted for publication, this is the one combined package of 'analytical data and code' that I would consider putting up online as supporting information for the paper.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Example&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I followed Noble's advice to create a driver script to set up the folder structure:&lt;/li&gt;
&lt;li&gt;it is in my Github R package &lt;code&gt;disentangle&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;&amp;gt; dir.create(&quot;exposures_blending&quot;)
&amp;gt; setwd(&quot;exposures_blending&quot;)
&amp;gt; disentangle::AdminTemplate()
[1] TRUE
&amp;gt; dir.create(&quot;results&quot;)
&amp;gt; setwd(&quot;results&quot;)
&amp;gt; makeProject::makeProject(&quot;2015-10-23-preliminary-modelling&quot;)
Creating Directories ...
Creating Code Files ...
Complete ...
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This populates the folders as shown below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/worklog-ivan3.png&quot; alt=&quot;/images/worklog-ivan3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Conclusions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I feel pretty happy with this as a replication of Noble's method&lt;/li&gt;
&lt;li&gt;My colleagues can look at my work and see a high level log that links to the gory details of day to day life in the trenches&lt;/li&gt;
&lt;li&gt;The only downside I can see at the moment is that my colleagues on windows will see a text file that is pretty dense, and will not be as easy to navigate as a word document (or emacs org file)&lt;/li&gt;
&lt;li&gt;Perhaps notepad++ can be used instead.  On my windows machine I did a quick experiment with NPP and found that under the Language menu &gt; Define your language there is a method to define code folding with ** as the opening.  Just need to define a closing tag.  I experimented with '---' which might be good, but ultimately I don't think my colleagues are going to want to do this on their machines.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>2015-10-23 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/keeping-an-electronic-lab-notebook-for-computational-statistics-projects/</link>
			</item>
		
			<item>
				<title>Reproducible research pipelines improve description of method steps</title>
				<description>&lt;p&gt;Adequately documenting the methods and results of data analysis helps
safeguard against errors of execution and
interpretation. It is proposed that
reproducible research pipelines address the problem of
adequate documentation of data analysis.&lt;/p&gt;

&lt;p&gt;This is because they make it easy to
check the methods. Assumptions are easy to challenge and results
verified in new analyses. Reproducible research pipelines extend
traditional research.  They do this by encoding the steps in a
computer ‘scripting’ language and distributing the data and code with
publications.  Traditional research moves through the steps of
hypothesis and design, measured data, analytic data, computational
results (for figures, tables and numerical results), and reports (text
and formatted manuscript).&lt;/p&gt;

&lt;h2&gt;Fundamental components of a reproducible research pipeline&lt;/h2&gt;

&lt;p&gt;The basic components of a pipeline are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data Management Plan and Data Inventory&lt;/li&gt;
&lt;li&gt;Method steps&lt;/li&gt;
&lt;li&gt;Code&lt;/li&gt;
&lt;li&gt;Data storage&lt;/li&gt;
&lt;li&gt;Reports&lt;/li&gt;
&lt;li&gt;Distribution.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Method Steps&lt;/h3&gt;

&lt;p&gt;The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.&lt;/p&gt;

&lt;p&gt;A simple way to keep track of the steps, inputs and outputs is shown in the Table below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(stringr)
steps &amp;lt;- read.csv(textConnection('
CLUSTER ,  STEP  , INPUTS                   , OUTPUTS                   
A  ,  Step1      , &quot;Input 1, Input 2&quot;       , &quot;Output 1&quot;                 
A  ,  Step2      ,  Input 3                  , Output 2                   
B  ,  Step3      , &quot;Output 1, Output 2&quot;      , Output 3                  
'), stringsAsFactors = F, strip.white = T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;The steps and data listed in the Table above can be visualised.
To achieve this an R function was written as part of this PhD project and is distributed in my own R package available on Github &lt;a href=&quot;https://github.com/ivanhanigan/disentangle&quot;&gt;https://github.com/ivanhanigan/disentangle&lt;/a&gt;.
This is the &lt;code&gt;newnode&lt;/code&gt; function.  The function returns a string of text
written in the &lt;code&gt;dot&lt;/code&gt; language which can be rendered in R using the
&lt;code&gt;DiagrammeR&lt;/code&gt; package, or the standalone &lt;code&gt;graphviz&lt;/code&gt; package.   This creates the graph of this pipeline shown in Figure below.  Note that a new field was added for Descriptions as these are highly recommended.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(disentangle); library(stringr)
nodes &amp;lt;- newnode(indat = steps,   names_col = &quot;STEP&quot;, in_col = &quot;INPUTS&quot;,
  out_col = &quot;OUTPUTS&quot;, 
  nchar_to_snip = 40)
sink(&quot;fig-basic.dot&quot;);
cat(nodes);
sink()
# or DiagrammeR::grViz(nodes)
system(&quot;dot -Tpdf fig-basic.dot -o fig-basic.pdf&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/fig-basic.png&quot; alt=&quot;/images/fig-basic.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-10-18 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/reproducible-research-pipelines-improve-description-of-method-steps/</link>
			</item>
		
			<item>
				<title>How To Effectively Implement Electronic Lab Notebooks In Epidemiology</title>
				<description>&lt;ul&gt;
&lt;li&gt;It is often stated in the literature that an electronic lab notebook is a core component of reproducible research&lt;/li&gt;
&lt;li&gt;For example the following is from Buck, S. (2015). Solving reproducibility. Science, 348(6242), 1403–1403. &lt;a href=&quot;http://dx.doi.org/10.1126/science.aac8041&quot;&gt;http://dx.doi.org/10.1126/science.aac8041&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;one of the most effective ways to promote high-quality science 
is to create free open-source tools that give scientists
easier and cheaper ways to incorporate transparency
into their daily workflow: from open lab notebooks, to
software that tracks every version of a data set, to dynamic 
document generation.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;But I have struggled to operationalise the lab notebook in the epidemiology projects I work in&lt;/li&gt;
&lt;li&gt;Here are some notes based on my recent readings and attempts with a new team&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Modularised lab notebooks&lt;/h2&gt;

&lt;p&gt;There seem to be a small number of components to a lab notebook that can be defined as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data management plan&lt;/li&gt;
&lt;li&gt;Workplan&lt;/li&gt;
&lt;li&gt;Worklog&lt;/li&gt;
&lt;li&gt;Workflow&lt;/li&gt;
&lt;li&gt;Distribution&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;One thing I think is important is to have levels of organisation in a hierarchy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Macro level: The 'Research Programme' level is about the entire breadth of the projects in the group.

&lt;ul&gt;
&lt;li&gt;Data Management Plan: including managing the computers and a Data Inventory&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Personal Workplan and Worklog&lt;/em&gt;: this is an overview of things I do, plan to do or learn along the way (this is for the high level things like planning professional development, or a holiday)&lt;/li&gt;
&lt;li&gt;This is operationalised in the blog you are reading right now.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Meso level: the 'Research Project' level is about a single study, or a small group of studies based around a core dataset or Concept

&lt;ul&gt;
&lt;li&gt;This is the level that you might write up a manuscript for a journal, or report to a client&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Project workplan&lt;/em&gt;: at this level there may be high level information about the study design, hypotheses, resources and admin for managing relationships with a variety of collaborators&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Worklog&lt;/em&gt;: WS Noble &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1000424&quot;&gt;http://dx.doi.org/10.1371/journal.pcbi.1000424&lt;/a&gt; recommends that this be the main lab notebook for the analysts&lt;/li&gt;
&lt;li&gt;He says 'This is a document that resides in the root of the results directory and that records your progress in detail. Entries in the notebook should be dated, and they should be relatively verbose, with links or embedded images or tables displaying the results of the experiments that you performed. In addition to de- scribing precisely what you did, the notebook should record your observations, conclusions, and ideas for future work'&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Micro level: the 'Experiment Results' level is about work you might do on a single day, or over a week

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Workflow&lt;/em&gt; scripts: At this level each 'experiment' is written up in chronological order, as entries to the Worklog at the meso level&lt;/li&gt;
&lt;li&gt;Noble recommends 'create either a README file, in which I store every command line that I used while performing the experi- ment, or a driver script (I usually call this runall) that carries out the entire experiment automatically'...&lt;/li&gt;
&lt;li&gt;and 'you should end up with a file that is parallel to the lab notebook entry. The lab notebook contains a prose description of the exper- iment, whereas the driver script contains all the gory details.'&lt;/li&gt;
&lt;li&gt;this is the level I usually think of managing the distribution side of things.  I will want to pack up the results and email to my collaborators, or decide on the one set of tables and figures to write into the manuscript for submission to a journal.  If this is accepted for publication, this is the one combined package of 'analytical data and code' that I would consider putting up online (to github) as supporting information for the paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>2015-10-17 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/</link>
			</item>
		
			<item>
				<title>GIS Issues when R is Used for Transforming Coordinate Systems</title>
				<description>&lt;ul&gt;
&lt;li&gt;I have been using RGDAL to transform and write out spatial data in GDA94&lt;/li&gt;
&lt;li&gt;It is a problem to know what I need to do to create the right prj file for ArcGIS to read without complaining&lt;/li&gt;
&lt;li&gt;I have an example of code below, that I used on a dataset I knew was in GDA94 when I read it in.  I want to do this mostly for when I have to convert from one to another, but I have done the manual hack a couple of times now and thought I better just check with an expert.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;infile &amp;lt;- &quot;ap_map&quot;
outfile &amp;lt;- gsub(&quot;map&quot;, &quot;map_GDA94.shp&quot;, infile)
outfile
setwd(indir)
shp &amp;lt;- readOGR(&quot;.&quot;, infile)
setwd(projdir)
plot(shp, add = T)
#str(shp)
shp@proj4string@projargs
#[1] &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;
# confirm this is GDA94
epsg &amp;lt;- make_EPSG()
str(epsg)
epsg[grep(4283, epsg$code),]
#     code    note                                                       prj4
# 212 4283 # GDA94   +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs
# Arcmap sees this as GRS 1980(IUGG, 1980) which I think is the same thing
# to be on the safe side I will force it to refer to GDA94
shp2 &amp;lt;- spTransform(shp, CRS(&quot;+init=epsg:4283&quot;))
shp2@proj4string@projargs
#now write
setwd(outdir)
dir()
writeOGR(shp2,  outfile, gsub(&quot;.shp&quot;, &quot;&quot;, outfile), driver = &quot;ESRI Shapefile&quot;)
setwd(projdir)
# Checking this shows it did not write the correct prj file, but I believe that this is because the GDA94 definition is not different to the WGS80 params.
# one other option is to manually replace that prj file with the correct text found at http://spatialreference.org/ref/epsg/4283/
# SO I did this to avoid any future confusions
# OLD GEOGCS[&quot;GRS 1980(IUGG, 1980)&quot;,DATUM[&quot;D_unknown&quot;,SPHEROID[&quot;GRS80&quot;,6378137,298.257222101]],PRIMEM[&quot;Greenwich&quot;,0],UNIT[&quot;Degree&quot;,0.017453292519943295]]
# NEW GEOGCS[&quot;GDA94&quot;,DATUM[&quot;D_GDA_1994&quot;,SPHEROID[&quot;GRS_1980&quot;,6378137,298.257222101]],PRIMEM[&quot;Greenwich&quot;,0],UNIT[&quot;Degree&quot;,0.017453292519943295]]
# checked with ArcMap and they align ok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;I asked on of my colleagues and here is his reply&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I usually use the sp and maptools library, rather than the readOGR and writeOGR functions.  Generally my workflow, as an example of projecting a GDA94 zone 55 file to WGS84, would be something like:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;shp &amp;lt;- readShapePoly(&quot;myfile.shp&quot;)
proj4string(shp) &amp;lt;- &quot;+proj=utm +zone=55 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;
shp2 &amp;lt;- spTransform(shp, CRS(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs &quot;))
writePolyShape(shp2,&quot;myfile_p.shp&quot;,proj4string=CRS(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs &quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I've never &lt;em&gt;noticed&lt;/em&gt; any problems doing this, the projection file is generated and I've never noticed any alignment problems in ArcMap.  It's true that I recall ArcMap sometimes displays an incorrect plaintext projection description, but things seem to be in the right place.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, I don't really ever rely on EPSG numbers in R - I just grab the proj4 strings and try to use them directly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO, see if PostGIS handles this OK&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I wonder if my FOSS GIS stack should still pass things through PostGIS as I suspect it does these things better.&lt;/li&gt;
&lt;li&gt;I might check the output if I run it through the DB use st_transform and then extract to shapefile.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<published>2015-10-16 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/gis-issues-when-r-used-transforming-coordinate-systems/</link>
			</item>
		
			<item>
				<title>A Set Of Guidelines For Exploratory Data Analysis And Cleaning</title>
				<description>&lt;p&gt;The New York Times ran a piece on August 17, 2014: “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights” [1].  The article bemoaned the need for too much of what data scientists call “data wrangling”, “data munging” and “data janitor work”.  In essence this means data quality control processes.
A key task of my role as data manager/analyst is to perform Exploratory Data Analysis (EDA) to review data deposited for consistency, quality and its compliance with standards to ensure that reusability of data published in the portal is maximised.   To do this, I use relevant data formatting standards (for example the International Standards from ISO), undertake thorough taxonomic reviews of each dataset and have well documented procedures for dealing with miscellaneous errors such as data inconsistencies, duplicate variable names and reformatting numeric or character strings. I sometimes make changes to the data and give no further thoughts to it, but at  other times I need to make recommendations to the data provider and ask for their decisions/approvals on what to change.&lt;/p&gt;

&lt;p&gt;I have put down this set of guidelines for my procedures to create standardised data structures, based on things that have been recommended in the literature [2-6] to make data as re-usable as possible. Here is a list of standard amendments undertaken during my EDA process:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;identify any out-of-range values (based on the specified units), or questionable data in general;&lt;/li&gt;
&lt;li&gt;rename all files and variables using lower_case_with_underscores naming convention;&lt;/li&gt;
&lt;li&gt;tabulate frequencies and variable distributions, note any outliers for review;&lt;/li&gt;
&lt;li&gt;identify any opportunities to make wide data longer, or many files that can be merged;&lt;/li&gt;
&lt;li&gt;If you have multiple linked tables, each table should include a column that allows those tables to be linked unambiguously (such as the site_ID variables); check that linking variables that link two or more data tables are identical in each table&lt;/li&gt;
&lt;li&gt;check that values in linked files marry up to values in other files (eg a site code in one file that is missing from the spatial data file);&lt;/li&gt;
&lt;li&gt;write as CSV with quote encapsulated strings (for archival purposes);&lt;/li&gt;
&lt;li&gt;code missing data as NA, or identify if these were actually censored;&lt;/li&gt;
&lt;li&gt;coerce dates to ISO 8601 to be in the the YYYY-MM-DD format, or MMM-YYYY;&lt;/li&gt;
&lt;li&gt;cast nominal variables that use integer codes as character;&lt;/li&gt;
&lt;li&gt;check that all value labels in enumerated lists are described (ie codes for “1” = “low”, “2” = “mid” and “3” = “high”);&lt;/li&gt;
&lt;li&gt;attempt to identify and split any combined variables (like season AND year like “winter-97” or species and comments ie “Banksia Dead”);&lt;/li&gt;
&lt;li&gt;review any species lists against current scientific name conventions, recommend any modifications;&lt;/li&gt;
&lt;li&gt;rename any non-conformant species lists (for instance including comments such as Alive/Dead) to “fauna_descriptor” or “flora_descriptor”;&lt;/li&gt;
&lt;li&gt;identify any characters in numeric or date variables and replace with NA, (add to a comments variable if possible);&lt;/li&gt;
&lt;li&gt;identify any values that Excel may try to convert to date type (for eg. site code “1-5” will appear as 5-Jan and should be rewritten as “site_1-5”);&lt;/li&gt;
&lt;li&gt;use a GIS to confirm spatial coordinates and add geographical coordinates in decimal degrees (GDA94) if only supplied in metres UTM or AMG (always request the datum and the zone);&lt;/li&gt;
&lt;li&gt;check what the coordinates refer to (e.g. approximate location of SW corner of 1ha plot).&lt;/li&gt;
&lt;li&gt;rename files to be consistent with all data in the LTERN Data Portal.  Our standardised names have been created using controlled vocabularies.  Packages and files are designated tracking numbers – this is denoted by a plot network code and a unique “Package” number ascribed to each data package.  Each data package contains one or more data table which is the smallest trackable unit (denoted by a unique “Table” number).&lt;/li&gt;
&lt;li&gt;while we prefer deposit of plain text CSV files, if we receive Excel spreadsheets we check for hidden rows or columns that might not be intended for publication (and  may have been deposited as an oversight).&lt;/li&gt;
&lt;li&gt;it is always best to open Excel workbooks and use the in-built export function to save as CSV files for further re-use.  While R packages and other tools exist to programmatically extract the data from Excel, few tools that interoperate with Excel actually get the all the bug/feature cases right.  It has been noted that
&quot;because working with data that has passed through Excel is hard to get right, data that has passed through Excel is often wrong.&quot; [7]&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;References:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Lohr, S. &lt;a href=&quot;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0&quot;&gt;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., &amp;amp; Supp, S. (2013). Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution, 6(2), 1–10. &lt;a href=&quot;http://dx.doi.org/10.4033/iee.2013.6b.6.f&quot;&gt;http://dx.doi.org/10.4033/iee.2013.6b.6.f&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wickham, H. (Under Review). Tidy data. Journal of Statistical Software, VV(Ii).&lt;/li&gt;
&lt;li&gt;Leek, J. 2014. &lt;a href=&quot;https://github.com/jtleek/datasharing&quot;&gt;https://github.com/jtleek/datasharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Borer, E., Seabloom, E., Jones, M., and Schildhauer, M. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America 90:205–214. &lt;a href=&quot;http://dx.doi.org/10.1890/0012-9623-90.2.205&quot;&gt;http://dx.doi.org/10.1890/0012-9623-90.2.205&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Campbell, J. L., Rustad, L. E., Porter, J. H., Taylor, J. R., Dereszynski, E. W., Shanley, J. B., Gries, C., Henshaw, D. L., Martin, M. E., Sheldon, W. M., and Boose, E. R. 2013. Quantity is Nothing
without Quality: Automated QA/QC for Streaming Environmental Sensor Data. BioScience, 63,
574-585. &lt;a href=&quot;http://dx.doi.org/10.1525/bio.2013.63.7.10&quot;&gt;http://dx.doi.org/10.1525/bio.2013.63.7.10&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mount, J. 2014. Excel spreadsheets are hard to get right. &lt;a href=&quot;http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/&quot;&gt;http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
				<published>2015-10-14 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/a-set-of-guidelines-for-exploratory-data-analysis-and-cleaning/</link>
			</item>
		
	</channel>
</rss>