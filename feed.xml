<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>disentangle</title>
		<description>Disentangle Things</description>
		<link>http://ivanhanigan.github.com</link>
		
			<item>
				<title>show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger</title>
				<description>&lt;ul&gt;
&lt;li&gt;This is a revision of my post &lt;a href=&quot;/2015/10/show-missingness-in-large-dataframes-v2&quot;&gt;/2015/10/show-missingness-in-large-dataframes-v2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This guy posted &lt;a href=&quot;http://www.njtierney.com/r/missing%20data/rbloggers/2015/12/01/ggplot-missing-data/&quot;&gt;http://www.njtierney.com/r/missing%20data/rbloggers/2015/12/01/ggplot-missing-data/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Let's try it out!&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
# depends
install.packages(&quot;gbm&quot;)
install_github(&quot;tierneyn/neato&quot;)
library(neato)
# small eg
locs=c(&quot;Australia&quot;,&quot;India&quot;,&quot;New Zealand&quot;,&quot;Sri Lanka&quot;,&quot;Uruguay&quot;,&quot;Somalia&quot;)
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable[atable == FALSE] &amp;lt;- NA
atable
png(&quot;ggplotmissing.png&quot;)
ggplot_missing(atable)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/ggplotmissing.png&quot; alt=&quot;/images/ggplotmissing.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The one I had problems with because too large is:&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;# Cool but what about a big one?
dat &amp;lt;- read.csv(&quot;~/path/to/file.csv&quot;)
str(dat)
png(&quot;ggplotmissing2.png&quot;, height=1800, width = 3000, res = 200)
ggplot_missing(dat)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/ggplotmissing2.png&quot; alt=&quot;/images/ggplotmissing2.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-12-02 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger/</link>
			</item>
		
			<item>
				<title>Notes from Dr Climate Re data reference syntax models for file organisation and naming</title>
				<description>&lt;ul&gt;
&lt;li&gt;This is an excellent explanation of the Australian Integrated Marine Observing System (IMOS) Data Reference Syntax by Damien Irving on the Dr Climate blog  &lt;a href=&quot;https://drclimate.wordpress.com/2015/09/04/managing-your-data/&quot;&gt;https://drclimate.wordpress.com/2015/09/04/managing-your-data/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Data Reference Syntax (DRS) – a convention for naming your files&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;&amp;lt;computer&amp;gt;/&amp;lt;project&amp;gt;/&amp;lt;organisation&amp;gt;/&amp;lt;collection&amp;gt;/&amp;lt;facility&amp;gt;/&amp;lt;data-type&amp;gt;/&amp;lt;site-code&amp;gt;/&amp;lt;year&amp;gt;/

The data type has a sub-DRS of its own, which tells us that the data
represents the 1-hourly average surface current for a single month
(October 2012), and that it is archived on a regularly spaced spatial
grid and has not been quality controlled.

Just in case the file gets separated from this informative directory
structure, much of the information is repeated in the file name
itself, along with some more detailed information about the start and
end time of the data, and the last time the file was modified:

&amp;lt;project&amp;gt;_&amp;lt;facility&amp;gt;_V_&amp;lt;time-start&amp;gt;_&amp;lt;site-code&amp;gt;_FV00_&amp;lt;data-type&amp;gt;_&amp;lt;time-end&amp;gt;_&amp;lt;modified&amp;gt;.nc.gz

In the first instance this level of detail seems like a bit of
overkill... 

Since the data are so well labelled,
locating all monthly timescale ACORN data from the Turquoise Coast and
Rottnest Shelf sites (which represents hundreds of files) would be as
simple as typing the following at the command line:

$ ls */ACORN/monthly_*/{TURQ,ROT}/*/*.nc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Damien's personalised DRS&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;It is worthwhile thinking through these ideas and incorporating them in ones data management system as early as possible&lt;/li&gt;
&lt;li&gt;Damien has also helpfully openly shared his own DRS at &lt;a href=&quot;https://github.com/DamienIrving/climate-analysis/blob/master/data_reference_syntax.md&quot;&gt;https://github.com/DamienIrving/climate-analysis/blob/master/data_reference_syntax.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Here is a summary of some key items I'm going to implement versions of for my own work&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basic data files&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;var&amp;gt;_&amp;lt;dataset&amp;gt;_&amp;lt;level&amp;gt;_&amp;lt;time&amp;gt;_&amp;lt;spatial&amp;gt;.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Sub-categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;time&amp;gt;&lt;/code&gt;: &lt;code&gt;&amp;lt;tstep&amp;gt;-&amp;lt;aggregation&amp;gt;-&amp;lt;season&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;spatial&amp;gt;&lt;/code&gt;: &lt;code&gt;&amp;lt;grid&amp;gt;-&amp;lt;region&amp;gt;-&amp;lt;bounds&amp;gt;-&amp;lt;np&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;tstep&amp;gt;&lt;/code&gt;: &lt;code&gt;daily&lt;/code&gt;, &lt;code&gt;monthly&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;aggregation&amp;gt;&lt;/code&gt;: &lt;code&gt;030day-runmean&lt;/code&gt;, &lt;code&gt;anom-wrt-1979-2011&lt;/code&gt;, &lt;code&gt;anom-wrt-all&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;season&amp;gt;&lt;/code&gt;: &lt;code&gt;JJA&lt;/code&gt;, &lt;code&gt;MJJASO&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;grid&amp;gt;&lt;/code&gt;: &lt;code&gt;native&lt;/code&gt; or something like &lt;code&gt;y181x360&lt;/code&gt;, which describes the number of latitude (181) and longitude (360) points (in this case it is a 1 by 1 degree horizontal grid).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;region&amp;gt;&lt;/code&gt;: Region names are defined in &lt;code&gt;netcdf_io.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;bounds&amp;gt;&lt;/code&gt;: e.g. &lt;code&gt;lon225E335E-lat10S10N&lt;/code&gt; or &lt;code&gt;mermax&lt;/code&gt;, &lt;code&gt;zonal-anom&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;np&amp;gt;&lt;/code&gt;: North pole location, e.g. &lt;code&gt;np20N260E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Examples include:&lt;br/&gt;
&lt;code&gt;psl_Merra_surface_daily_y181x360.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;More complex file names&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;inside&amp;gt;_&amp;lt;filters&amp;gt;_&amp;lt;prev-var&amp;gt;_&amp;lt;dataset&amp;gt;_&amp;lt;level&amp;gt;_&amp;lt;time&amp;gt;_&amp;lt;spatial&amp;gt;.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Sub-categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;inside&amp;gt;&lt;/code&gt;: The variable inside the file. e.g. &lt;code&gt;tas-composite&lt;/code&gt;, &lt;code&gt;datelist&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;filters&amp;gt;&lt;/code&gt;: e.g. &lt;code&gt;samgt90pct&lt;/code&gt; (&lt;code&gt;gt&lt;/code&gt; and &lt;code&gt;lt&lt;/code&gt; and used for greater and less than, &lt;code&gt;pct&lt;/code&gt; for percentile)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;prev-var&amp;gt;&lt;/code&gt;: if it's not obvious what variable &lt;code&gt;&amp;lt;inside&amp;gt;&lt;/code&gt; was created from, include the previous variable/s&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Examples:&lt;br/&gt;
&lt;code&gt;tas-composite_pwigt90pct_ERAInterim_500hPa_030day-runmean-anom-wrt-all_native-sh.png&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Principles of Tidy Data&lt;/h3&gt;

&lt;p&gt;In the words of Hadley Wickham the order that data should be
arranged in follows some generic principles:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;'A good ordering makes it easier to scan the raw values. One way of
organizing variables is by their role in the analysis: are values
fixed by the design of the data collection, or are they measured
during the course of the experiment? Fixed variables describe the
experimental design and are known in advance. Computer scientists
often call fixed variables dimensions, and statisticians usually
denote them with subscripts on random variables. Measured variables
are what we actually measure in the study. Fixed variables should come
first, followed by measured variables, each ordered so that related
variables are contiguous. Rows can then be ordered by the first
variable, breaking ties with the second and subsequent (fixed)
variables.'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;An exemplar&lt;/h3&gt;

&lt;p&gt;In my last project the protocol we developed (for an ecology and biodiversity database) had a naming convention which relied heavily on a sequence of information being used to order the names of folders, subfolders and files.  This is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The project name (and optional sub-project name)&lt;/li&gt;
&lt;li&gt;Data type (such as experimental unit, observational unit, and/or measurement methods)&lt;/li&gt;
&lt;li&gt;Geographic location (locality name, State, Country)&lt;/li&gt;
&lt;li&gt;Temporal frequency and coverage (such as annual or seasonal tranches).&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;The concepts of slow moving dimensions and fast moving variables&lt;/h3&gt;

&lt;p&gt;The concept of dimensions and variables can be useful here, and especially for deciding on filenames.  Dimensions are fixed or change slowly while variables change more quickly.  By 'change', this  means that there are more of them. For example the project name is 'fixed', that is it does not change across the files, but the sub-project name does change, just more slowly (say there may be 2-3 different sub-projects within a project). Then there may be a set of data types, and these 'change' more quickly than the sub-project name.  Then the geographic and temporal variables might change quickest of all.&lt;/p&gt;

&lt;p&gt;So a general rule for the order of things can be stated. The fixed and slowly changing variables should come first (those things that don't change, or don't change much),
followed by the more fluid variables (or things that change more across the project).
List elements can then be ordered so that the groups of things that are similar will always be contiguous, and vary sequentially within clusters.&lt;/p&gt;

&lt;p&gt;So the only thing I disagree with Damien about is his decision to put space after time:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;var&amp;gt;_&amp;lt;dataset&amp;gt;_&amp;lt;level&amp;gt;_&amp;lt;time&amp;gt;_&amp;lt;spatial&amp;gt;.nc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This is  because I think that the geography is more stable than the time period for a data collection, and as most of my studies look at changes of variables measured at a location over time I generally want to compare the same spot at multiple times.  There are pros and cons of each approach such as if the analyst wants to make maps of a variable measured at several locations at a single point in time then having the data arranged by time first and then location may make that job simpler.&lt;/p&gt;

&lt;p&gt;I also notice however that the IMOS syntax puts the site spatial location before the year.&lt;/p&gt;
</description>
				<published>2015-11-29 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/notes-from-dr-climate-re-data-reference-syntax-models-for-file-organisation-and-naming/</link>
			</item>
		
			<item>
				<title>Visualisation tools for communicating data management concepts</title>
				<description>&lt;h1&gt;Hyperlinked table of contents that looks like a filing system&lt;/h1&gt;

&lt;p&gt;This looks like it might be useful to display information about filing systems, with a clickable toc that looks like a filing system!&lt;/p&gt;

&lt;p&gt;Source:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://tex.stackexchange.com/a/36185&quot;&gt;http://tex.stackexchange.com/a/36185&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;object data=&quot;/images/tikz_hlink.pdf&quot; type=&quot;application/pdf&quot; width=&quot;600&quot; height=&quot;800&quot;&gt;
  &lt;p&gt;Alternative text - include a link &lt;a href=&quot;images/tikz_hlink.pdf&quot;&gt;to the PDF!&lt;/a&gt;&lt;/p&gt;
&lt;/object&gt;

</description>
				<published>2015-11-26 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/visualisation-tools-for-communicating-data-management-concepts/</link>
			</item>
		
			<item>
				<title>A picture of the newnode function for workflow visualisation in R</title>
				<description>&lt;h2&gt;Newnode: An R function for visualising workflows&lt;/h2&gt;

&lt;p&gt;The scientific workflow concept is essentially a pipeline.  The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.&lt;/p&gt;

&lt;p&gt;A simple way to keep track of the steps, inputs and outputs is shown in Table below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CLUSTER ,  STEP       , INPUTS                   , OUTPUTS                   
A  ,       Step1      , &quot;Input 1, Input 2&quot;       , Output 1                 
A  ,       Step2      ,  Input 3                 , Output 2                   
B  ,       Step3      , &quot;Output 1, Output 2&quot;     , Output 3                  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps and data listed in this way can be visualised.
To achieve this an R function was written as part of my PhD project and is distributed in the R package available on Github &lt;a href=&quot;https://github.com/ivanhanigan/disentangle&quot;&gt;https://github.com/ivanhanigan/disentangle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is the &lt;code&gt;newnode&lt;/code&gt; function.  The function returns a string of text
written in the &lt;code&gt;dot&lt;/code&gt; language which can be rendered in R using the
&lt;code&gt;DiagrammeR&lt;/code&gt; package, or the standalone &lt;code&gt;graphviz&lt;/code&gt; package.   This creates the graph of this pipeline shown in Figure below.  Note that a new field was added for Descriptions as these are highly recommended.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/steps_basic_rstudio-test.png&quot; alt=&quot;/images/steps_basic_rstudio-test.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-11-17 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/a-picture-of-the-newnode-function-for-workflow-visualisation-in-r/</link>
			</item>
		
			<item>
				<title>Putting bibtex key into my lit review database needs sanitized latex characters</title>
				<description>&lt;p&gt;As I compile my papers for the thesis I am keeping notes for my presentation to discuss the results and scope of each study.  The work on the 'evidence tables' I described in the last two posts has proved useful here.  I allowed my self a breif distraction to dig out some code I had developed for a database of case studies demonstrating eco-social tipping points from historical evidence, and show here how to include bibtex info.  The bibtex key (from Mendeley in my case) is added to the database, and then the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(xtable)
library(rpostgrestools) # my own work
if(!exists(&quot;ch&quot;)) ch &amp;lt;- connect2postgres2(&quot;data_inventory_hanigan_dev4&quot;)

dat &amp;lt;- dbGetQuery(ch,
&quot;SELECT id, dataset_id, bibtex_key, title,  key_results,background_to_study, 
       research_question, study_extent, outcomes, exposures,  covariates, method_protocol,
        general_comments
  FROM publication
  where key_results is not null and key_results != '';
&quot;)
names(dat) &amp;lt;- gsub(&quot;_&quot;, &quot; &quot;, names(dat))
tabcode &amp;lt;- xtable(dat[,1:8])
align(tabcode) &amp;lt;-  c( 'l', 'p{.7in}','p{.8in}','p{1.7in}', 'p{1.7in}', 'p{1.7in}','p{1.8in}','p{1.7in}', 'p{1.7in}')
print(tabcode,  include.rownames = F, table.placement = '!ht',
      floating.environment='sidewaystable',
      sanitize.text.function = function(x) x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Produces the below table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/bibtable.png&quot; alt=&quot;/images/bibtable.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-11-15 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/putting-bibtex-key-into-my-lit-review-database-needs-sanitized-latex-characters/</link>
			</item>
		
			<item>
				<title>Developing a lit review database</title>
				<description>&lt;p&gt;My work yesterday on implementing &lt;a href=&quot;/2015/11/judging-the-evidence-using-a-literature-review-database&quot;&gt;a lit review section of my data inventory database&lt;/a&gt;
went pretty well, and I tested this while collating information on the papers I compile into my thesis.&lt;/p&gt;

&lt;p&gt;However, as I worked through the information for each paper I realised that attaching this stuff at the EML/dataset level is not always going to work.
In particular I have projects in which several papers are part of the one dataset.  To deal with this I have invented a non-EML relationship module for publications.
So in my new schema the following is possible&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;eml/project/
           /dataset1/
                    /publication1
                    /publication2
           /dataset2/
                    /etc  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;In this scenario a single dataset (ie bushfire smoke, temperature and mortality) can be used to write one paper focused on smoke, controlling for temperature.  Another paper on heatwaves, controlling for smoke. Indeed we might use time-series Poisson models for one and case-crossover design for the other (this is similar to what I did with Johnston and Morgan).&lt;/p&gt;

&lt;p&gt;So the simple thing to do is input these 'evidence tables' fields at the publication level, rather than the dataset as I did yesterday.&lt;/p&gt;

&lt;p&gt;This also partially solves my problem about aggregating these non-EML tags inside the text of the abstract, and worrying about parsing that to extract the elements of information.&lt;/p&gt;

&lt;h2&gt;The fields&lt;/h2&gt;

&lt;!-- html table generated in R 3.2.2 by xtable 1.7-4 package --&gt;


&lt;!-- Sat Nov 14 10:12:49 2015 --&gt;


&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt; data_inventory_field &lt;/th&gt; &lt;th&gt; description &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Citation &lt;/td&gt; &lt;td&gt; At a minimum author-date-journal, perhaps DOI? &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Key results &lt;/td&gt; &lt;td&gt; Include both effect estimates and uncertainty &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Background to the study &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Research question &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Study extent &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Outcomes &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Exposures &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Covariates &lt;/td&gt; &lt;td&gt; Include covariates, effect modifiers, confounders and subgroups &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Method protocol &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; General Comments &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;


&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;An example&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/datinv_pub.png&quot; alt=&quot;/images/datinv_pub.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-11-14 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/developing-a-lit-review-database/</link>
			</item>
		
			<item>
				<title>Judging the evidence using a literature review database</title>
				<description>&lt;p&gt;I recently read through a lecture slide deck called 'Judging the Evidence' by  Adrian Sleigh for a course PUBH7001 Introduction to Epidemiology, April 30, 2001.&lt;/p&gt;

&lt;p&gt;It had a lot of great material in it but I especially liked the section 'CRITIQUE OF AN EPIDEMIOLOGIC STUDY' and slide 11 'Quantity of data, duplication' which says:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Set clear criteria for admission of studies to your ‘judgement of evidence’
Devise ways to tabulate the information
‘Evidence tables’ show key features of design 
  (source, sample and study pop, N)
  exposures-outcomes measured
  observation methods
  confounding
  key results
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I thought this was a great idea, to build a database for keeping 'evidence tables' for each study I read.&lt;/p&gt;

&lt;p&gt;I then read through all the slides.  There is a lot of great information here, but it was spread out across the narrative.  I realised I wanted to collate these into a 'evidence table'. I also compared this with my understanding of the Ecological Metadata Language (EML) schema and the 'ANU Data Analysis Plan Template' and have put together a bit of a 'cross-walk' that lets me combine all this info and create a evidence table (database).&lt;/p&gt;

&lt;p&gt;I have started to use the database I built which uses EML concepts heavily and I include some these other ideas into my free &lt;code&gt;data_inventory&lt;/code&gt; application for a web2py database &lt;a href=&quot;https://github.com/ivanhanigan/data_inventory&quot;&gt;https://github.com/ivanhanigan/data_inventory&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is a webform style data entry interface, and I think good for these 'evidence tables'.
In the first instance I piggy back a lot of the elements into single EML tags, especially the abstract.
This may make it hard to parse.  The simple solution is to try to keep each element on a seperate line of the absract.&lt;/p&gt;

&lt;h2&gt;The key info for an evidence table entry per study&lt;/h2&gt;

&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt; EML &lt;/th&gt; &lt;th&gt; ANU &lt;/th&gt; &lt;th&gt; Adrian_Sleigh &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/title &lt;/td&gt; &lt;td&gt;  Study name &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/creator &lt;/td&gt; &lt;td&gt;  Person conducting analysis &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; project/personnel/[data_owner or orginator] &lt;/td&gt; &lt;td&gt;  Chief investigator &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/abstract &lt;/td&gt; &lt;td&gt;  Background to the study &lt;/td&gt; &lt;td&gt;  Purpose of Study &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;   Study research question  &lt;/td&gt; &lt;td&gt;  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;   Specific hypothesis under study &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;  outcomes of interest/ Exposure variables /Covariates &lt;/td&gt; &lt;td&gt;  exposures-outcomes measured &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;          &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;td&gt;  key results &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/studyextent &lt;/td&gt; &lt;td&gt;  Study population &lt;/td&gt; &lt;td&gt;  Study Setting &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;                     &lt;/td&gt; &lt;td&gt;                   &lt;/td&gt; &lt;td&gt;  source / sample and study pop &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/temporalcoverage &lt;/td&gt; &lt;td&gt;  Duration of study &lt;/td&gt; &lt;td&gt;   &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/methods_protocol &lt;/td&gt; &lt;td&gt;  Study Type &lt;/td&gt; &lt;td&gt;  Type of study &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/sampling_desc &lt;/td&gt; &lt;td&gt;    &lt;/td&gt; &lt;td&gt;  Subject Selection &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/methods_steps &lt;/td&gt; &lt;td&gt;  analytical strategy &lt;/td&gt; &lt;td&gt;  Statistical procedures &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt;                       &lt;/td&gt; &lt;td&gt;  exposures/ potential confounders or effect modifiers &lt;/td&gt; &lt;td&gt;  Confounding &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; entity/numberOfRecords &lt;/td&gt; &lt;td&gt;  Number study subjects &lt;/td&gt; &lt;td&gt;  N &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; dataset/distribution_methods &lt;/td&gt; &lt;td&gt;  dissemination strategy &lt;/td&gt; &lt;td&gt;    &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;




&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Here is a screen shot of my data inventory data entry form&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/datinv_entry.png&quot; alt=&quot;/images/datinv_entry.png&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-11-13 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/judging-the-evidence-using-a-literature-review-database/</link>
			</item>
		
			<item>
				<title>Adopting a bullet point style</title>
				<description>&lt;p&gt;With respect to bullet points:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;there are a range of styles accepted&lt;/li&gt;
&lt;li&gt;you just have to be consistent&lt;/li&gt;
&lt;li&gt;I have decided I like the bullet point style from
&lt;a href=&quot;http://www.monash.edu/about/editorialstyle/editing/punctuation&quot;&gt;http://www.monash.edu/about/editorialstyle/editing/punctuation&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;End the introductory phrase preceding a list of bullet points in a
colon. If the individual bullet points are sentence fragments, don't
use a full stop, comma or semi-colon. Leave it bare until the last
bullet point, and then use a full stop. Don't use capitals. Use full
stops if each bullet point is a complete sentence.
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-11-12 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/11/adopting-a-bullet-point-style/</link>
			</item>
		
			<item>
				<title>Decisions to make when modelling interactions</title>
				<description>&lt;p&gt;This post focuses on decision making during statistical
modelling.  The case of investigating effect modification is used as
an example.  The analyst has several options available to them when
constructing the model parametrisation for adjustment to explain
modification effectively.  Unlike confounding (in which the criterion
of substantial magnitude change of the effect estimate when
controlling for a third variable can be easily assessed), models
including effect modification can be hard to interpret.  Taking
account of effect modification becomes increasingly important when
modelling complex interactions.&lt;/p&gt;

&lt;p&gt;If an effect moderator is present then the relationship between
exposure and response varies between different levels of the
moderator. An example is provided by a paper we published, in which the
relationship between proximity to wetlands and Ross River virus
infection was found to be different for towns and rural areas, where the
'urban' variable is the 'effect moderator'.&lt;/p&gt;

&lt;p&gt;There are three common ways for data analysts to address this question
in statistical models:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;multiple models for each group,&lt;/li&gt;
&lt;li&gt;interaction terms or&lt;/li&gt;
&lt;li&gt;re-parametrisation to explicitly depict interactions.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The first way is
to split apart the dataset and conduct separate analyses of multiple
groups. For example, one could run the regression in the urban zone
and the rural zone seperately and see if there were different exposure
response functions in the two models.  This was the approach of our paper.
This approach has the strength that it is simple to do and yeilds
results that are easy to interpret.  A limitation of this method is
that by splitting the dataset one loses degrees of freedom and
therefore statistical power.&lt;/p&gt;

&lt;p&gt;The second approach (which removes that limitation of the first option) is to
fit interaction terms.  The example shown in Figure 1 is a multiplicative interaction model (Brambor 2006).
This  approach was not taken in the models reported in our paper, but can easily be implemented and shows that the function of distance was
estimated to have different 'slopes' in each of the dichotomous urban
groups.&lt;/p&gt;

&lt;p&gt;The statistical method can be easily implemented in
software by including a multiplicative term between two variables,
however in practice the resulting post estimation regression outputs
can be difficult to interpret.  For example, say one wants to calculate
the effect and standard error for exposure X on health outcome Y with
the interaction of the effect modifier Z.  The form of this model can
be written as:&lt;/p&gt;

&lt;p&gt;$$ Y ~ \beta&lt;em&gt;{1}X + \beta&lt;/em&gt;{2}Z + \beta_{3}XZ $$&lt;/p&gt;

&lt;p&gt;where B1, B2 and B3 are the regression coefficients estimated and  the term XZ is the interaction between exposure and effect modifier.&lt;/p&gt;

&lt;p&gt;The difficulty for interpretation comes when using this method for calculating the marginal effect of X on Y and the conditional standard error.  The specific method described in Brambor et al. is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calculate the coefficients and the variance-covariance matrix from the regression model&lt;/li&gt;
&lt;li&gt;The marginal effect is $\beta&lt;em&gt;{1} + \beta&lt;/em&gt;{3}XZ$ where Z is the level of the
modifying factor (0 or 1 in the dichotomous effect modifier case)&lt;/li&gt;
&lt;li&gt;The conditional standard error is:&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;$$\sqrt{var(\beta&lt;em&gt;{1}) + Z&lt;sup&gt;2&lt;/sup&gt; var(\beta&lt;/em&gt;{3}) + 2Zcov(\beta&lt;em&gt;{1}\beta&lt;/em&gt;{3})}$$&lt;/p&gt;

&lt;p&gt;Therefore the strengths of this approach is that it does not reduce
degrees of freedom and is straightforward to specify the model in
standard statistical software packages. The limitations are related to
interpretation of the resulting coefficients for both the main effects and the marginal effects, and standard errors for these.&lt;/p&gt;

&lt;p&gt;The third approach available to analysts makes it easier to access the
resulting regression output.  This method was employed in Paper 1 in
the final modelling phase in which estimates were calculated for the
different drought exposure-response funcitons in each of the
sub-groups. In the terms of Figure \ref{fig:effectmod.png} it is simple to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calculate X1 = X * Z (i.e = exposure for condition is met, zero otherwise)&lt;/li&gt;
&lt;li&gt;Calculate X0 = X * (1-Z) (i.e. = exposure for NON-condition, zero for condition is TRUE)&lt;/li&gt;
&lt;li&gt;Instead of X, Z and XZ, fit X1, X0 and Z.  This model also contains three parameters and captures the same interactions as it is the same model with a different parametrisation.  The standard errors for the X1 and X0 are calculated directly from the regression.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This method is much easier to implement and interpret.  This is also
considerably more flexible than the other two approaches.  A
limitation remains for this method in that the pre-processing steps
required are more complicated, and there are inherently more
possibilities for the data analyst to make errors in writing their
code as they make these changes to the analytical data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Demonstrating this with the data from the paper (available in table 2)
## the following code shows the different parametrisations for the effect modification by urban
## we show that the coeffs and se are equivalent but that the psuedo-R
## squared will be better when including all our data in stratified analysis

# model 0 effect in eastern
d_eastern
fit &amp;lt;- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_eastern )
summa &amp;lt;- summary(fit)
summa
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept) -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer      -0.24702    0.07921  -3.119  0.00182 **

# model 1 effect in urban
fit1 &amp;lt;- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_urban )
summa &amp;lt;- summary(fit1)
summa
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept) -5.52363    0.33354 -16.561   &amp;lt;2e-16 ***
## buffer       0.03853    0.06352   0.607    0.544

# step 1, combine the urban and rural data
d_eastern$urban &amp;lt;- 0
d_urban$urban &amp;lt;- 1
dat2 &amp;lt;- rbind(d_eastern, d_urban)
str(dat2)
dat2

# model 2 is a multiplicative term
fit2 &amp;lt;- glm(cases ~ buffer * urban + offset(log(1+pops)), family = 'poisson', data = dat2)
summa &amp;lt;- summary(fit2)
summa
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer       -0.24702    0.07921  -3.119  0.00182 **
## urban        -0.69938    0.36350  -1.924  0.05435 .
## buffer:urban  0.28555    0.10153   2.812  0.00492 **

# the coeff on buffer is for urban = 0 is main effect
# the coeff on buffer:urban is for urban = 1 is the marginal effect
b1 &amp;lt;- summa$coeff[2,1]
b3 &amp;lt;- summa$coeff[4,1]
b1 + b3
# 0.0385268
# but what about that p-value?  and the se?
str(fit2)
fit2_vcov &amp;lt;- vcov(fit2)
fit2_vcov
# now calculate the conditional standard error for the marginal effect of buffer for the value of the modifying variable (Z, urban =1)
varb1&amp;lt;-fit2_vcov[2,2]
varb3&amp;lt;-fit2_vcov[4,4]
covarb1b3&amp;lt;-fit2_vcov[2,4]
Z&amp;lt;-1
conditional_se &amp;lt;- sqrt(varb1+varb3*(Z^2)+2*Z*covarb1b3)
conditional_se



# model 3 is the re-parametrisation
dat2$buffer_urban &amp;lt;- dat2$buffer * dat2$urban
dat2$buffer_eastern &amp;lt;- dat2$buffer * (1-dat2$urban)

fit3 &amp;lt;- glm(cases ~ buffer_urban + buffer_eastern + urban + offset(log(1+pops)), family = 'poisson', data = dat2)
summa &amp;lt;- summary(fit3)
summa

## Coefficients:
##                Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)    -4.82425    0.14451 -33.382  &amp;lt; 2e-16 ***
## buffer_urban    0.03853    0.06352   0.607  0.54416
## buffer_eastern -0.24702    0.07921  -3.119  0.00182 **
## urban          -0.69938    0.36350  -1.924  0.05435 .
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-10-31 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/decisions-to-make-when-modelling-interactions/</link>
			</item>
		
			<item>
				<title>Show missingness in large dataframes, version 2</title>
				<description>&lt;ul&gt;
&lt;li&gt;UPDATE: the other day I blogged this but I needed to tweak things, so this is a re-post with extra&lt;/li&gt;
&lt;li&gt;UPDATE 2: Today an R blogger has posted a new solution &lt;a href=&quot;/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger&quot;&gt;/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;The old post&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Sometime ago I saw this example of a method for assessing missing data in a large data frame &lt;a href=&quot;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&quot;&gt;http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I asked my colleague Grant about doing this in R and he whipped up the following code to generate such an image:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/bankstown_traffic_counts_full_listing_june_2014.svg&quot; alt=&quot;/images/bankstown_traffic_counts_full_listing_june_2014.svg&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;misstable &amp;lt;- function(atable){
 op &amp;lt;- par(bg = &quot;white&quot;)
 plot(c(0, 400), c(0, 1000), type = &quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;,
     main = &quot;Missing Data Table&quot;)


 pmin=000
 pmax=400
 stre=pmax-pmin
 lnames=length(atable)
 cstep = (stre/lnames)
 for(titles in 1:lnames){
 text((titles-1) * cstep+pmin+cstep/2,1000,colnames(atable)[titles])
 }

 gmax=900
 gmin=0
 gstre=gmax-gmin
 rvec = as.vector(atable[ [ 1 ] ])
 dnames=length(rvec)
 step = gstre / dnames
 for(rows in 1:dnames){
 text(30,gmax - (rows-1)*step-step/2,rvec[rows])
 ymax=gmax - (rows-1)*step
 ymin=gmax - (rows)*step
 for(col in 2:lnames-1){
 if(atable[rows,col+1] == F){
 tcolor = &quot;red&quot;
 }
 if(atable[rows,col+1] == T){
 tcolor = &quot;white&quot;
 }
 rect((col) * (stre/lnames)+pmin, ymin, (col+1) * (stre/lnames)+pmin,
 ymax,col=tcolor,lty=&quot;blank&quot;)
 }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Now things to note are that the function expects the data to be TRUE if Not NA and  FALSE if is NA&lt;/li&gt;
&lt;li&gt;so might need to massage things a bit first&lt;/li&gt;
&lt;li&gt;here is the small test Grant supplied&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(grDevices)

# Make a quick dataframe with true/false representing data availability
locs=c(&quot;Australia&quot;,&quot;India&quot;,&quot;New Zealand&quot;,&quot;Sri Lanka&quot;,&quot;Uruguay&quot;,&quot;Somalia&quot;)
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable
#Draw the table.
misstable(atable)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;here is the one I worked on today&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# having defined the input dir and input file tried reading the excel sheet (without head 3 rows)
#dat &amp;lt;- readxl::read_excel(file.path(indir, infile), skip =3)
# got lots of warnings()
## 50: In read_xlsx_(path, sheet, col_names = col_names, col_types = col_types,  ... :
##   [1278, 4]: expecting date: got '[NULL]'
# I always worry about using excel connections so open in excel (in windows) 
# and save as to convert to CSV
dat &amp;lt;- read.csv(file.path(indir, gsub(&quot;.xlsx&quot;, &quot;.csv&quot;, infile)), skip =3, stringsAsFactor = F)
str(dat)
# 'data.frame':     1396 obs. of  167 variables:
# but most of the cols and a third of the rows are empty!
# check missings
dat2 &amp;lt;- data.frame(id = 1:nrow(dat), dat)
str(dat2)
# first if they are empty strings
dat2[dat2 == &quot;&quot;] &amp;lt;- NA
# now if NA
dat2[,2:ncol(dat2)] &amp;lt;- !is.na(dat2[,2:ncol(dat2)])

# Truncate the hundreds of empty cols
str(dat2[,1:18])
tail(dat2[,1:18])
svg(file.path(outdir, gsub(&quot;.csv&quot;, &quot;.svg&quot;, outfile))    )
misstable(dat2[,1:18])
dev.off()
browseURL(file.path(outdir, gsub(&quot;.csv&quot;, &quot;.svg&quot;, outfile))    )

# cool, that is an effective way to look at the data
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-10-28 00:00:00 +1100</published>
				<link>http://ivanhanigan.github.com/2015/10/show-missingness-in-large-dataframes-v2/</link>
			</item>
		
	</channel>
</rss>