<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>DisentangleThings</title>
 <link href="http://ivanhanigan.github.com/feed/" rel="self"/>
 <link href="http://ivanhanigan.github.com/"/>
 <updated>2014-01-26T07:59:36+11:00</updated>
 <id>http://ivanhanigan.github.com/</id>
 <author>
   <name>ivanhanigan</name>
   <email>mail@ivan.hanigan@gmail.com</email>
 </author>

 
 <entry>
   <title>studygroup-review-of-tree-based-models-for-testing-multiple-working-hypotheses</title>
   <link href="http://ivanhanigan.github.com/2014/01/studygroup-review-of-tree-based-models-for-testing-multiple-working-hypotheses/"/>
   <updated>2014-01-23T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2014/01/studygroup-review-of-tree-based-models-for-testing-multiple-working-hypotheses</id>
   <content type="html">&lt;p&gt;Yesterday I had the opportunity to review the Tree based modelling methods we are implementing in a study I'm working on at the moment.
I based the discussion on &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00330124.2012.724347&quot;&gt;a paper from 2012&lt;/a&gt;, along with &lt;a href=&quot;/pdfs/TreeModelNotes.pdf&quot;&gt;some notes I have made related to the use of these methods in our context&lt;/a&gt;.
I had an hour and a half with a couple of senior statisticians and a bunch of sociologists at the study group yesterday.&lt;/p&gt;

&lt;h4&gt;My main question for the group was:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;What do you think of the proposition that these models are suitable for &quot;studies that test hypotheses generated from multiple theories&quot;?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The statisticians where underwhelmed by the Cutts study, but thought the stats method was &quot;neat&quot; and one had not heard of tree models before (!).&lt;/p&gt;

&lt;h4&gt;Key outcomes of that session were:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First stage PCA&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Q: did they need to do the PCA first stage to either a) reduce the the large number of potentially collinear variables or b) to control for measurement error?&lt;/li&gt;
&lt;li&gt;A: No they didn't need the PCA.  The large number, collinearity and measurement efficacy are dealt with by the tree based methods (ie cross-validation on steroids, grouping primary and surrogate splits, etc).  Also the trees have an advantage with the large number of predictors in that they are non-parametric and able to automatically detect interactions whereas PCA is parametric and typically assumes linearity. (Note that I saw Steve afterward and he is still not convinced.  Maybe he will respond to this with some description of the reason why not?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rescaling the variables&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Q: Did they need to centre each variable on the grand sample mean so that the relative weight of each variable was even?&lt;/li&gt;
&lt;li&gt;A: No the tree models are not effected by this, and it reduced the interpret-ability of the decision tree graphic.  The PCA is effected, however, and maybe that is why they did it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multiple working hypotheses&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Q: What do you think of the Cutts et al proposition that these models are suitable for &quot;studies that test hypotheses generated from multiple theories&quot;?&lt;/li&gt;
&lt;li&gt;A: It is a promising approach.  Not clear from this study if it is successful but the proposition is plausible.  It has more of a chance than the General Linear Model approach which cannot (this point was made categorically by one of the statisticians).  It will be important to be very clear about what exactly each of the &quot;Theories&quot; are predicting, so that the explanatory power attributable to those variables from one &quot;theory&quot; can be compared with that of the other theories.  The ability to uncover complex interactions is very attractive (extrovert/introvert personality type interacting with group demographics etc) but it also increases the potential for spurious results (and 'chasing the noise') ergo the need to base inferences on what the theories predict, not just on what the data reveal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In general we thought that authors had overdone the stats.  It showed what 'could' be done with trees and forests, but possibly not what 'should' be done.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the A3 variable really missing from ctree output but dominant in the randomForest in Figure3?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;when another group I'm in reviewed it last year it was noted as odd that fig3 has A3 most important in randomforest but does not appear in ctree. At the time I thought this might be due to the cross validation like in the rpart or tree package, but it turns out that ctree does not do that....&lt;/li&gt;
&lt;li&gt;Mislabelling is a possibility as these graphics appear to be edited from the default produced by the 'party' package.&lt;/li&gt;
&lt;li&gt;However, it also reasonable that these results could be correct. ctree (and rpart, tree, etc.) use greedy algorithms, meaning that the best local split is used even if this is suboptimal globally.&lt;/li&gt;
&lt;li&gt;Basically there are algorithmic reasons that this might be a true difference between ctree and randomforest.  It might just have been the way the cookie crumbled for the ctree's best local split which may not have survived through the randomForest's  thousands of iterations.&lt;/li&gt;
&lt;li&gt;But on page 569 they do say &quot;total knowledge is lowest among those whod do not think that they are empowered and are unaware of information sources&quot; which implies to me that the top or left-most node might be mis-labelled I3 when it is actually A3.&lt;/li&gt;
&lt;li&gt;this would give the appropriate left most box plot (ie I3 &amp;lt;= -0.22 &amp;amp; A3 &amp;lt;= -0.64 gives lowest total knowledge??)&lt;/li&gt;
&lt;li&gt;at the group yesterday people also generally suspected mislabelling, given the dominance of A3 in randomforest.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Morpho And R-EML Use Case Marsupial mulgara Dasycercus cristicauda</title>
   <link href="http://ivanhanigan.github.com/2014/01/morpho-and-reml-use-case-marsupial-mulgara-dasycercus-cristicauda/"/>
   <updated>2014-01-21T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2014/01/morpho-and-reml-use-case-marsupial-mulgara-dasycercus-cristicauda</id>
   <content type="html">&lt;h1&gt;Aim&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&quot;https://github.com/ropensci/EML&quot;&gt;EML R package&lt;/a&gt; uses the Ecological Metadata Language (EML) approach that allows archiving of very heterogeneous data without having to standardize everything into a narrow and pre-defined syntax.&lt;/li&gt;
&lt;li&gt;XML files in specificed schemas involve strict criteria and are thus best generated by software.&lt;/li&gt;
&lt;li&gt;Morpho is an application that provides another GUI based method of generating EML but is a rather tedious tool for generating EML files. Unfortunately, without the ability to script inputs or automatically detect existing data structures, we are forced through the rather arduous process of adding all metadata annotation each time.&lt;/li&gt;
&lt;li&gt;The aim of this experiment is to use the EML package to create some advanced metadata quickly and then finish this off with Morpho, using &lt;a href=&quot;http://en.wikipedia.org/wiki/Boilerplate_code&quot;&gt;&quot;boilerplate code&quot;&lt;/a&gt; wherever possible.&lt;/li&gt;
&lt;li&gt;this builds on my previous post &lt;a href=&quot;http://ivanhanigan.github.io/2013/10/morpho-and-reml-streamline-the-process-of-metadata-entry&quot;&gt;http://ivanhanigan.github.io/2013/10/morpho-and-reml-streamline-the-process-of-metadata-entry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Background&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;I am basically very lazy when it comes to entering metadata and when I use the Morpho package for metadata data entry I get frustrated with having to step through ever SINGLE variable and use the drop down menus etc to describe them as essentially &quot;number&quot; or &quot;text&quot;&lt;/li&gt;
&lt;li&gt;A big reason I like Morpho is because Metacat is a great data portal and Kepler is a promising scientific workflow tool, and all three are produced by the same group so it would be great to get them working together...&lt;/li&gt;
&lt;li&gt;Morpho and Metacat are open source software designed to host all kinds of ecological data. More information about it can be found &lt;a href=&quot;http://knb.ecoinformatics.org/index.jsp&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More info about the Metacat Data Portal System is &lt;a href=&quot;https://knb.ecoinformatics.org/knb/docs/&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For technical reasons, I'm running an older version of the Morpho software because I'm working with an older version of the  Metacat portal software and so are also constrained to running the older Morpho version too (but will be upgrading soon).&lt;/li&gt;
&lt;li&gt;You might want to look at the background of the Ecological Metadata Language (EML) standard.  I like this page &lt;a href=&quot;http://carlboettiger.info/2013/06/23/notes-on-leveraging-the-ecological-markup-language.html&quot;&gt;http://carlboettiger.info/2013/06/23/notes-on-leveraging-the-ecological-markup-language.html&lt;/a&gt; along with the references he cites at the bottom.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Material&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To tie this experiment back to something that is actually useful for a scientist, I will use the field-based example data on the effects on mulgara of removing spinifex, from:&lt;/p&gt;

&lt;p&gt;  McCarthy, M. a., &amp;amp; Masters, P. (2005). Profiting from prior
  information in Bayesian analyses of ecological data. Journal of
  Applied Ecology, 42(6),
  1012–1019. doi:10.1111/j.1365-2664.2005.01101.x&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Brief description is:&lt;/p&gt;

&lt;p&gt;  an experimental manipulation of habitat was conducted by Masters,
  Dickman &amp;amp; Crowther (2003) in which vegetation cover of a site in
  arid inland Australia was reduced and the response of the mammal
  fauna monitored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;you can find the data in the download file from &lt;a href=&quot;http://www.nceas.ucsb.edu/~mccarthy/research.html&quot;&gt;the &quot;Code for analysing the mulgara experiment&quot; from here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;I first checked that these data aren;t already on &lt;a href=&quot;https://knb.ecoinformatics.org/m/&quot;&gt;the KNB repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I searched for Marsupial, Australia, Mulgara and etc, finding no hits.&lt;/li&gt;
&lt;li&gt;We will assume that these data are not already published there.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Methods&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Step one install the github version of EML and &lt;a href=&quot;http://ivanhanigan.github.io/2013/10/morpho-and-reml-streamline-the-process-of-metadata-entry/#sec-1-9&quot;&gt;a function I wrote for this a while back&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Note that I made a few changes since that post &lt;a href=&quot;https://github.com/ivanhanigan/disentangle/blob/master/R/reml_boilerplate.r&quot;&gt;https://github.com/ivanhanigan/disentangle/blob/master/R/reml_boilerplate.r&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
library(&quot;devtools&quot;)
install_github(&quot;EML&quot;, &quot;ropensci&quot;)       
library(&quot;EML&quot;)
install_github(&quot;disentangle&quot;, &quot;ivanhanigan&quot;)       
library(&quot;disentangle&quot;)

# load                                                                    
datatext &amp;lt;- 'Treat, Before, After1, After2
0,  2.833213344,    1.609437912,    2.48490665
0,  1.791759469,    2.197224577,    2.079441542
0,  3.044522438,    2.708050201,    3.135494216
0,  2.772588722,    1.791759469,    2.197224577
0,  1.098612289,    1.609437912,    2.63905733
1,  2.944438979,    0.693147181,    1.791759469
1,  2.564949357,    0.693147181,    1.791759469
1,  2.564949357,    1.609437912,    1.609437912
1,  0.693147181,    1.098612289,    1.098612289
1,  1.609437912,    0,      1.098612289'
analyte &amp;lt;- read.csv(textConnection(datatext))

# check
analyte

# do
## from a work dir with a subdir for data
write.csv(analyte, &quot;data/mulgara.csv&quot;, row.names = F)
reml_boilerplate(
  data_set = analyte
  ,
  created_by = &quot;Ivan Hanigan &amp;lt;ivanhanigan@gmail.com&amp;gt;&quot;
  ,
  data_dir = &quot;data&quot;
  ,
  titl = &quot;mulgara&quot;
  ,
  desc = &quot;Experimental data: effect of cover reduction on mulgara Dasycercus cristicauda&quot;
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Now open morpho and under file &gt; import browse to this  data directory and import.&lt;/li&gt;
&lt;li&gt;Got several warnings, about unable to display data, and an older version that could be updated&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Results&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Result does not display  in morpho&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/mulgara-morpho-import.png&quot; alt=&quot;mulgara-morpho-import.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Discussion&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;There is something different about the way the EML R package writes the data and what Morpho 1.8 is expecting.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Conclusions&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Further research is required.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Github, gh-pages and disqus comments</title>
   <link href="http://ivanhanigan.github.com/2014/01/github-gh-pages-and-disqus-commentsdatasharing/"/>
   <updated>2014-01-14T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2014/01/github-gh-pages-and-disqus-commentsdatasharing</id>
   <content type="html">&lt;p&gt;A while ago I posted about &lt;a href=&quot;http://ivanhanigan.github.io/2013/11/sharing-and-extending-research-protocols/&quot;&gt;sharing-and-extending-research-protocols&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I've started a new experiment for hosting a discussion around issues, suggesting new issues, agreeing on solutions, toward an agreement on methods that could become a protocol:
&lt;a href=&quot;http://ivanhanigan.github.com/datasharing&quot;&gt;http://ivanhanigan.github.com/datasharing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I forked the material from the original author Jeff Leek &lt;a href=&quot;https://github.com/jtleek/datasharing/network&quot;&gt;https://github.com/jtleek/datasharing/network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The goal of my experiment is something along the lines of the Prometheus Wiki &lt;a href=&quot;http://prometheuswiki.publish.csiro.au&quot;&gt;http://prometheuswiki.publish.csiro.au&lt;/a&gt;​ which is a site for sharing research protocols. That idea is to give people a place to post research protocols since everyone develops them and then mentions them in papers but they rarely make it online in a usable format.&lt;/p&gt;

&lt;p&gt;But I was talking with an user of that and he complained it lacked a kind of &quot;dynamic collaboration with a front-end markup system in place that was integrated with a good website-type backend&quot;.  This is what the github site might be able to do.&lt;/p&gt;

&lt;p&gt;I discussed with a colleague and he seemed to be receptive to experimenting with this, so long as it was not more cumbersome than:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;shooting off an email with a list of points or&lt;/li&gt;
&lt;li&gt;catching me in the tea room and saying &quot;by the way - missing values should never be -9999&quot;&lt;/li&gt;
&lt;li&gt;and then these being copied into a master document we all share.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The system I'm using in the proposed experiment uses the hi-tech tools gh-pages with disqus comments.  This let's:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;casual users chip in their two cents worth quickly via the  comments,&lt;/li&gt;
&lt;li&gt;users can vote up or vote down other peoples comments,&lt;/li&gt;
&lt;li&gt;track the discussion via their emails (if they choose that option),&lt;/li&gt;
&lt;li&gt;but those wanting  deeper involvement can fork and edit the pages and then submit pull  requests to the lead author.&lt;/li&gt;
&lt;li&gt;Github's wiki and issues tracking functionality also could be used for serious development.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>cwt-lter-data-submission-template-critique</title>
   <link href="http://ivanhanigan.github.com/2014/01/cwt-lter-data-submission-template-critique/"/>
   <updated>2014-01-07T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2014/01/cwt-lter-data-submission-template-critique</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;A colleague sent me the cwt_data_subm_template_2013.xls today&lt;/li&gt;
&lt;li&gt;LTER is The U.S. Long-Term Ecological Research (LTER) network&lt;/li&gt;
&lt;li&gt;I made the following notes, this is not intended to be a nasty critique&lt;/li&gt;
&lt;li&gt;The following is a few Frank and Fearless comments I'll be using to compare the pros and cons of a variety of data documentation approaches&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Critique&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;opened first on windows, saw comments on cells with instructions&lt;/li&gt;
&lt;li&gt;opened next on linux with libreOffice and comments are gone&lt;/li&gt;
&lt;li&gt;opened at the last tab (split in two for no reason?)&lt;/li&gt;
&lt;li&gt;noticed recommended name &quot;GCE site&quot; = Site, otherwise &quot;permanent plot&quot; =      Plot?&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nsmn1.uh.edu/steve/research/gce/gce.htm&quot;&gt;GCE = Georgia Coastal Ecosystems LTER program&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;flip to first tab, point 4 suggests there is some export functionality I cannot see (a VBA script?)&lt;/li&gt;
&lt;li&gt;cell 11 a    NOTE: When submitting updated metadata or re-using templates please highlight fields with modified contents in yellow&lt;/li&gt;
&lt;li&gt;and use glitter pen???&lt;/li&gt;
&lt;li&gt;personnell tab OK&lt;/li&gt;
&lt;li&gt;instrumentation, variable measured is free text. ok but for eg &quot;max temp&quot;, &quot;temperature maxima&quot;, &quot;maximum temperature (c)&quot; &quot;maximum temperature in 24 hours after 9am local time in degrees&quot; etc&lt;/li&gt;
&lt;li&gt;too wide, last column was off my wide screen! noticed wasted real estate in column A&lt;/li&gt;
&lt;li&gt;tabular data &quot;–  Paste or enter your data values into the 'Values' section (white cells), starting with the indicated cell&quot;&lt;/li&gt;
&lt;li&gt;this is an invitation for clerical error!
–  Fill in missing values in the table with NaN (not a number), including text fields, and do not skip columns&lt;/li&gt;
&lt;li&gt;but what about missing values imbued with other meanings (NA = not observed, censored etc)?&lt;/li&gt;
&lt;li&gt;ask users to format digit rounding in Excel?? oh no&lt;/li&gt;
&lt;li&gt;old excel users may still be restricted to 65,536 rows by 256 columns.&lt;/li&gt;
&lt;li&gt;non tabular sheet is ok&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>tumbarumba-supersite-dem</title>
   <link href="http://ivanhanigan.github.com/2014/01/tumbarumba-supersite-dem/"/>
   <updated>2014-01-03T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2014/01/tumbarumba-supersite-dem</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;The first dataset I downloaded from ASN for playing around with was the Tumba Lidar.&lt;/li&gt;
&lt;li&gt;I had thought it might be better to offer this as a OGC service rather than downloadable geotiff&lt;/li&gt;
&lt;li&gt;just in terms of the size firstly (104MB)&lt;/li&gt;
&lt;li&gt;but I also soon realised it would need some specialised tweaking which non-GIS users might struggle a bit and could avoid if the serverside data is set up by a GIS specialist (although can we assume only GIS specialists will download this kind of data)?&lt;/li&gt;
&lt;li&gt;kudos to &lt;a href=&quot;http://stackoverflow.com/questions/11966503/how-to-replace-nas-in-a-raster-object&quot;&gt;http://stackoverflow.com/questions/11966503/how-to-replace-nas-in-a-raster-object&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:tumbarumba-supersite-dem&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;setwd(&quot;~/data/supersite/tumba-lidar&quot;)
require(raster)
fname  &amp;lt;- dir(pattern = &quot;tif$&quot;)
fname
r  &amp;lt;- raster(fname)
str(r)
dfr &amp;lt;- as.data.frame(r)
summary(dfr)
# the -1 code looks like it might be missing?  They are around the edge.
rna &amp;lt;- reclassify(r, cbind(-1, 1197))
png(&quot;tumba-lidar.png&quot;)
plot(rna, col=terrain.colors(100), xlab = &quot;eastings (m)&quot;, ylab = &quot;northings (m)&quot;)
title(&quot;Tumbarumba Supersite Digital Elevation Model&quot;)
title(sub = &quot;packageId=lloyd.374.2&quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/tumba-lidar.png&quot; alt=&quot;tumba-lidar.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Alternately use a geoserver&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;If we use a Geoserver we could set it up so people can view this without downloading the data&lt;/li&gt;
&lt;li&gt;following the instructions &lt;a href=&quot;http://suite.opengeo.org/opengeo-docs/processing/contour/setup.html&quot;&gt;http://suite.opengeo.org/opengeo-docs/processing/contour/setup.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;using the ANU GIS library server&lt;/li&gt;
&lt;/ul&gt;


&lt;iframe style=&quot;border: none;&quot; height=&quot;400&quot; width=&quot;600&quot; src=&quot;http://brawn.anu.edu.au:8081/geoexplorer/viewer/#maps/1&quot;&gt;&lt;/iframe&gt;


&lt;p&gt;;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PS the map might take a minute to show up, not sure why, might ask the sysadmin to look at the server performance&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>extend-Rs-data-frame-class-with-metadata</title>
   <link href="http://ivanhanigan.github.com/2013/12/extend-Rs-data-frame-class-with-metadata/"/>
   <updated>2013-12-24T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/extend-Rs-data-frame-class-with-metadata</id>
   <content type="html">&lt;p&gt;&quot;reml now extends R's data.frame class by introducing the data.set class which includes additional metadata required by EML&quot;
&lt;a href=&quot;https://github.com/ropensci/reml&quot;&gt;https://github.com/ropensci/reml&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;and
&quot;I’d like to define a class that acts just like a data.frame, just like the data.table class does, but contains some additional metadata (e.g. the units associated with the columns) and has some additional methods associated with it (e.g. that might do something with those units) while also working with any function that simply knows how to handle data.frame objects.
How might this be done?&quot;
&lt;a href=&quot;http://carlboettiger.info/2013/09/11/extending-data-frame-class.html&quot;&gt;http://carlboettiger.info/2013/09/11/extending-data-frame-class.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also this guys attempt was interesting (I like TraMineR too!)
&lt;a href=&quot;http://ivanhanigan.github.io/2013/11/handling-survey-data-with-r/&quot;&gt;http://ivanhanigan.github.io/2013/11/handling-survey-data-with-r/&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>a-few-best-practices-for-statistical-programming</title>
   <link href="http://ivanhanigan.github.com/2013/12/a-few-best-practices-for-statistical-programming/"/>
   <updated>2013-12-24T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/a-few-best-practices-for-statistical-programming</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;John Myles White invented the ProjectTemplate R Package&lt;/li&gt;
&lt;li&gt;This is a great application that helps streamline the process of creating a data analysis project&lt;/li&gt;
&lt;li&gt;Recently John posted about some tips for &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2013/01/24/writing-better-statistical-programs-in-r/&quot;&gt;best practices for statistical programming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Best Practices for Statistical Programming&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Write Out a Directed Acyclic Graph (DAG)&lt;/li&gt;
&lt;li&gt;Vectorize Your Operations&lt;/li&gt;
&lt;li&gt;Profile your code and understand where it spends its time&lt;/li&gt;
&lt;li&gt;Generate Data and Fit Models&lt;/li&gt;
&lt;li&gt;Correctness: always ensure that code infers  parameters of models given simulated data with known parameters.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Additional suggestions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Unit Testing (use testthat)&lt;/li&gt;
&lt;li&gt;Create modular code with discrete chunks&lt;/li&gt;
&lt;li&gt;Write functions as much as possible, put these into a personal 'misc' package&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>add-2d-plots-of-trend-and-wiggle-to-catastrophic-regime-shifts-plot</title>
   <link href="http://ivanhanigan.github.com/2013/12/add-2d-plots-of-trend-and-wiggle-to-catastrophic-regime-shifts-plot/"/>
   <updated>2013-12-21T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/add-2d-plots-of-trend-and-wiggle-to-catastrophic-regime-shifts-plot</id>
   <content type="html">&lt;p&gt;&lt;head&gt;
&lt;title&gt;Catastrophic Regime Shifts Visualisation  &lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;Catastrophic Regime Shifts Visualisation  &quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-12-21T11:39+1100&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Catastrophic Regime Shifts Visualisation  &lt;/h1&gt;


&lt;hr/&gt;

&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Try adding 2D plot of the trend overtime and the variation within basins of attraction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 figure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; Try adding 2D plot of the trend overtime and the variation within basins of attraction&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;ul&gt;
&lt;li&gt;Following on from the previous work I now want to calculate the 2D paths.
&lt;/li&gt;
&lt;li&gt;This will then form the basis for a &quot;walk through&quot; animation 
&lt;/li&gt;
&lt;li&gt;Either with recorded narration or annotations that appear at the right time to describe each transition
&lt;/li&gt;
&lt;li&gt;This is most of what I want to include except I have not added the wiggly variations around the main trend line, that show the system varying within the basin of attraction
&lt;/li&gt;
&lt;li&gt;I got advice that Blender3d was the best way to finish this off.  Any other suggestions?
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.1&lt;/span&gt; figure&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-1&quot;&gt;

&lt;p&gt;&lt;img src=&quot;/images/TrendsAndTriggers-v2.1.gif&quot;  alt=&quot;/images/TrendsAndTriggers-v2.1.gif&quot; /&gt;
&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.2&lt;/span&gt; code&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-2&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;functions&lt;/span&gt;
x &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; seq(from=-2.5, to=2.5, by=0.1)

&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;load&lt;/span&gt;
data_out &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;TrendsAndTriggers-v2.csv&quot;&lt;/span&gt;)

&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;do&lt;/span&gt;
x2d  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; matrix(&lt;span style=&quot;color: #b58900;&quot;&gt;NA&lt;/span&gt;, ncol = 3, nrow = 0)
xindex  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(rep(-1.9, 5), 
             -1.7, -1.5 , -1.3, -1.1, 0, 1.1, 1.3, 1.5, 1.7
             , rep(2, 4))
j  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; 1
xind  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; xindex[j]
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(index &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; c(1:5,rep(5,8), 6:10)){
x2d &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; rbind(x2d, subset(data_out, y == index &amp;amp; x == xind))
j &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; j + 1
xind &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; xindex[j]
}
&lt;span style=&quot;color: #586e75;&quot;&gt;#  &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;x2d&lt;/span&gt;

&lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;png(&quot;/images/TrendsAndTriggers-v2.1.gif&quot;)&lt;/span&gt;
setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;images&quot;&lt;/span&gt;)
saveGIF(
{
ani.options(interval = 0.2)  
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(ith &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 100:140){
layout(matrix(c(1,2,1,3,1,4), 3, 2, byrow = &lt;span style=&quot;color: #b58900;&quot;&gt;TRUE&lt;/span&gt;), widths=c(2,1), heights=c(2,2,2))
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;layout.show(4)&lt;/span&gt;
res &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt;  persp(x, 1:10, matrix(data_out$z, ncol = 10, nrow = length(x)),
               ylab= &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;y&quot;&lt;/span&gt;,  xlab= &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x&quot;&lt;/span&gt;, zlab = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;z&quot;&lt;/span&gt;,  
               theta = ith, 
               phi = 42, ltheta = 120, shade = 0.75,
               expand = 0.5, col = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;lightgrey&quot;&lt;/span&gt;)
lines (trans3d(x2d$x, x2d$y, x2d$z, pmat = res), col = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;red&quot;&lt;/span&gt;, lwd = 4)
plot(x2d$x, x2d$y, type = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;l&quot;&lt;/span&gt;, xlab=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x&quot;&lt;/span&gt;, ylab=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;y&quot;&lt;/span&gt;)
plot(x2d$x, x2d$z, type = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;l&quot;&lt;/span&gt;, xlab=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x&quot;&lt;/span&gt;, ylab=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;z&quot;&lt;/span&gt;)
plot(x2d$y, x2d$z, type = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;l&quot;&lt;/span&gt;, xlab=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;y&quot;&lt;/span&gt;, ylab=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;z&quot;&lt;/span&gt;)
}
}

outdir = getwd(), movie.name = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;TrendsAndTriggers-v2.1.gif&quot;&lt;/span&gt;
)
setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;..&quot;&lt;/span&gt;)

&lt;span style=&quot;color: #586e75;&quot;&gt;#  &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;dev.off()&lt;/span&gt;
&lt;/pre&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Animation Illustrating Catastrophic Regime Shifts</title>
   <link href="http://ivanhanigan.github.com/2013/12/animation-illustrating-catastrophic-regime-shifts/"/>
   <updated>2013-12-20T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/animation-illustrating-catastrophic-regime-shifts</id>
   <content type="html">&lt;p&gt;&lt;head&gt;
&lt;title&gt;Trends and Triggers Figure &lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;Trends and Triggers Figure &quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-12-20T00:18+1100&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Trends and Triggers Figure &lt;/h1&gt;


&lt;hr/&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 The History&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3&quot;&gt;3 3D surface&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3-1&quot;&gt;3.1 figure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3-2&quot;&gt;3.2 code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-4&quot;&gt;4 Try an animation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-4-1&quot;&gt;4.1 figure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-4-2&quot;&gt;4.2 code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-5&quot;&gt;5 Next Steps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;This work toward a enhanced figure that might be used to tell a detailed story about the mixture of trend, triggers and wiggles.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;2&lt;/span&gt; The History&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-2&quot;&gt;

&lt;p&gt;The original image I base my imagination on is:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scheffer, M., &amp;amp; Carpenter, S. R. (2003). Catastrophic regime shifts in ecosystems: linking theory to observation [Review]. TRENDS in Ecology and Evolution, 18(12), 648–656. Retrieved from &lt;a href=&quot;http://eaton.math.rpi.edu/csums/papers/Ecostability/scheffercatastrophe.pdf&quot;&gt;http://eaton.math.rpi.edu/csums/papers/Ecostability/scheffercatastrophe.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
Which was based on 
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scheffer, M., Carpenter, S., Foley, J. a, Folke, C., &amp;amp; Walker, B. (2001). Catastrophic shifts in ecosystems. Nature, 413(6856), 591–6. &lt;i&gt;&amp;lt;doi:10.1038/35098000&amp;gt;&lt;/i&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
&lt;img src=&quot;/images/catastrophe.png&quot;  alt=&quot;/images/catastrophe.png&quot; /&gt;
&lt;/p&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;3&lt;/span&gt; 3D surface&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-3&quot;&gt;

&lt;p&gt;First I want to generate a 3d computer image to play with perspective and shading.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-3-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-3-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;3.1&lt;/span&gt; figure&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-3-1&quot;&gt;


&lt;p&gt;
&lt;img src=&quot;/images/TrendsAndTriggers-v2.png&quot;  alt=&quot;/images/TrendsAndTriggers-v2.png&quot; /&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-3-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;3.2&lt;/span&gt; code&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-3-2&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;#### &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:generate-surface ####&lt;/span&gt;
x &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; seq(from=-2.5, to=2.5, by=0.1)
zid &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; .1
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(y &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:10)
{    
  z &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; x^4 - zid * x^3 - 7 * x^2 + x + 6
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(y == 1)
  {
    data_out &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; cbind(x,y,z)  
  } &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;else&lt;/span&gt; {
    data_out &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; rbind(data_out, cbind(x,y,z))
  }
  zid &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; zid + 0.1
}

data_out &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; as.data.frame(data_out)
write.csv(data_out, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;TrendsAndTriggers-v2.csv&quot;&lt;/span&gt;, row.names = F)

png(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;/images/TrendsAndTriggers-v2.png&quot;&lt;/span&gt;)
persp(x, 1:10, matrix(data_out$z, ncol = 10, nrow = length(x)), ylab= &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;&quot;&lt;/span&gt;,  xlab= &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;&quot;&lt;/span&gt;, zlab = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;&quot;&lt;/span&gt;,  
      theta = 140, 
      phi = 42, 
      expand = 0.5, col = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;lightgrey&quot;&lt;/span&gt;)
dev.off()
&lt;/pre&gt;



&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-4&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-4&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;4&lt;/span&gt; Try an animation&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-4&quot;&gt;

&lt;p&gt;I'd like to move the ball through the surface.  First need to calculate the path.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-4-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;4.1&lt;/span&gt; figure&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-1&quot;&gt;

&lt;p&gt;&lt;img src=&quot;/images/animation.gif&quot;  alt=&quot;/images/animation.gif&quot; /&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-4-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;4.2&lt;/span&gt; code&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-2&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;functions&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(animation)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;animation&quot;&lt;/span&gt;);
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(animation)

&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;load&lt;/span&gt;
data_out &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;TrendsAndTriggers-v2.csv&quot;&lt;/span&gt;)

&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;do&lt;/span&gt;
setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;images&quot;&lt;/span&gt;)
saveGIF(
{
  ani.options(interval = 0.2)
  xindex  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(0, -1, rep(-1.9, 6), -1, 0, 1, rep(2, 6), 1, 0 , 0)
  j  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; 1
  xind  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; xindex[j]
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(index &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; c(1:10, 9:1)){
  with(subset(data_out, y == index),
       plot(x, z, type = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;l&quot;&lt;/span&gt;, ylim = c(-15,15))
       )
  with(subset(data_out, y == index &amp;amp; x == xind),
       points(x,z,pch=16,cex = 3)
       )
  j &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; j + 1
  xind &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; xindex[j]
  }
},
outdir = getwd()
)
setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;..&quot;&lt;/span&gt;)
&lt;/pre&gt;


&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-5&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-5&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;5&lt;/span&gt; Next Steps&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-5&quot;&gt;

&lt;ul&gt;
&lt;li&gt;the polynomials should move up and down to give the height of originals
&lt;/li&gt;
&lt;li&gt;the hump in the middle needs to change, so the ball flips more easily
&lt;/li&gt;
&lt;li&gt;I'd like the ball to wiggle, add a random walk 
&lt;/li&gt;
&lt;li&gt;the time series dimension needs to be shown
&lt;/li&gt;
&lt;li&gt;It'd be great to combine this with 2D line plots as well
&lt;/li&gt;
&lt;/ul&gt;





&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Marco Fahmi Farewell From ASN-LTERN Data Portal Team</title>
   <link href="http://ivanhanigan.github.com/2013/12/marco-fahmi-farewell-from-asn-ltern-data-portal-team/"/>
   <updated>2013-12-19T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/marco-fahmi-farewell-from-asn-ltern-data-portal-team</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;This is Marco Fahmi's final week with the ASN-LTERN Data Portal Team and I'd like to take a moment to reflect on the contributions he has made.&lt;/li&gt;
&lt;li&gt;In this age of distributed teams across the cloud and e-commuting, the old style office whip around and card to sign is not possible so this is my attempt a farewell card using e-collaboration techniques.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Who is Marco Fahmi?&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/marco.png&quot; alt=&quot;marco.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Marco is a great guy and a extremely good project manager.&lt;/li&gt;
&lt;li&gt;I got this picture of Marco from his Semaphore project team website &lt;a href=&quot;http://semaphoreblog.wordpress.com/team-bios/&quot;&gt;http://semaphoreblog.wordpress.com/team-bios/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It is a little out of date, he has less hair than that now&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Semaphore bio&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Marco sold his first piece of software in 1991; 
a Microsoft Excel macro that was to be used in a masters research project.

His promising career as a software developer came to an end a few months 
before the Dotcom crash when he decided to leave the dungeon and see the world.

Marco has been an academic Ronin since 2000. In his spare time, he plays Capoeira, 
writes a PhD dissertation, spends time with his family and 
contemplates the ideal work-life balance.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Also on twitter &lt;a href=&quot;https://twitter.com/fahmiger&quot;&gt;https://twitter.com/fahmiger&lt;/a&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;I do lots of things -- sometimes upside down.    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Marco's achievements with ASN-LTERN&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;A full list of Marco's achievements in relation to recent TERN and ANDS projects is formidable and I cannot do justice here.&lt;/li&gt;
&lt;li&gt;I particularly want to note that Marco was instrumental in setting up the first Data Portal and worked very hard getting the Metacat Service working for the Australian Supersite Network (ASN)&lt;/li&gt;
&lt;li&gt;Then Marco spread the joy of that to the Long Term Ecological Network (LTERN) project&lt;/li&gt;
&lt;li&gt;Now the two facilities enjoy a solid platform to build our data portals on.&lt;/li&gt;
&lt;li&gt;Marco was also very influential in me starting to use this blog as my Open Notebook and has offered sage advice&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;THANKS MARCO&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Next year Marco is going to move on to other adventures.&lt;/li&gt;
&lt;li&gt;On behalf of myself and the rest of the team I'd like to say a big THANKYOU!&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Quotable quotes by Marco&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Science Comes First... Well actually people come first, then Science.&lt;/li&gt;
&lt;li&gt;People Come First, then Science... Well actually money comes first, then People, then Science.&lt;/li&gt;
&lt;li&gt;Follow the lying person to the door of their house, if they continue lying follow them inside to see where they will stop (my paraphrasing, sorry if I muddled it up.)&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Please pop a note in the comments if you want to send Marco a message.&lt;/h4&gt;
</content>
 </entry>
 
 <entry>
   <title>animations-using-R</title>
   <link href="http://ivanhanigan.github.com/2013/12/animations-using-R/"/>
   <updated>2013-12-18T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/animations-using-R</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;following on from my previous posts about &lt;a href=&quot;http://ivanhanigan.github.io/2013/07/animated-maps/&quot;&gt;animated maps&lt;/a&gt;, &lt;a href=&quot;http://ivanhanigan.github.io/2013/06/spatio-temporal-animations/&quot;&gt;spatio-temporal animations&lt;/a&gt; and animations &lt;a href=&quot;http://ivanhanigan.github.io/button/index.html&quot;&gt;with buttons for go, stop and reverse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Here is a quick note about how to do a simple animation with R to create a movie file (GIF)&lt;/li&gt;
&lt;li&gt;To create this movie&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/animation/animation.gif&quot; alt=&quot;animation.gif&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code:animations-using-R&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;if(!require(animation)) install.packages(&quot;animation&quot;);
require(animation)
saveGIF(
{
ani.options(nmax = 100, interval = 0.5)
par(mar = c(3, 2.5, 0.5, 0.2), pch = 20, mgp = c(1.5, 0.5,0))
buffon.needle(mat = matrix(c(1, 2, 1, 3), 2))
},
outdir = getwd()
)
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>links-to-useful-data-munging-posts</title>
   <link href="http://ivanhanigan.github.com/2013/12/links-to-useful-data-munging-posts/"/>
   <updated>2013-12-16T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/links-to-useful-data-munging-posts</id>
   <content type="html">&lt;p&gt;Here are a few links to some recent data munging tips I picked up last week:&lt;/p&gt;

&lt;h4&gt;Database Relationships&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://pirategrunt.com/2013/12/13/24-days-of-r-day-13/&quot;&gt;This is a a very quick way to look at the relationships in a database&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;MS Access field (column) descriptions:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I'm looking for methods to access the metadata related to the columns.&lt;/li&gt;
&lt;li&gt;In general MS Access seems to hide these:&lt;/li&gt;
&lt;li&gt;http://blogannath.blogspot.com.au/2010/03/microsoft-access-tips-tricks-list-table.html&lt;/li&gt;
&lt;li&gt;&quot;Field descriptions can be entered by the user when creating the table in design view. It is a highly encouraged practice since the description can provided valuable documentation about the purpose of each field in a table. The inability to extract the field descriptions as part of the table documentation using Access's built-in documenter is therefore quite inconvenient.&quot;&lt;/li&gt;
&lt;li&gt;I can see there is &lt;a href=&quot;http://stackoverflow.com/questions/7041824/retrieve-msaccess-database-column-description&quot;&gt;a C# method, but I'd need visual studio or someone to compile this I suppose?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My mate Francis said: &quot;you'll have to use a script that uses the Microsoft OLE-DB, as in the stackoverflow answers. However, you don't need Visual Studio or C# to do this, just any language that can interface with Windows COM objects. Python can do this, so this might be your excuse to finally learn it. I imagine there might even by a R library out there somewhere, although it would probably be more convenient to go the python route here.&quot;&lt;/li&gt;
&lt;li&gt;To get started with COM and python, you &lt;a href=&quot;http://timgolden.me.uk/pywin32-docs/html/com/win32com/HTML/QuickStartClientCom.html&quot;&gt;could do worse than to start with&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Data manipulation&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://christophergandrud.blogspot.com.au/2013/12/three-quick-and-simple-data-cleaning.html&quot;&gt;This guy has created some custom functions that look helpful&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.revolutionanalytics.com/2013/12/tutorial-basic-data-processing-with-r.html&quot;&gt;Revolutions Blog links to several Data Wrangling resources&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;Code editor / IDE&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://www.r-bloggers.com/new-version-of-rstudio-v0-98/&quot;&gt;Updates to Rstudio server are always worth checking out&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>open-access-journal-templates-for-latex-with-emacs-orgmode</title>
   <link href="http://ivanhanigan.github.com/2013/12/open-access-journal-templates-for-latex-with-emacs-orgmode/"/>
   <updated>2013-12-13T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/open-access-journal-templates-for-latex-with-emacs-orgmode</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;I appreciate all the arguments for using OA journals&lt;/li&gt;
&lt;li&gt;Using a LaTeX template is also attractive&lt;/li&gt;
&lt;li&gt;This post is a record of some experiences with the BioMedCentral (BMC) template and the Public Library of Science (PLOS) templates&lt;/li&gt;
&lt;li&gt;implemented with my favourite editor for everything Emacs Orgmode.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;BMC seemed like a good option&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I started this journey 6 months ago with the BMC template&lt;/li&gt;
&lt;li&gt;because I pitch my work mostly at the health science community BMC seemed logical&lt;/li&gt;
&lt;li&gt;It turns out I downloaded their old LaTeX template and it SUCKED&lt;/li&gt;
&lt;li&gt;If you Download &lt;a href=&quot;http://www.biomedcentral.com/authors/tex&quot;&gt;the template and bibliography stuff&lt;/a&gt; now it looks better&lt;/li&gt;
&lt;li&gt;But I am suspicious now because of the last little while I've been struggling so I've got a skeptical approach&lt;/li&gt;
&lt;li&gt;First copy the stuff to a test dir&lt;/li&gt;
&lt;li&gt;then use TexWorks to test compiling it&lt;/li&gt;
&lt;li&gt;try deleting the provided BBL file, this reveales that the BIB file has to be compiled with bibtex to work&lt;/li&gt;
&lt;li&gt;orgmode produces BBL files when evaluated but I don't know if it'll be smooth with the template&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;PLOS seems like a smoother option&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Download from &lt;a href=&quot;http://www.plosone.org/static/latexGuidelines&quot;&gt;the LaTeX template site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;First thing I notice is there is no BIB file, Yay!&lt;/li&gt;
&lt;li&gt;just make sure  bibliographystyle and bibliography are set&lt;/li&gt;
&lt;li&gt;Looks like it isn't trying so hard&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>dual-code-repository-and-project-website</title>
   <link href="http://ivanhanigan.github.com/2013/12/dual-code-repository-and-project-website/"/>
   <updated>2013-12-09T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/dual-code-repository-and-project-website</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;I really like the idea of using github or bitbucket for a dual code repository and project website&lt;/li&gt;
&lt;li&gt;in Github I set up the master branch with the R package code&lt;/li&gt;
&lt;li&gt;and set up the website using the gh-pages branch&lt;/li&gt;
&lt;li&gt;it has to be called gh-pages, and add an index.html, it will appear at your-username.github.com/repo-name&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Github&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;The best bit about doing this on github is that the R package &quot;devtools&quot; can be used to install the package (so long as you can compile the package)&lt;/li&gt;
&lt;li&gt;on windows you may need to install Rtools and configure the path&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code: install_github&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(devtools)
install_github(&quot;repo-name&quot;, &quot;github-account-name&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Bitbucket&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I recently took these notes about setting up a website on Bitbucket&lt;/li&gt;
&lt;li&gt;I am not sure about how this would work for R packages (the devtools installer mentioned above is a great tool for developing)&lt;/li&gt;
&lt;li&gt;but with unlimited private repositories this seems like a good platform for my &quot;open notebook - selected content&quot; (I need to be selective about what I share and what I keep restricted access only)&lt;/li&gt;
&lt;li&gt;the page is available for &quot;your-account&quot;.bitbucket.org&lt;/li&gt;
&lt;li&gt;create a new repo on the web UI&lt;/li&gt;
&lt;li&gt;I kept the repo private, but the pages will be public,&lt;/li&gt;
&lt;li&gt;added issue tracking and Wiki&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;using git shell&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;mkdir ~/projects/your-account.bitbucket.org
cd ~/projects/your-account.bitbucket.org
git init
git remote add origin ssh://git@bitbucket.org/your-account/your-account.bitbucket.org.git
touch index.org
# use emacs to make changes and publish the html (C-c C-e h)

git commit -m &quot;First commit&quot;
git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;now it is online at http://your-account.bitbucket.org/&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;More info at:
&lt;a href=&quot;https://confluence.atlassian.com/display/BITBUCKET/Publishing+a+Website+on+Bitbucket&quot;&gt;https://confluence.atlassian.com/display/BITBUCKET/Publishing+a+Website+on+Bitbucket&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>bitbucket-has-unlimited-private-git-repositories-for-universities</title>
   <link href="http://ivanhanigan.github.com/2013/12/bitbucket-has-unlimited-private-git-repositories-for-universities/"/>
   <updated>2013-12-09T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/bitbucket-has-unlimited-private-git-repositories-for-universities</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;I just got introduced to bitbucket, an alternative to Github&lt;/li&gt;
&lt;li&gt;I've used Github for a couple of years and have been paying for extra private repositories&lt;/li&gt;
&lt;li&gt;I did not realise that bitbucket worked with git too (I thought it was just for Mercurial)&lt;/li&gt;
&lt;li&gt;it offers unlimited public and private repository and unlimited users (to collaborate) if you have the Academic plan.&lt;/li&gt;
&lt;li&gt;when you sign up with your academic email address, &lt;a href=&quot;http://blog.bitbucket.org/2011/04/01/free-unlimited-user-source-code-hosting-for-university-students/&quot;&gt;you automatically get an unlimited academic plan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>auto-download-bureau-meteorology-diurnal-data</title>
   <link href="http://ivanhanigan.github.com/2013/12/auto-download-bureau-meteorology-diurnal-data/"/>
   <updated>2013-12-06T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/auto-download-bureau-meteorology-diurnal-data</id>
   <content type="html">&lt;p&gt;&lt;head&gt;
&lt;title&gt;Excess Heat Indices &lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=iso-8859-1&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;Excess Heat Indices &quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-12-07T23:25+1100&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Excess Heat Indices &lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 auto-download-bureau-meteorology-diurnal-data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 First the FTP server URL structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 R Code: bom&lt;sub&gt;download&lt;/sub&gt;.r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4&quot;&gt;1.4 R Code: bom&lt;sub&gt;collation&lt;/sub&gt;.r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5&quot;&gt;1.5 BAT file (windoze)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6&quot;&gt;1.6 check the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-7&quot;&gt;1.7 Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; auto-download-bureau-meteorology-diurnal-data&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;


&lt;ul&gt;
&lt;li&gt;We;re looking at health impacts of high temperatures at work 
&lt;/li&gt;
&lt;li&gt;need to see the highest temperatures during the working hours
&lt;/li&gt;
&lt;li&gt;bom provides hourly data for download, but only 3 days at a time
&lt;/li&gt;
&lt;li&gt;we build a script and set it on a schedule to run every day, download the data and collate the results
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1&lt;/span&gt; First the FTP server URL structure&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1&quot;&gt;


&lt;ul&gt;
&lt;li&gt;The URLS are predictable, just need the station id, state and a code if metro or rural
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2&lt;/span&gt; table&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2&quot;&gt;

&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;left&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;Station&lt;sub&gt;ID&lt;/sub&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;State&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;City&lt;sub&gt;9&lt;/sub&gt;&lt;sub&gt;or&lt;/sub&gt;&lt;sub&gt;regional&lt;/sub&gt;&lt;sub&gt;8&lt;/sub&gt;_&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94774&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;95719&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94768&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94763&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94767&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94910&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94929&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;95896&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94693&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94691&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;N&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;95677&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;S&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94675&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;S&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94672&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;S&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94866&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;V&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;95867&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;V&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94868&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;V&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;94875&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;V&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;




&lt;ul&gt;
&lt;li&gt;now create a script called &quot;bom&lt;sub&gt;download&lt;/sub&gt;.r&quot;
&lt;/li&gt;
&lt;li&gt;it takes the station details and paste into the URLs
&lt;/li&gt;
&lt;li&gt;downloads the files
&lt;/li&gt;
&lt;li&gt;stores in a directory for each days downloads
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3&lt;/span&gt; R Code: bom&lt;sub&gt;download&lt;/sub&gt;.r&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;filename = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;~/data/ExcessHeatIndices/inst/doc/weather_stations.csv&quot;&lt;/span&gt;
output_directory = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;~/bom-downloads&quot;&lt;/span&gt;
setwd(output_directory)

urls &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(filename)
urls_list &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; paste(sep = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;http://www.bom.gov.au/fwo/ID&quot;&lt;/span&gt;,
                  urls$State,
                  &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;60&quot;&lt;/span&gt;, 
                  urls$City_9_or_regional_8_,
                  &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;01/ID&quot;&lt;/span&gt;,
                  urls$State,
                  &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;60&quot;&lt;/span&gt;,
                  urls$City_9_or_regional_8_,
                  &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;01.&quot;&lt;/span&gt;,
                  urls$Station_ID,
                  &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;.axf&quot;&lt;/span&gt;)

output_directory &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; file.path(output_directory,Sys.Date())
dir.create(output_directory)

&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(url &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; urls_list)
{
  output_file &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; file.path(output_directory,basename(url))
  download.file(url, output_file, mode = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;wb&quot;&lt;/span&gt;)

}
print(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;SUCCESS&quot;&lt;/span&gt;)

&lt;/pre&gt;




&lt;ul&gt;
&lt;li&gt;Now the data can be combined
&lt;/li&gt;
&lt;li&gt;clean up the header and extraneous extra line at the bottom
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4&lt;/span&gt; R Code: bom&lt;sub&gt;collation&lt;/sub&gt;.r&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4&quot;&gt;





&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;this takes data in directories from bom_download.r&lt;/span&gt;
 
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;first get list of directories&lt;/span&gt;
filelist &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; dir(pattern = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;axf&quot;&lt;/span&gt;, recursive = T)
filelist
 
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;next get directories for days we haven't done yet&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(file.exists(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;complete_dataset.csv&quot;&lt;/span&gt;))
{
complete_data &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;complete_dataset.csv&quot;&lt;/span&gt;, stringsAsFactors = F)
&lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;str(complete_data)&lt;/span&gt;
last_collated &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; max(as.Date(complete_data$date_downloaded))
&lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;max(complete_data$local_hrmin)&lt;/span&gt;
 
days_downloaded &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; dirname(filelist)
filelist &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; filelist[which(as.Date(days_downloaded) &amp;gt; as.Date(last_collated))]
}
 
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;for these collate them into the complete file&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(f &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; filelist)
{
  &lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;f &amp;lt;- filelist[2]&lt;/span&gt;
  print(f)
  fin &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(f, colClasses = c(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;local_date_time_full.80.&quot;&lt;/span&gt; = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;character&quot;&lt;/span&gt;), 
    stringsAsFactors = F, skip = 19)
  fin &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; fin[1:(nrow(fin) - 1),]
  fin$date_downloaded &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; dirname(f)
  fin$local_year &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(fin$local_date_time_full.80., 1, 4)
  fin$local_month &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(fin$local_date_time_full.80., 5, 6)
  fin$local_day &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(fin$local_date_time_full.80., 7, 8)
  fin$local_hrmin &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(fin$local_date_time_full.80., 9, 12)
  fin$local_date &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; paste(fin$local_year, fin$local_month, fin$local_day, sep = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;-&quot;&lt;/span&gt;)
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(file.exists(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;complete_dataset.csv&quot;&lt;/span&gt;))
  {
  write.table(fin, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;complete_dataset.csv&quot;&lt;/span&gt;, row.names = F, sep = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;,&quot;&lt;/span&gt;, append = T, col.names = F)
  } &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;else&lt;/span&gt; {
  write.table(fin, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;complete_dataset.csv&quot;&lt;/span&gt;, row.names = F, sep = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;,&quot;&lt;/span&gt;)
  }
}
&lt;/pre&gt;


&lt;ul&gt;
&lt;li&gt;so now let;s automate the process
&lt;/li&gt;
&lt;li&gt;make a BAT file
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5&lt;/span&gt; BAT file (windoze)&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5&quot;&gt;





&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;C:\Program Files\R\R-2.15.2\bin\Rscript.exe&quot;&lt;/span&gt; &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;~\bom-downloads\bom_download.r&quot;&lt;/span&gt;
&lt;/pre&gt;


&lt;ul&gt;
&lt;li&gt;add this  bat file to the scheduled tasks in your control panel
&lt;/li&gt;
&lt;li&gt;use chron for a linux version
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-6&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.6&lt;/span&gt; check the data&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-6&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;#### &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:check the data ####&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(plyr)

setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;~/bom-downloads&quot;&lt;/span&gt;)
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;source&lt;/span&gt;(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;bom_download.r&quot;&lt;/span&gt;)
dir()
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;source&lt;/span&gt;(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;bom_collation.r&quot;&lt;/span&gt;)

complete_data &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;complete_dataset.csv&quot;&lt;/span&gt;, stringsAsFactors = F)
str(complete_data)

&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;Quick and dirty de-duplication&lt;/span&gt;
table(complete_data$name.80.)
qc &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; subset(complete_data, name.80. == &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;Broken Hill Airport&quot;&lt;/span&gt;)
qc &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; ddply(qc, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;local_date_time_full.80.&quot;&lt;/span&gt;,
  summarise, apparent_temp = mean(apparent_t))

names(qc)
png(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;qc-diurnal-plot.png&quot;&lt;/span&gt;)
with(qc,
     plot(apparent_temp, type= &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;l&quot;&lt;/span&gt;)
     )
dev.off()
&lt;/pre&gt;


&lt;p&gt;
&lt;img src=&quot;/images/qc-diurnal-plot.png&quot;  alt=&quot;qc-diurnal-plot.png&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-7&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-7&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.7&lt;/span&gt; Conclusions&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-7&quot;&gt;


&lt;ul&gt;
&lt;li&gt;watch the data roll on in
&lt;/li&gt;
&lt;li&gt;each day there are about 3 days downloaded
&lt;/li&gt;
&lt;li&gt;meaning duplicates will be frequent, need to write a script to de-duplicate
&lt;/li&gt;
&lt;li&gt;cheers!
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>the-history-of-ons</title>
   <link href="http://ivanhanigan.github.com/2013/12/the-history-of-ons/"/>
   <updated>2013-12-04T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/the-history-of-ons</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;according to &lt;a href=&quot;http://en.wikipedia.org/wiki/Open_notebook_science&quot;&gt;http://en.wikipedia.org/wiki/Open_notebook_science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The term Open Notebook Science[7] was first used in a blog post by Jean-Claude Bradley,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This article &lt;a href=&quot;http://www.infotoday.com/it/sep10/Poynder.shtml&quot;&gt;http://www.infotoday.com/it/sep10/Poynder.shtml&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  says Jean-Claude Bradley an organic chemist at Drexel University in
  Philadelphia. As with most scientists, Bradley used to be very
  secretive. He kept his research under wraps until publication and
  frequently applied for patents on his work in nanotechnology and gene
  therapy.&lt;/p&gt;

&lt;p&gt;  However, he asked himself a difficult question 5 years ago: Was his
  research having the kind of impact he would like? He had to conclude
  that the answer was “no,” and this was partly a consequence of the
  culture of secrecy that permeates research today. So Bradley was
  determined to be more open. Since his collaborators were not of the
  same mind, he severed his ties with them and, in 2005, he launched a
  web-based initiative called UsefulChem. As the name implies, the aim
  of the initiative was also to work in the world of useful science and,
  today, Bradley makes new anti-malarial compounds.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other links&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://opensourcemalaria.org/&quot;&gt;http://opensourcemalaria.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://malaria.ourexperiment.org/&quot;&gt;http://malaria.ourexperiment.org/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mat Todd's group is at the School of Chemistry at The University of Sydney is practicing this with a (Schistosomiasis notebook and Malaria notebook)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://openwetware.org/wiki/Todd&quot;&gt;http://openwetware.org/wiki/Todd&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>non-linear-relationships-vs-non-linear-models-vis-a-vis-curvi-linear-terms</title>
   <link href="http://ivanhanigan.github.com/2013/12/non-linear-relationships-vs-non-linear-models-vis-a-vis-curvi-linear-terms/"/>
   <updated>2013-12-03T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/non-linear-relationships-vs-non-linear-models-vis-a-vis-curvi-linear-terms</id>
   <content type="html">&lt;p&gt;&lt;head&gt;
&lt;title&gt;non-linear-model-vs-non-linear-relationship &lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=iso-8859-1&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;non-linear-model-vs-non-linear-relationship &quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-12-03T17:29+1100&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;script type=&quot;text/javascript&quot; src=&quot;http://orgmode.org/mathjax/MathJax.js&quot;&gt;
/**
 *
 * @source: http://orgmode.org/mathjax/MathJax.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 * Copyright (C) 2012-2013  MathJax
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 */

/*
@licstart  The following is the entire license notice for the
JavaScript code below.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code below is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code below.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: [&quot;MMLorHTML.js&quot;], jax: [&quot;input/TeX&quot;],
            jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;],
        extensions: [&quot;tex2jax.js&quot;,&quot;TeX/AMSmath.js&quot;,&quot;TeX/AMSsymbols.js&quot;,
                     &quot;TeX/noUndefined.js&quot;],
        tex2jax: {
            inlineMath: [ [&quot;\\(&quot;,&quot;\\)&quot;] ],
            displayMath: [ ['$$','$$'], [&quot;\\[&quot;,&quot;\\]&quot;], [&quot;\\begin{displaymath}&quot;,&quot;\\end{displaymath}&quot;] ],
            skipTags: [&quot;script&quot;,&quot;noscript&quot;,&quot;style&quot;,&quot;textarea&quot;,&quot;pre&quot;,&quot;code&quot;],
            ignoreClass: &quot;tex2jax_ignore&quot;,
            processEscapes: false,
            processEnvironments: true,
            preview: &quot;TeX&quot;
        },
        showProcessingMessages: true,
        displayAlign: &quot;center&quot;,
        displayIndent: &quot;2em&quot;,

        &quot;HTML-CSS&quot;: {
             scale: 100,
             availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;],
             preferredFont: &quot;TeX&quot;,
             webFont: &quot;TeX&quot;,
             imageFont: &quot;TeX&quot;,
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    &quot;MML&quot;,
                 Firefox: &quot;MML&quot;,
                 Opera:   &quot;HTML&quot;,
                 other:   &quot;HTML&quot;
             }
        }
    });
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;non-linear-model-vs-non-linear-relationship &lt;/h1&gt;


&lt;hr/&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 non-linear-relationships-vs-non-linear-models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Nonlinear Regression vs. Linear Regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1-1&quot;&gt;1.1.1 Model 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1-2&quot;&gt;1.1.2 Model 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; non-linear-relationships-vs-non-linear-models&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;


&lt;ul&gt;
&lt;li&gt;I value precise language very highly
&lt;/li&gt;
&lt;li&gt;this is because in multi-disciplinary teams it is easy to talk using the same words and mean different things
&lt;/li&gt;
&lt;li&gt;in recent discussion about &lt;a href=&quot;http://cran.r-project.org/web/packages/dlnm/index.html&quot;&gt;Distributed Lag Non-linear Models&lt;/a&gt; I started to reflect on something that has bothered me for a While
&lt;/li&gt;
&lt;li&gt;back in 2005 my old mate Prof Keith Dear picked me up on using the term &quot;non-linear model&quot; incorrectly and explained the maths&amp;hellip;
&lt;/li&gt;
&lt;li&gt;I kind of understood but promptly forgot and found a lot of people use the term non-linear model a bit carelessly
&lt;/li&gt;
&lt;li&gt;Yesterday I was in a discussion about comparing non-linear relationships between different studies in a meta-analysis
&lt;/li&gt;
&lt;li&gt;I immediatly felt uncomfortable when we started to discuss these as &quot;non-linear models&quot;
&lt;/li&gt;
&lt;li&gt;so here is a quick bit of google fu (with a session at the coffee shop with Steve and Mishka) to remind me about the difference between 
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.1&lt;/span&gt; Nonlinear Regression vs. Linear Regression&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-1&quot;&gt;


&lt;ul&gt;
&lt;li&gt;the following comes from
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/sas/library/SASNLin_os.htm&quot;&gt;http://www.ats.ucla.edu/stat/sas/library/SASNLin_os.htm&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;verbatim except for my attempt at mathjax notation in latex
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
A regression model is called nonlinear, if the derivatives
of the model with respect to the model parameters depends on one or
more parameters. This definition is essential to distinguish nonlinear
from curvilinear regression. A regression model is not necessarily
nonlinear if the graphed regression trend is curved. A polynomial
model such as this:
&lt;/p&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1.1&lt;/span&gt; Model 1&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1-1&quot;&gt;




\(Y_{i} = \beta_{0} + \beta_{1} X_{i} + \beta_{2} X_{i}^2 + \epsilon_{i}\)

&lt;ul&gt;
&lt;li&gt;appears curved when y is plotted against x. It is, however, not a nonlinear model. To see this, take derivatives of y with respect to the parameters b0, b1
&lt;/li&gt;
&lt;li&gt;dy/db0 = 1 
&lt;/li&gt;
&lt;li&gt;dy/db1 = x 
&lt;/li&gt;
&lt;li&gt;dy/db2 = x&lt;sup&gt;2&lt;/sup&gt; 
&lt;/li&gt;
&lt;/ul&gt;



&lt;ul&gt;
&lt;li&gt;None of these derivatives depends on a model parameter, the model is linear. In contrast, consider the log-logistic model 
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1.2&lt;/span&gt; Model 2&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1-2&quot;&gt;




\(Y_{i} = d + (a - d)/(1 + e^{b \times log(x/g)}) + \epsilon\)

&lt;ul&gt;
&lt;li&gt;Take derivatives with respect to d, for example: 
&lt;/li&gt;
&lt;/ul&gt;




\(dy/dd = 1 - 1/(1 + e^{b \times log(x/g)})\)

&lt;ul&gt;
&lt;li&gt;The derivative involves other parameters, hence the model is nonlinear.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.2&lt;/span&gt; Conclusions&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-2&quot;&gt;


&lt;ul&gt;
&lt;li&gt;It is probably best to refer to the polynomial as a &quot;non-linear relationship&quot; in a linear model
&lt;/li&gt;
&lt;li&gt;reserving &quot;non-linear model&quot; for things like Model 2
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>research-protocol-we-used-for-our-bushfire-project</title>
   <link href="http://ivanhanigan.github.com/2013/12/research-protocol-we-used-for-our-bushfire-project/"/>
   <updated>2013-12-02T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/research-protocol-we-used-for-our-bushfire-project</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;For a three year project on Bushfire smoke and Health we used the following structure in a wiki&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Sections:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;A.Background        
B.Proposals         
C.Approvals         
D.Budget    
E.Datasets  
F.Analysis  
G.Literature        
H.Communication     
I.Correspondance    
J.Meetings  
K.Completion        
ContactDetails      
README
TODO        
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;it worked quite well in the first year.&lt;/li&gt;
&lt;li&gt;we didn't use it much after that.&lt;/li&gt;
&lt;li&gt;it is still on the ANU webserver.  I sometimes refer back to it now, a couple of years later.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>research-protocol-for-manitoba-centre-for-health-policy</title>
   <link href="http://ivanhanigan.github.com/2013/12/research-protocol-for-manitoba-centre-for-health-policy/"/>
   <updated>2013-12-02T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/research-protocol-for-manitoba-centre-for-health-policy</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;When we started planning for a three year project on bushfire smoke we did some planning&lt;/li&gt;
&lt;li&gt;We had a deep discussion about this research management protocol from &lt;a href=&quot;http://umanitoba.ca/faculties/medicine/units/community_health_sciences/departmental_units/mchp/protocol/media/manage_guidelines.pdf&quot;&gt;University Manitoba Centre for Health Policy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Data Management Plan Checklist&lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 U-Manitoba Centre for Health Policy Guidelines&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Confidentiality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Project Team Makeup&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-1&quot;&gt;1.2.1 Principal Investigator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-2&quot;&gt;1.2.2 Research Coordinator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-3&quot;&gt;1.2.3 The Programmer Coordinator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-4&quot;&gt;1.2.4 Programmer Analyst&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2-5&quot;&gt;1.2.5 Research Support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 Project Team considerations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-1&quot;&gt;1.3.1 Roles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-2&quot;&gt;1.3.2 Continuity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-3&quot;&gt;1.3.3 Access levels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-4&quot;&gt;1.3.4 Working group&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3-5&quot;&gt;1.3.5 Atmospherics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4&quot;&gt;1.4 File organization and Documentation Development.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-1&quot;&gt;1.4.1 Managing MCHP resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-2&quot;&gt;1.4.2 MCHP directory structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-3&quot;&gt;1.4.3 Managing project files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4-4&quot;&gt;1.4.4 Recommended Directories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5&quot;&gt;1.5 Communication&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5-1&quot;&gt;1.5.1 E-mail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5-2&quot;&gt;1.5.2 Meetings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6&quot;&gt;1.6 Administrative&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6-1&quot;&gt;1.6.1 Time entry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-7&quot;&gt;1.7 Report preparation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-7-1&quot;&gt;1.7.1 Reliability and Validity Checks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8&quot;&gt;1.8 Project Completion&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-1&quot;&gt;1.8.1 Final Project Meeting.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-2&quot;&gt;1.8.2 Final Documentation Review.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-3&quot;&gt;1.8.3 System Cleanup.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8-4&quot;&gt;1.8.4 Integration of new material to institution repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; U-Manitoba Centre for Health Policy Guidelines&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;These guidelines come from:
&lt;/p&gt;
&lt;p&gt;
\noindent &lt;a href=&quot;http://umanitoba.ca/faculties/medicine/units/mchp/protocol/media/manage_guidelines.pdf&quot;&gt;http://umanitoba.ca/faculties/medicine/units/mchp/protocol/media/manage_guidelines.pdf&lt;/a&gt;&lt;br/&gt;
&lt;/p&gt;
&lt;p&gt;
Most of the material below is taken verbatim from the original. Unfortunately many of the items described below have links to internal MCHP documents that we cannot access.  Nonetheless the structure of the guidelines provides a useful skeleton to frame our thinking. 
&lt;/p&gt;
&lt;p&gt;
The following areas should be reviewed with project team members near the beginning of the study and throughout the
project as needed:
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Confidentiality
&lt;/li&gt;
&lt;li&gt;Project team
&lt;/li&gt;
&lt;li&gt;File organization and documentation development
&lt;/li&gt;
&lt;li&gt;Communication
&lt;/li&gt;
&lt;li&gt;Administrative
&lt;/li&gt;
&lt;li&gt;Report Preparation
&lt;/li&gt;
&lt;li&gt;Project Completion
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.1&lt;/span&gt; Confidentiality&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-1&quot;&gt;

&lt;p&gt;Maintaining data access
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.2&lt;/span&gt; Project Team Makeup&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-2&quot;&gt;

&lt;p&gt;Roles and contact information should be documented on the project website for the following, where applicable (information may also be included on level of access approved for each team member).
&lt;/p&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.1&lt;/span&gt; Principal Investigator&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-1&quot;&gt;

&lt;p&gt;This is the lead person on the project, who assumes responsibility for delivering the project. The PI makes decisions on project direction and analysis requirements, with input from programmers and the research coordinator (an iterative process). If there is more than one PI (e.g., multi-site studies), overall responsibility for the study needs to be determined, and how the required work will be allocated and coordinated among the co-investigators. Researcher Workgroup website (internal link) 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.2&lt;/span&gt; Research Coordinator&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-2&quot;&gt;

&lt;p&gt;Th RC is always assigned to deliverables and is usually brought in on other types of projects involving multiple sites, investigators and/or programmers. Responsibilities include project documentation, project management (e.g., ensuring that timelines are met, ensuring that project specifications are being followed), and working with both investigator(s) and the Programmer Coordinator throughout the project to coordinate project requirements. 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.3&lt;/span&gt; The Programmer Coordinator&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-3&quot;&gt;

&lt;p&gt;The PC is a central management role who facilitates assignment of programming resources to projects, ensuring the best possible match among programmers and investigators.
Research Coordinator Workgroup website(internal link) 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.4&lt;/span&gt; Programmer Analyst&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-4&quot;&gt;

&lt;p&gt;This is primarily responsible for programming and related programming documentation (such that the purpose of the program and how results were derived can be understood by others). However, a major role may be taken in the analyses of the project as well, and this will characteristically vary with the project.
Programmer Analyst Workgroup website(internal link) 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2.5&lt;/span&gt; Research Support&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2-5&quot;&gt;

&lt;p&gt;This is primarily responsible for preparing the final product (i.e., the report), including editing and formatting of final graphs and manuscript and using Reference Manager to set up the references. Research support also normally sets up and attends working group meetings. All requests for research support go through the Office Manager.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.3&lt;/span&gt; Project Team considerations&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-3&quot;&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.1&lt;/span&gt; Roles&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-1&quot;&gt;

&lt;p&gt;It is important to clarify everyone's roles at the beginning of the project; for example, whether the investigator routinely expects basic graphs and/or programming logs from the programmer.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.2&lt;/span&gt; Continuity&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-2&quot;&gt;

&lt;p&gt;It is highly desirable to keep the same personnel, from the start of the project, where possible. It can take some time to develop a cohesive working relationship, particularly if work styles are not initially compatible. Furthermore, requesting others to temporarily fill in for team absences is generally best avoided, particularly for programming tasks (unless there is an extended period of absence). The original programmer will know best the potential impact of any changes that may need to be made to programming code.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.3&lt;/span&gt; Access levels&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-3&quot;&gt;

&lt;p&gt;Access to MCHP internal resources (e.g., Windows, Unix) need to be assessed for all team members and set up as appropriate to their roles on the project.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.4&lt;/span&gt; Working group&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-4&quot;&gt;

&lt;p&gt;A WG is always set up for deliverables (and frequently for other projects):
Terms of Reference for working group (internal)
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3.5&lt;/span&gt; Atmospherics&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3-5&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-4&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.4&lt;/span&gt; File organization and Documentation Development.&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-4&quot;&gt;

&lt;p&gt;All project-related documentation, including key e-mails used to update project methodology, should be saved within the project directory. Resources for directory setup and file development include:
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.1&lt;/span&gt; Managing MCHP resources&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-1&quot;&gt;

&lt;p&gt;This includes various process documents as well as an overview of the documentation process for incorporating research carried out by MCHP into online resources: Documentation Management Guide (internal)
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.2&lt;/span&gt; MCHP directory structure&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-2&quot;&gt;

&lt;p&gt;A detailed outline of how the Windows environment is structured at MCHP
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.3&lt;/span&gt; Managing project files&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-3&quot;&gt;

&lt;p&gt;How files and sub-directories should be organized and named as per the MCHP Guide to Managing Project Files (internal pdf). Information that may be suitable for incorporating into MCHP online resources should be identified; for example, a Concept Development section for subsequent integration of a new concept(s) into the MCHP
Concept Dictionary. The deliverable glossary is another resource typically integrated into the MCHP Glossary.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4.4&lt;/span&gt; Recommended Directories&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4-4&quot;&gt;

&lt;p&gt;NOTE this is a diversion from the MCHP guidelines.  These recommended directories are from a combination of sources that we have synthesised.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-1&quot;&gt;Background: concise summaries: possibly many documents for main project and any main analyses based on the 1:3:25 paradigm: one page of main messages; a three-page executive summary; 25 pages of detailed findings.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-2&quot;&gt;Proposals: for documents related to grant applications.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-3&quot;&gt;Approvals: for ethics applications.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-4&quot;&gt;Budget: spreadsheets and so-forth.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-5&quot;&gt;Data&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-5-1&quot;&gt;dataset1&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-5-2&quot;&gt;dataset2&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6&quot;&gt;Paper1&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-1&quot;&gt;Data&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-1-1&quot;&gt;merged dataset1 and 2&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2&quot;&gt;Analysis (also see &lt;a href=&quot;http://projecttemplate.net&quot;&gt;http://projecttemplate.net&lt;/a&gt; for a programmer oriented template)&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-1&quot;&gt;exploratory analyses&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-2&quot;&gt;data cleaning&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-3&quot;&gt;main analysis&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-4&quot;&gt;sensitivity analysis&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-5&quot;&gt;data checking&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-6&quot;&gt;model checking&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-2-7&quot;&gt;internal review&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3&quot;&gt;Document&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-1&quot;&gt;Draft&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-2&quot;&gt;Journal1&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-2-1&quot;&gt;rejected? :-(&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-3&quot;&gt;Journal2&lt;br/&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-3-3-1&quot;&gt;Response to reviews&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-4&quot;&gt;Versions: folders named by date - dump entire copies of the project at certain milestones/change points&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-6-5&quot;&gt;Archiving final data with final published paper&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-7&quot;&gt;Papers 2, 3, etc: same structure as paper 1 hopefully the project spawns several papers&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-8&quot;&gt;Communication: details of communication with stakeholders and decision makers&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-9&quot;&gt;Meetings: for organisation and records of meetings&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-10&quot;&gt;Contact details. table contacts lists&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-11&quot;&gt;Completion: checklists to make sure project completion is systematic.  Factor in a critical reflection of lessons learnt.&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-4-4-12&quot;&gt;References&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-5&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.5&lt;/span&gt; Communication&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-5&quot;&gt;

&lt;p&gt;Project communication should be in written form, wherever possible, to serve as reference for project documentation. Access and confidentiality clearance levels for all involved in the project will determine whether separate communication plans need to be considered for confidential information.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5.1&lt;/span&gt; E-mail&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5-1&quot;&gt;

&lt;p&gt;provides opportunities for feedback/ discussion from everyone and for documenting key project decisions. Responses on any given issue would normally be copied to every project member, with the expectation of receiving feedback within a reasonable period of time - e.g.,a few days). The Research Coordinator should be copied on ALL project correspondence in order to keep the information up to date on the project website.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-5-1-1&quot;&gt;E-mail etiquette (internal)&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5.2&lt;/span&gt; Meetings&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5-2&quot;&gt;

&lt;p&gt;Regularly-scheduled meetings or conference calls should include all project members where possible. Research Coordinators typically arrange project team meetings and take meeting minutes, while Research Support typically arranges the Working Group meetings.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-5-2-1&quot;&gt;Tips for taking notes (internal)&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-5-2-2&quot;&gt;Outlook calendar&lt;br/&gt;
Used for booking rooms, it displays information on room availability and may include schedules of team members.
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-6&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.6&lt;/span&gt; Administrative&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-6&quot;&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-6-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.6.1&lt;/span&gt; Time entry&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-6-1&quot;&gt;

&lt;p&gt;Time spent on projects should be entered by all MCHP employees who are members of the project team.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-6-1-1&quot;&gt;website for time entry (internal)&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-6-1-2&quot;&gt;procedures for time entry (internal)&lt;br/&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-7&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-7&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.7&lt;/span&gt; Report preparation&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-7&quot;&gt;

&lt;p&gt;This includes:
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Policies - e.g., Dissemination of Research Findings
&lt;/li&gt;
&lt;li&gt;Standards - e.g., deliverable production, use of logos, web publishing
&lt;/li&gt;
&lt;li&gt;Guidelines - e.g., producing PDFs, powerpoint, and Reference Manager files
&lt;/li&gt;
&lt;li&gt;Other resources - e.g., e-mail etiquette, technical resources, photos.
&lt;/li&gt;
&lt;/ul&gt;



&lt;/div&gt;

&lt;div id=&quot;outline-container-1-7-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-7-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.7.1&lt;/span&gt; Reliability and Validity Checks&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-7-1&quot;&gt;

&lt;p&gt;Making sure the numbers &quot;make sense&quot;. Carrying out these checks requires spelling out who will do which checks.
&lt;/p&gt;&lt;ul&gt;
&lt;li id=&quot;sec-1-7-1-1&quot;&gt;Data Validity Checks&lt;br/&gt;
A variety of things to check for at various stages of the study. Programming can be reviewed, for example, by checking to ensure all programs have used the right exclusions, the correct definitions, etc. , and output has been accurately transferred to graphs, tables, and maps for the report.
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-7-1-2&quot;&gt;Discrepancies between data sources&lt;br/&gt;
In this case it is MCHP and Manitoba Health Reports - an example of cross-checking against another source of data.
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1-8&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1.8&lt;/span&gt; Project Completion&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1-8&quot;&gt;

&lt;p&gt;Several steps need to take place to &quot;finish&quot; the project:
&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.1&lt;/span&gt; Final Project Meeting.&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-1&quot;&gt;

&lt;p&gt;Wind-up or debriefing meetings are held shortly after public release of a deliverable. Such meetings provide all team members with an opportunity to communicate what worked/did not work in bringing the project to completion, providing lessons learned for future deliverables.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.2&lt;/span&gt; Final Documentation Review.&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-2&quot;&gt;

&lt;p&gt;Findings from the wind-up meeting should be used to update and finalize the project website (including entering the date of release of report/paper). Both Windows and Unix project directories should be reviewed to ensure that only those SAS programs relevant to project analyses are kept (and well-documented) for future reference. Any related files which may be stored in a user directory should be moved to the project directory.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.3&lt;/span&gt; System Cleanup.&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-3&quot;&gt;

&lt;p&gt;When the project is complete, the Systems Administrator should be informed. Project directories, including program files and output data sets, will be archived to tape or CD. Tape backups are retained for a 5-year period before being destroyed so any project may be restored up to five years after completion.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8.4&lt;/span&gt; Integration of new material to institution repository&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8-4&quot;&gt;

&lt;p&gt;This is with MCHP resource repository - a general overview of this process is described in General Documentation Process {internal}.
&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>graphviz-automagic-flowcharts</title>
   <link href="http://ivanhanigan.github.com/2013/12/graphviz-automagic-flowcharts/"/>
   <updated>2013-12-01T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/graphviz-automagic-flowcharts</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;Back in 2009 Joseph Guillaume worked with me on a complicated workflow&lt;/li&gt;
&lt;li&gt;He came up with this python script to help keep track of the steps&lt;/li&gt;
&lt;li&gt;The simple text file is a list of transformations, inputs and output&lt;/li&gt;
&lt;li&gt;It is converted to the right format and graphviz creates a html page with pop-outs&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Simple text list&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Transformation
        description
                the thing 
        inputs
                the thing before
                another thing
        output
                the next thing
        notes
                the thing is this other thing   this is a really long description 
                blah blah asdfasdfasdfasdfasdfa 

Transformation
        description
                yet another thing
        inputs
                the next thing
        output
                a final thing
        notes
                this is a note
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;keep this in your work directory and update it whenever you add a step to the workflow&lt;/li&gt;
&lt;li&gt;the list can be as big as you like (hundreds of steps), and entered in any order, the inputs/output relationships determine how the graph looks at the end&lt;/li&gt;
&lt;li&gt;to run the script just do the one line&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Python code: run&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;python transformations.py workflow_steps.txt index
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;open the html page and click on a square box to bring up the pop-out&lt;/li&gt;
&lt;li&gt;short text is shown, long text is replaced by an ellipse and only shown in pop-out&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I've popped the script up as a &lt;a href=&quot;https://github.com/ivanhanigan/transformations&quot;&gt;Github repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The example is in the &lt;a href=&quot;http://ivanhanigan.github.io/transformations/&quot;&gt;gh-pages branch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Gantt Charts For Health Professionals</title>
   <link href="http://ivanhanigan.github.com/2013/12/gantt-charts-for-health-professionals/"/>
   <updated>2013-12-01T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/12/gantt-charts-for-health-professionals</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;Gantt Charts are a method used in Project Management to visualise the task list and progress with a calendar.&lt;/li&gt;
&lt;li&gt;The title of this post comes from &lt;a href=&quot;http://berkeley.academia.edu/TomasAragon/Teaching/31679/Project_Management_for_Health_Professionals&quot;&gt;Dr Tomas Aragon's paper 'Project Management for Health Professionals'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There is also an accompanying &lt;a href=&quot;http://medepi.com/2012/02/05/project-mgmt/&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dr Aragon also has a couple of suggestions for software to use for creating Gantt Charts &lt;a href=&quot;http://medepi.com/2011/11/29/software-recs/&quot;&gt;on this page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I haven't used those but here are some I have used&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Gantter&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;This is an online tool, that can integrate with Google Drive &lt;a href=&quot;http://www.gantter.com/&quot;&gt;http://www.gantter.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It also imports MS Project files&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Taskjuggler&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Emacs orgmode can be used with a Ruby package called Taskjuggler (tj3)&lt;/li&gt;
&lt;li&gt;The command C-c C-e j will export a file such as this description ready for conversion &lt;a href=&quot;http://orgmode.org/worg/org-tutorials/org-taskjuggler.html&quot;&gt;http://orgmode.org/worg/org-tutorials/org-taskjuggler.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I also needed to include this suggested by d.tchin 29 Feb 2012,  &lt;a href=&quot;http://permalink.gmane.org/gmane.emacs.orgmode/52844&quot;&gt;http://permalink.gmane.org/gmane.emacs.orgmode/52844&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;an example I use for a template is in my &lt;a href=&quot;https://raw.github.com/ivanhanigan/disentangle/gh-pages/gantt-tj3/gantt-tj3.org&quot;&gt;disentangle Github repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;which produces &lt;a href=&quot;http://ivanhanigan.github.io/disentangle/gantt-tj3/Gantt%20Chart.html&quot;&gt;this output&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/gantt-tj3.png&quot; alt=&quot;gantt-tj3.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>sharing-and-extending-research-protocols</title>
   <link href="http://ivanhanigan.github.com/2013/11/sharing-and-extending-research-protocols/"/>
   <updated>2013-11-27T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/sharing-and-extending-research-protocols</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;Last week I started a new discussion about sharing and extending research protocols.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;My Colleague sent me a link to the &lt;a href=&quot;http://prometheuswiki.publish.csiro.au/tiki-custom_home.php&quot;&gt;Prometheus Wiki&lt;/a&gt;​&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;a site for sharing research protocols. The idea is to give people
a place to post research protocols since everyone develops them
and then mentions them in papers but they rarely make it online in
a usable format. We see this as a big bottleneck in the rate of
knowledge discovery. Since users will always want to change and
adapt a given protocol for changing applications or new technology
or just to improve it. This sort of thing is done with open source
all the time (i.e. git) but how you'd do it with an online
protocol based out of a wiki is unknown. The current version of
the website is not very flexible and doesn't really enable the
sort of dynamic collaboration that I envision would make this a
really useful tool. Git seems like an obvious conceptual starting
point but there'd need to be a front end markup system in place
that was integrated with that git-type backend. Since you've
mentioned git a few times and the open science project, I thought
you might find this interesting to think about. any suggestions or
inspirations would be welcomed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;My past experience&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I have previously tried to use a wiki for the development of new sections in an existing study protocol on estimating the effects of bushfire smoke air pollution and human health.&lt;/li&gt;
&lt;li&gt;We combined elements of two major air pollution/health study protocols (one from Europe, one from America) with local considerations and decisions made within our team.&lt;/li&gt;
&lt;li&gt;On reflection all this was mostly useful for me, to a lesser extent the other programmer and I don't think the project manager or statistician ever even looked at any of this.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Jeff Leek's current approach using GitHub&lt;/h4&gt;

&lt;p&gt;I have been keeping an eye on the work by Jeff Leek on putting protocols onto Github and inviting collaboration to edit and extend them &lt;a href=&quot;http://simplystatistics.org/2013/10/07/the-leek-group-policy-for-developing-sustainable-r-packages/&quot;&gt;as he says&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I put it on Github because I'm still not 100% sure I got it
right... I would welcome feedback/pull requests on how we can
improve the policy to make it better
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Leek Group policies and protocols on github so far&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jtleek/rpackages&quot;&gt;https://github.com/jtleek/rpackages&lt;/a&gt; accompanied by &lt;a href=&quot;http://simplystatistics.org/2013/10/07/the-leek-group-policy-for-developing-sustainable-r-packages/&quot;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jtleek/reviews&quot;&gt;https://github.com/jtleek/reviews&lt;/a&gt; accompanied by &lt;a href=&quot;http://simplystatistics.org/2013/10/23/the-leek-group-guide-to-reviewing-scientific-papers/&quot;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jtleek/datasharing&quot;&gt;https://github.com/jtleek/datasharing&lt;/a&gt; accompanied by &lt;a href=&quot;http://simplystatistics.org/2013/11/14/the-leek-group-guide-to-sharing-data-with-a-statistician-to-speed-collaboration/&quot;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;I forked &quot;datasharing&quot; for my own. Renamed: &quot;How to share data to avoid misunderstanding&quot;&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I liked the datasharing policy so much &lt;a href=&quot;http://ivanhanigan.github.io/datasharing/&quot;&gt;I forked it for my own collaborations&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I initially just made a minor recommendation to one of the lines in the original using the great Github feature that if you edit a repo you don't have access to, it forks the repo and creates a feature branch behind the scenes. After you submit the changes (which is a commit) it puts you right into the pull request form.&lt;/li&gt;
&lt;li&gt;Therefore it can all be done on the GitHub site. I submitted &lt;a href=&quot;https://github.com/jtleek/datasharing/pull/11&quot;&gt;the pull request without leaving my web browser&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;But then I thought I would have done things a bit differently.  In particular I'd like a webpage with a table of contents, and a nicer looking landing page.&lt;/li&gt;
&lt;li&gt;so I cloned my fork of the repo, then created a new branch called gh-pages, then copied the original to a new file called index.org, added some Emacs Orgmode HTML export magic and then &quot;C-c C-e h&quot; and WHAMM-O I've got my own version for extending with at &lt;a href=&quot;http://ivanhanigan.github.io/datasharing/&quot;&gt;http://ivanhanigan.github.io/datasharing/&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;The code version:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:ivanhanigan/datasharing.git ~/tools/datasharing
cd ~/tools/datasharing
git checkout -b gh-pages
mv README.md index.org
touch README.md
# edits to index.org, &quot;C-c C-e h&quot; and WHAMM-O
git add index.org index.html README.md
git push origin gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;In this way I think people can share and collaborate easily &lt;a href=&quot;http://yihui.name/en/2013/06/fix-typo-in-documentation/&quot;&gt;(if they know git)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;as well as easily take work started by someone else and extend it for their own work.&lt;/li&gt;
&lt;li&gt;I wonder a little bit about how much of my extensions I should offer back to Jeff Leek as the original author.&lt;/li&gt;
&lt;li&gt;I guess he can always track what I do to it via the link back to his original shown in &lt;a href=&quot;https://github.com/jtleek/datasharing/network&quot;&gt;https://github.com/jtleek/datasharing/network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;PS Jeff Leek uses github a lot!&lt;/h4&gt;

&lt;p&gt;As he says in &lt;a href=&quot;http://simplystatistics.org/2013/11/21/future-of-statistics-take-home-messages-futureofstats/&quot;&gt;this post from the future-of-statistics unconference&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;You can read it on Github here (https://github.com/jtleek/futureofstats).
I put it on Github for two reasons:

- I agree with Hadley's statement that the future of statistics is on Github.
- I summarized them based on my interpretation and would love
  collaboration on the document. If you want to add your new
  thoughts/summaries, add a new section with your bullet pointed
  ideas and send me a pull request!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Jeff Leek is on to something really interesting with these github policies and protocols&lt;/li&gt;
&lt;li&gt;I find the look of his plain markdown README.md pages a bit un-inspiring&lt;/li&gt;
&lt;li&gt;but then I spend too much of my time tweaking my html by far (while Jeff is getting on with the real work of publishing papers)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>our-paper-is-out-heat-health-warning-systems</title>
   <link href="http://ivanhanigan.github.com/2013/11/our-paper-is-out-heat-health-warning-systems/"/>
   <updated>2013-11-27T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/our-paper-is-out-heat-health-warning-systems</id>
   <content type="html">&lt;p&gt;Finally our article has been published in final version in Environmental Health.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Title: The impact of heat on mortality and morbidity in the Greater Metropolitan Sydney Region: a case crossover analysis&lt;/li&gt;
&lt;li&gt;Authors: Wilson Ann Leigh, Gerard Morgan  Geoffrey, Hanigan Charles Ivan, Johnston H Fay, Abu-Rayya  Hisham, Broome  Richard, Gaskin  Clive, Jalaludin  Bin,&lt;/li&gt;
&lt;li&gt;Environmental Health.2013, 12:98.&lt;/li&gt;
&lt;li&gt;DOI: 10.1186/10.1186/1476-069X-12-98&lt;/li&gt;
&lt;li&gt;URL: &lt;a href=&quot;http://www.ehjournal.net/content/12/1/98&quot;&gt;http://www.ehjournal.net/content/12/1/98&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Finally.......
thanks to everyone,  especially Geoff Morgan, Richard Broome, Fay Johnston; Bin Jalaludin&lt;/p&gt;

&lt;p&gt;And especially Leigh Wilson.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>guest-post-by-marco-fahmi-why-morpho</title>
   <link href="http://ivanhanigan.github.com/2013/11/guest-post-by-marco-fahmi-why-morpho/"/>
   <updated>2013-11-27T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/guest-post-by-marco-fahmi-why-morpho</id>
   <content type="html">&lt;p&gt;I asked my colleague &lt;a href=&quot;https://twitter.com/fahmiger&quot;&gt;Marco Fahmi&lt;/a&gt; to
post this as a guest post.  It came my way as a part of an email
exchange in which another colleague had a question regarding their task
of recording previous work in an ecological field study:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ideally in such a way that we would end up with a complete metadata
profile of previous work carried out. I would also like to see the
establishment of a system that could also be used by the group going
forward to keep track of information and data produced at the
site. I hear you have been involved with establishing the
a metadata reporting system. How does this effort currently stand,
is it online? I was also wondering if you would be amenable to sharing
what you have with us with the hope that i could use this as a model
for our own system.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;Our colleague Sheila responded first that:&lt;/h4&gt;

&lt;p&gt;All metadata are created using a standard template document and then transfered into a software package called &lt;a href=&quot;http://knb.ecoinformatics.org/morphoportal.jsp&quot;&gt;Morpho&lt;/a&gt;.  Metadata are then uploaded with the data to the Australian Supersite Network (ASN) Portal &lt;a href=&quot;http://www.tern-supersites.net.au/knb/&quot;&gt;http://www.tern-supersites.net.au/knb/&lt;/a&gt;.   Any other useful documents are uploaded to the website &lt;a href=&quot;http://www.tern-supersites.net.au&quot;&gt;http://www.tern-supersites.net.au&lt;/a&gt; either under the specific supersite tab on the left hand menu or under the Publications - Resources for SuperSite Users tab.&lt;/p&gt;

&lt;h4&gt;Marco says:&lt;/h4&gt;

&lt;p&gt;Morpho is an open source piece of software designed to host all kinds of ecological data. More information about how we use it at ASN can be found here: &lt;a href=&quot;http://www.tern-supersites.net.au/index.php/data/repository-tutorial&quot;&gt;http://www.tern-supersites.net.au/index.php/data/repository-tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Morpho should be enough for an individual researcher to organise and describe their personal data collection. If you want to share the data with colleagues or publish them online, then you will also need Metacat. There is a worldwide Metacat server available from the link account. All you need to do is request an account and connect to it via Morpho. Alternatively you can set up your own; but then you will need your own server and tech knowhow to configure it (and maintain it).&lt;/p&gt;

&lt;p&gt;For technical reasons, we have our own server running an older version of the Metacat software. You are welcome to use it if you wish (Shiela can issue you an account to log in and upload). We are also happy to provide assistance if you want to set something a standalone server for DRO. (Like any other piece of infrastructure, someone will need to look after the server after it is set up, so that's probably a decision that will need to be considered carefully).&lt;/p&gt;

&lt;h4&gt;My comment:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;&quot;Morpho should be enough for an individual researcher to organise
and describe their personal data collection&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I agree, but would emphasise the &lt;em&gt;should&lt;/em&gt; and then say &lt;em&gt;but&lt;/em&gt; ...
Ultimately I'd like to see something as easy and intuitive as iTunes for music or Endnote and Mendeley for bibliographies, but...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>a-sharp-looking-orgmode-latex-export-header</title>
   <link href="http://ivanhanigan.github.com/2013/11/a-sharp-looking-orgmode-latex-export-header/"/>
   <updated>2013-11-26T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/a-sharp-looking-orgmode-latex-export-header</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;I got this header for a nice looking report from Bull, G. (2011). Example Sweave Document. SharpStatistics.co.uk.&lt;/li&gt;
&lt;li&gt;This has a bunch of useful parameters, but I just really like the header and footer on pages 2 onward&lt;/li&gt;
&lt;li&gt;The original was a Sweave file.  I really like Sweave but orgmode allows other languages as well as R to be inter-woven into the script&lt;/li&gt;
&lt;li&gt;An alternative to Sweave is knitr and is still on my todo list, but this works well at the moment&lt;/li&gt;
&lt;li&gt;I also like how you can quickly change this to a Beamer presentation style.&lt;/li&gt;
&lt;li&gt;Once this is in your file use C-c C-e d to export and compile the PDF&lt;/li&gt;
&lt;li&gt;This example is available at &lt;a href=&quot;/pdfs/SharpReportTemplate.pdf&quot;&gt;this link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Emacs orgmode Code: Put this into your .org file&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#+TITLE: Sharp Report Template
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LaTeX_HEADER: \usepackage{amssymb,amsmath}
#+LaTeX_HEADER: \usepackage{fancyhdr} %For headers and footers
#+LaTeX_HEADER: \pagestyle{fancy} %For headers and footers
#+LaTeX_HEADER: \usepackage{lastpage} %For getting page x of y
#+LaTeX_HEADER: \usepackage{float} %Allows the figures to be positioned and formatted nicely
#+LaTeX_HEADER: \floatstyle{boxed} %using this
#+LaTeX_HEADER: \restylefloat{figure} %and this command
#+LaTeX_HEADER: \usepackage{url} %Formatting of yrls
#+LaTeX_HEADER: \lhead{ivanhanigan.github.com}
#+LaTeX_HEADER: \chead{}
#+LaTeX_HEADER: \rhead{\today}
#+LaTeX_HEADER: \lfoot{Draft}
#+LaTeX_HEADER: \cfoot{}
#+LaTeX_HEADER: \rfoot{\thepage\ of \pageref{LastPage}}
#+LATEX: \tableofcontents

* Introduction
This is a sharp looking report template I got from an R blogger \cite{Bull2011}.

The pages after the first page have a nice looking header, footer and page number.
\clearpage

* Section 1
In the Org file you can see some hidden R code that computes a linear regression and returns the results shown in Table \ref{ATable}.
\input{ATable.tex}
\clearpage
*** COMMENT some-code
#+name:some-code
#+begin_src R :session *R* :tangle no :exports none :eval yes
#### name:some-code ####
x&amp;lt;-rnorm(100,10,5)
y&amp;lt;-rnorm(100,20,15)
fit &amp;lt;- lm(y~x)
library(xtable)
sink(&quot;ATable.tex&quot;)
xtable(fit, caption=&quot;Example Table&quot;,digits=4,table.placement=&quot;H&quot;,label=&quot;ATable&quot;)
sink()
#+end_src

#+RESULTS: some-code



* References
\bibliographystyle{apalike}
\bibliography{/home/ivan_hanigan/references/library}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Setting Up A Workflow Script With Code Chunks</title>
   <link href="http://ivanhanigan.github.com/2013/11/setting-up-a-workflow-with-code-chunks/"/>
   <updated>2013-11-25T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/setting-up-a-workflow-with-code-chunks</id>
   <content type="html">&lt;p&gt;This post describes some ideas and techniques I use to set up a &quot;workflow script&quot;.  I use this term to refer to the structured combination of code, data and narrative that make an executable Reproducible Research Report (RRR).&lt;/p&gt;

&lt;p&gt;A lot of these ideas are inpsired by  a great paper by Kieran Healy called  &quot;Choosing Your Workflow Applications&quot; available at &lt;a href=&quot;https://github.com/kjhealy/workflow-paper&quot;&gt;https://github.com/kjhealy/workflow-paper&lt;/a&gt; to accompany &lt;a href=&quot;http://kieranhealyo.org/resources/emacs-starter-kit.html&quot;&gt;his Emacs Starter Kit&lt;/a&gt;. My shortened version of his main points are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1 use a good code editor&lt;/li&gt;
&lt;li&gt;2 analyse data with scripts&lt;/li&gt;
&lt;li&gt;3 store your work simply and document it properly&lt;/li&gt;
&lt;li&gt;4 use a version control system&lt;/li&gt;
&lt;li&gt;5 Automate back ups&lt;/li&gt;
&lt;li&gt;6 Avoid distracting gadgets&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Here's my current approach in each of these categories&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;1 use Emacs with Orgmode (and kjhealy's drop-in set of useful defaults)&lt;/li&gt;
&lt;li&gt;2 Scripts that utilise the literate programming technique of mixing Code Chunks in with descriptive prose&lt;/li&gt;
&lt;li&gt;3 John Myles White's ProjectTemplate R Package and Josh Riech's LCFD paradigm&lt;/li&gt;
&lt;li&gt;4 git and GitHub for version control&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;5 Automated Backups and 6 Avoiding Gadgets are still somethings I find challenging&lt;/p&gt;

&lt;h4&gt;1 Use a good code editor&lt;/h4&gt;

&lt;p&gt;I like using Emacs with Orgmode.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I have previously tried a variety of code editors from Tinn-r, NppToR, Rstudio and Eclipse.&lt;/li&gt;
&lt;li&gt;Emacs with Orgmode suits me the most because it has a great number of features especially the linkage with LaTeX or HTML export&lt;/li&gt;
&lt;li&gt;A key reference to look at for reasons why Emacs is so good for scientific work is Eric Schulte et al &lt;a href=&quot;www.jstatsoft.org/v46/i03%E2%80%8E&quot;&gt;&quot;A Multi-Language Computing Environment for Literate Programming&quot;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Here is a &lt;a href=&quot;http://doc.norang.ca/org-mode.html&quot;&gt;link to a great orgmode description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(this guy spends a lot of time on tweaking his set up)&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;2 Analyse data with Scripts (stitch together code chunks)&lt;/h4&gt;

&lt;p&gt;I use Scripts but prefer to think of them as stitched together Code Chunks with prose into Compendia.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compendia are documents that weave together Code and Prose into an executable report&lt;/li&gt;
&lt;li&gt;The underlying philosophy is called Reproducible Research Reports&lt;/li&gt;
&lt;li&gt;A very useful tool is a keyboard shortcut to quickly create a chunk for code&lt;/li&gt;
&lt;li&gt;so you can be writing parts of the report like this: &quot;Blah Blah Blah as shown in Figure X and Table Y&quot;&lt;/li&gt;
&lt;li&gt;then just hit the correct keys and WHAMM-O there is a new chunk ready for the code that creates Figure X and Table Y to be written.&lt;/li&gt;
&lt;li&gt;Here is how I use Emacs to achieve this (the other editors I mentioned above also have the abiltiy to do this too).  The IPython Notebook does this stuff too but calls chunks &quot;cells&quot; for some reason.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Emacs Code: Put this into the ~/.emacs.d/init.el file&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;(define-skeleton chunk-skeleton
  &quot;Info for a code chunk.&quot;
  &quot;Title: &quot;
  &quot;*** &quot; str &quot;-code\n&quot;
  &quot;#+name:&quot; str &quot;\n&quot;
  &quot;#+begin_src R :session *R* :tangle src/&quot; str &quot;.r :exports reports :eval no\n&quot;
  &quot;#### name:&quot; str &quot; ####\n&quot;
  &quot;\n&quot;
  &quot;#+end_src\n&quot;
)
(global-set-key [?\C-x ?\C-\\] 'chunk-skeleton)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Using the Emacs Shortcut&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;now whenever you type Control-x control-\ a new code chunk will appear&lt;/li&gt;
&lt;li&gt;you'll be typing &quot;blah blah blah&quot; and think I need a figure or table, just hit it.&lt;/li&gt;
&lt;li&gt;move into the empty section and add some code&lt;/li&gt;
&lt;li&gt;you can hit C-c ' to enter a org-babel code execution session that will be able to send these line by line to an R session&lt;/li&gt;
&lt;li&gt;or within the main org buffer if your eval flag is set to yes then you can run the entire chunk (and return tabular output to the doc) using C-c C-c&lt;/li&gt;
&lt;li&gt;To export the code chunks and create the modular code scripts without the narrative prose use C-c C-v t&lt;/li&gt;
&lt;li&gt;this is called &quot;tangling&quot; and the chunks will be written out to the file specified in the chunk header &quot;:tangle&quot; flag&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Compiling the resulting Compendium&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Emacs uses LaTeX or HTML to produce the Report&lt;/li&gt;
&lt;li&gt;I find both of these outputs very pleasing&lt;/li&gt;
&lt;li&gt;to compile to TEX use C-c C-e d&lt;/li&gt;
&lt;li&gt;for HTML use C-c C-e h (FOR CODE HIGHLIGHTING INSTALL htmlize.el)&lt;/li&gt;
&lt;li&gt;these commands will also evaluate all the chunks where &quot;:eval&quot; = yes to load the data and calculate the results fresh.&lt;/li&gt;
&lt;li&gt;AWESOME!&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;3 Store your work simply and document it properly&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I use the &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/&quot;&gt;ProjectTemplate R package&lt;/a&gt; to organise my code sections into modules&lt;/li&gt;
&lt;li&gt;These modules are organised into the Reichian LCFD paradigm described first &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;on StackOverflow here&lt;/a&gt;, and encoded into &lt;a href=&quot;http://cran.r-project.org/web/packages/makeProject/makeProject.pdf&quot;&gt;the makeProject R package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;documentation is within the main orgmode script&lt;/li&gt;
&lt;li&gt;data documentation is a whole other universe that I will deal with in a separate post&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;4 use a version control system using git and github&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# once you have the project via R
R
require(ProjectTemplate)
create.project(&quot;AwesomeProject&quot;, minimal = T)
q()
# use the shell to start a git repo
cd AwesomeProject
git init
# and commit the TODO
git add TODO
git commit -m &quot;first commit&quot;
# tada!
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Emacs can now be used to manage the git repo using the C-x g command&lt;/li&gt;
&lt;li&gt;Rstudio has a really nice GUI for doing this inside it;s Project management interface too.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Using Github or another Git Server&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;You can easily set up a Github repo for this now but it will be public&lt;/li&gt;
&lt;li&gt;Alternatative is to set up your own private Git server.  I followed &lt;a href=&quot;http://blog.goosoftware.co.uk/2012/02/07/quick-git-server/&quot;&gt;these instructions to Running a Simple Git Server Using SSH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Either way once you have set up your remote git repo you need to set the remote tracking&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Git Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cd /path/to/local/git/repo
git remote add origin git@github-or-other-server:myname/myproject.git
git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;5 Automate back ups AND 6 Avoid distracting gadgets&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;OMG backups stress me out&lt;/li&gt;
&lt;li&gt;ideally I would follow &lt;a href=&quot;http://www.jwz.org/blog/2007/09/psa-backups/&quot;&gt;this advice because &quot;when it comes to losing your data the universe tends toward maximum irony. Don't push it.&quot;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;But I don;t fully comply&lt;/li&gt;
&lt;li&gt;Instead I generally use Dropbox for  basic project management admin stuff&lt;/li&gt;
&lt;li&gt;I use github for code projects I am happy to share, I also pay for 10 private repos&lt;/li&gt;
&lt;li&gt;I Set up a git server at my workplace for extra projects but this is on a test server that is not backed up, and I am not really happy about this&lt;/li&gt;
&lt;li&gt;In terms of Distracting Gadgets, I think that with the current tempo of new innovations related to new software tools for this type of work I should keep trying new things but I have pretty much settled into a comfortable zone with the gadgets I described here.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;This is how I've worked for a couple of years&lt;/li&gt;
&lt;li&gt;I find it very enjoyable, mostly productive but prone to the distractions of &quot;distractions by gadgets&quot;&lt;/li&gt;
&lt;li&gt;The main thing I want to point out is the usage of Code Chunks in RRR scripts.&lt;/li&gt;
&lt;li&gt;These things are awesome.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>sync-endnote-and-mendeley-references-using-r-xml</title>
   <link href="http://ivanhanigan.github.com/2013/11/sync-endnote-and-mendeley-using-r-xml/"/>
   <updated>2013-11-20T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/sync-endnote-and-mendeley-using-r-xml</id>
   <content type="html">&lt;h4&gt;Background&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I use Mendeley (despite them being bought out by Elsevier, who used to sell guns)&lt;/li&gt;
&lt;li&gt;My Colleagues use EndNote&lt;/li&gt;
&lt;li&gt;Need to sync, they find Endnote better for their workflow&lt;/li&gt;
&lt;li&gt;I tried to export my Mendeley as XML and import to Endnote, but found many duplicates that took time to rectify&lt;/li&gt;
&lt;li&gt;(and the risk is there that the RefNo they used in the Doc will be the duplicate that I removed)&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Aims&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;test if R and the XML package can help find refs in Endnote that aren't in Mendeley&lt;/li&gt;
&lt;li&gt;If so can I write those into an Mendeley import for seamless integrations&lt;/li&gt;
&lt;li&gt;and what about going from Mendeley to Endnote?&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Methods&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;The R XML package seems an obvious place to start&lt;/li&gt;
&lt;li&gt;before writing a function, just step thru the process&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Step 1: export XML from Mendeley and Endnote&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;In Mendeley Just select the refs in the list and then from the file menu&lt;/li&gt;
&lt;li&gt;In Endnote it is also under File menu&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Step 2: R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
# might need sudo apt-get install r-cran-xml?
require(XML)

# load
dir()
[1] &quot;EndnoteCollection.xml&quot;
    &quot;MendeleyCollection.Data&quot;
[3] &quot;MendeleyCollection.xml&quot; 

d1 &amp;lt;- xmlTreeParse(&quot;EndnoteCollection.xml&quot;, useInternal=T)

# clean
str(d1)
# ooooh xml schmexemhel voodoo?

# do
top &amp;lt;- xmlRoot(d1)
str(top)
names(top)
# top [[1]] # prints the whole thing
top [[1]][[1]]
top [[1]][[2]]
# prints a record (1 or 2)

# just messing around
length(top[[1]])
top [[1]][[120]]
names(top [[1]][[120]])
names(top [[1]][[120]][[&quot;contributors&quot;]])
names(top [[1]][[120]][[&quot;contributors&quot;]][[&quot;authors&quot;]])
top [[1]][[120]][[&quot;contributors&quot;]][[&quot;authors&quot;]][[2]]

i &amp;lt;- 110
top [[1]][[i]]
as.matrix(names(top [[1]][[i]]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;OK so XML as a list.&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I think if I do a merge of two author-date-title dataframes I can easily find the diffs&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;TRY a square wheel&lt;/h4&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;endnote_mendeley_df &amp;lt;- function(input_xml,
                                nrow_to_try = 1000){
  d1 &amp;lt;- xmlTreeParse(input_xml, useInternal=T)
  top &amp;lt;- xmlRoot(d1)

  output &amp;lt;- matrix(ncol = 4, nrow = 0)
  for(i in 1:nrow_to_try)
  {
  # i = 1000
    if(is.na(xmlValue(top [[1]][[i]]))) break
    if(
      !is.na(xmlValue(top [[1]][[i]][[&quot;contributors&quot;]][[&quot;authors&quot;]][[2]]))
      )
    {
      author &amp;lt;- paste(xmlValue(top [[1]][[i]][[&quot;contributors&quot;]][[&quot;authors&quot;]][[1]]), &quot;et al&quot;, &quot; &quot;)
    } else {
      author &amp;lt;- xmlValue(top [[1]][[i]][[&quot;contributors&quot;]][[&quot;authors&quot;]][[1]])
    }
    year &amp;lt;- xmlValue(top [[1]][[i]][[&quot;dates&quot;]][[&quot;year&quot;]])
    title &amp;lt;- xmlValue(top [[1]][[i]][[&quot;titles&quot;]][[1]])
    endnoteref &amp;lt;- xmlValue(top [[1]][[i]][[&quot;rec-number&quot;]])
    output &amp;lt;- rbind(output, c(author, year, title, endnoteref))

  }
  output &amp;lt;- as.data.frame(output)
  return(output)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Test:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;output &amp;lt;- endnote_mendeley_df(
  input_xml = &quot;EndnoteCollection.xml&quot;
  ,
  nrow_to_try = 10
  )

nrow(output)
write.csv(output, &quot;EndnoteCollection.csv&quot;, row.names = F)
output  &amp;lt;- read.csv(&quot;EndnoteCollection.csv&quot;, stringsAsFactors = F)
str(output)
output[,1:2]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Do-read:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;endnote &amp;lt;- endnote_mendeley_df(
  input_xml = &quot;EndnoteCollection.xml&quot;
  )
nrow(endnote)
mendeley &amp;lt;- endnote_mendeley_df(
  input_xml = &quot;MendeleyCollection.xml&quot;
  )
nrow(mendeley)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Do-concatenate and lowercase&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# TODO this is a really terrible way to do this.
# FIXME find out how to compare the two better
require(stringr)
mendeley2 &amp;lt;- str_c(mendeley$V1, mendeley$V2, mendeley$V3)
mendeley2 &amp;lt;- gsub(&quot; &quot;, &quot;&quot;, mendeley2)
mendeley2 &amp;lt;- gsub(&quot;,&quot;, &quot;&quot;, mendeley2)
mendeley2 &amp;lt;- tolower(mendeley2)
mendeley2[1:5]
mendeley$mendeley2 &amp;lt;- mendeley2

# now do this again from endnote
endnote2 &amp;lt;- str_c(endnote$V1, endnote$V2, endnote$V3)
endnote2 &amp;lt;- gsub(&quot; &quot;, &quot;&quot;, endnote2)
endnote2 &amp;lt;- gsub(&quot;,&quot;, &quot;&quot;, endnote2)
endnote2 &amp;lt;- tolower(endnote2)
endnote2[1:5]
endnote$endnote2 &amp;lt;- endnote2
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Do-merge:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;endnote_not_in_mendeley &amp;lt;- merge(endnote,
                                 mendeley,
                                 by.x = &quot;endnote2&quot;,
                                 by.y = &quot;mendeley2&quot;,
                                 all.x = T)
str(endnote_not_in_mendeley)
nrow(endnote_not_in_mendeley)
head(endnote_not_in_mendeley)
endnote_not_in_mendeley &amp;lt;- endnote_not_in_mendeley[
                                                   is.na(endnote_not_in_mendeley$V1.y),
                                                   ]
nrow(endnote_not_in_mendeley)
# 66 refs in endnote are not in mendeley
write.csv(endnote_not_in_mendeley,
      &quot;endnote_not_in_mendeley.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Open this as spreadsheet and cross check&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;make a new column for comments&lt;/li&gt;
&lt;li&gt;check off which ones were in AllDocuments and not in the Mendeley group&lt;/li&gt;
&lt;li&gt;this diff was because of when I imported the Endnote XML but had not assigned these to the mendeley group&lt;/li&gt;
&lt;li&gt;once have cleaned up the mendeley group export again and then check which are in mendeley but not in endnote&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;First here is a note&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;about a way to speed up the checks, excluding false positives using fuzzy matching&lt;/li&gt;
&lt;li&gt;my method relies on the author, date and title to be written the same in both ie initials then surname or visa verca&lt;/li&gt;
&lt;li&gt;But this is not always true&lt;/li&gt;
&lt;li&gt;I previously used levenshtein string matching to identify strings that are close but not identical&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://wiki.r-project.org/rwiki/doku.php?id=tips:data-strings:levenshtein&quot;&gt;Try this link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://rwiki.sciviews.org/doku.php?id=tips:data-strings:levenshtein&quot;&gt;OR this link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TODO I will share this code as a GitHub Gist later!&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code: possibility to speed up Checks&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;tmp1 &amp;lt;- mendeley[grep(&quot;Walker&quot;, mendeley$V1),&quot;mendeley2&quot;]
tmp2 &amp;lt;- endnote[grep(&quot;Walker&quot;, endnote$V1),&quot;endnote2&quot;]

# these differ slightly
# B. Walker et al vs Walker, Brian et al
source(&quot;~/Dropbox/tools/levenshtein.r&quot;)
levenshtein(
    tmp1
    ,
    tmp2
    )
# gives 92percent match
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Code: Find mendeley refs without endnote&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;  endnote &amp;lt;- endnote_mendeley_df(
    input_xml = &quot;EndnoteCollection.xml&quot;
    )
  nrow(endnote)
  mendeley &amp;lt;- endnote_mendeley_df(
    input_xml = &quot;MendeleyCollection2.xml&quot;
    )
  nrow(mendeley)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Code: Do-concatenate and lowercase&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(stringr)
mendeley2 &amp;lt;- str_c(mendeley$V1, mendeley$V2, mendeley$V3)
mendeley2 &amp;lt;- gsub(&quot; &quot;, &quot;&quot;, mendeley2)
mendeley2 &amp;lt;- gsub(&quot;,&quot;, &quot;&quot;, mendeley2)
mendeley2 &amp;lt;- tolower(mendeley2)
mendeley2[1:5]
mendeley$mendeley2 &amp;lt;- mendeley2

# now do this again from endnote
endnote2 &amp;lt;- str_c(endnote$V1, endnote$V2, endnote$V3)
endnote2 &amp;lt;- gsub(&quot; &quot;, &quot;&quot;, endnote2)
endnote2 &amp;lt;- gsub(&quot;,&quot;, &quot;&quot;, endnote2)
endnote2 &amp;lt;- tolower(endnote2)
endnote2[1:5]
endnote$endnote2 &amp;lt;- endnote2
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Do-merge:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;mendeley_not_in_endnote &amp;lt;- merge(mendeley,
                                 endnote,
                                 by.y = &quot;endnote2&quot;,
                                 by.x = &quot;mendeley2&quot;,
                                 all.x = T)
str(mendeley_not_in_endnote)
nrow(mendeley_not_in_endnote)
head(mendeley_not_in_endnote)
mendeley_not_in_endnote &amp;lt;- mendeley_not_in_endnote[
                                                      is.na(mendeley_not_in_endnote$V1.y),
                                                      ]
nrow(mendeley_not_in_endnote)
# 92 refs in endnote are not in mendeley
write.csv(mendeley_not_in_endnote,
      &quot;mendeley_not_in_endnote.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Not all these 92 will be true&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;so let;s try the string matching&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;source(&quot;~/Dropbox/tools/levenshtein.r&quot;)
pcnt_threshold &amp;lt;- 0.6
out_list &amp;lt;- matrix(ncol = 3, nrow = 0)
#out_list &amp;lt;- read.csv(&quot;mendeley_not_in_endnote_fz_match.csv&quot;, stringsAsFactors = F)
for(i in 36:nrow(mendeley_not_in_endnote))
    {
        print(i)
#        i = 2
tmp1 &amp;lt;- mendeley_not_in_endnote[i,1]
    for(j in 1:nrow(endnote))
        {
    #        j = 2
    if(exists(&quot;out&quot;)) rm(out)
    tmp2 &amp;lt;- endnote$endnote2[j]
    pcnt &amp;lt;- levenshtein(
            tmp1
            ,
            tmp2
            )
    #pcnt
    if(pcnt &amp;gt;= pcnt_threshold) out &amp;lt;- tmp2
    if(exists(&quot;out&quot;))
        out_list &amp;lt;- rbind(out_list, c(tmp1, tmp2, pcnt))
    if(exists(&quot;out&quot;)) break
        }

    }
out_list
write.csv(out_list, &quot;mendeley_not_in_endnote_fz_match.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R Code: Do-concatenate and lowercase&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(stringr)
out_list &amp;lt;- read.csv(&quot;mendeley_not_in_endnote_fz_match.csv&quot;, stringsAsFactors = F)
mendeley2 &amp;lt;- read.csv(&quot;mendeley_not_in_endnote.csv&quot;, stringsAsFactors=F)
mendeley2[1,]
out_list[1,]
mendeley2 &amp;lt;- merge(mendeley_not_in_endnote, out_list,
                   by.x = &quot;mendeley2&quot;,
                   by.y = &quot;V1&quot;, all.x = T)
mendeley2[2,]
mendeley2 &amp;lt;- mendeley2[is.na(mendeley2$V3),]
nrow(mendeley2)
# 48 records
write.csv(mendeley2, &quot;mendeley_not_in_endnote_best_estimate.csv&quot;, row.names=F)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Results&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I found that XML package in R can work with the Endnote and Mendeley export Files&lt;/li&gt;
&lt;li&gt;I think I made a lot of bad decisions about the way I went about  doing this!&lt;/li&gt;
&lt;li&gt;It seemed quite difficult to get the XML stuff to make sense to me&lt;/li&gt;
&lt;li&gt;I;ve heard that python has better libraries for working with XML&lt;/li&gt;
&lt;li&gt;the levenshtein string matching code proved useful again.  I should get out of the habit of looping and start using lapply etc to speed this up.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;This was an interesting if frustrating experiment&lt;/li&gt;
&lt;li&gt;The minor issues with importing from mendeley/endnote and deduplicating using their own tools was probably not worth writing all this half-baked R code.&lt;/li&gt;
&lt;li&gt;But I did learn more about working with XML in R (and realised this is probably not one of R's Strengths -- or mine for that matter!)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>git-can-be-simple-or-very-complicated</title>
   <link href="http://ivanhanigan.github.com/2013/11/git-can-be-simple-or-very-complicated/"/>
   <updated>2013-11-19T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/git-can-be-simple-or-very-complicated</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;Git is a Distributed Version Control System.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;centerforopenscience.org&quot;&gt;centerforopenscience.org&lt;/a&gt; has developed the Open Science Framework  which is they say &quot;a simplified front end to the powerful and popular version control system Git&quot;.&lt;/li&gt;
&lt;li&gt;I use Github a lot for extending the local features into an online space&lt;/li&gt;
&lt;li&gt;So I finally got around to poking the open science framework with the &lt;a href=&quot;https://openscienceframework.org/project/pyts3/&quot;&gt;Hutchinson Drought Index project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It turns out to be too simplified, and not have very many of the feature I love about Git and GitHub :-(&lt;/li&gt;
&lt;li&gt;For eg it is not really distributed in that you don't get to sync your local repo with the onlin version&lt;/li&gt;
&lt;li&gt;you upload a script or dataset, then you continue editing locally until you want to commit and then you have to upload again, one file at a time with the GUI rather than &quot;git add .&quot; and &quot;git push&quot;.&lt;/li&gt;
&lt;li&gt;I recommend having a look, it might work for you, but if you want more power checkout Yihui's suggestions for using github &lt;a href=&quot;http://yihui.name/en/2011/12/how-to-become-an-efficient-and-collaborative-r-programmer/&quot;&gt;http://yihui.name/en/2011/12/how-to-become-an-efficient-and-collaborative-r-programmer/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and &lt;a href=&quot;http://yihui.name/en/2013/06/fix-typo-in-documentation/&quot;&gt;http://yihui.name/en/2013/06/fix-typo-in-documentation/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;In general I don't think simple front-end's should be a barrier to accessing a sophisticated back-end!&lt;/h4&gt;
</content>
 </entry>
 
 <entry>
   <title>nectar cloud pumilio build got bogged down</title>
   <link href="http://ivanhanigan.github.com/2013/11/nectar-cloud-pumilio-build-got-bogged-down/"/>
   <updated>2013-11-18T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/nectar-cloud-pumilio-build-got-bogged-down</id>
   <content type="html">&lt;p&gt;I've been trying to build pumilio bioacoustics server on a Aust Nectar Research Cloud VM, but hit various roadblocks.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;built with NeCTAR Ubuntu 12.04.2 (Precise)&lt;/li&gt;
&lt;li&gt;these issues (especially the apt-get install r-cran-mysql etc ) might not occure with later Ubuntu?&lt;/li&gt;
&lt;li&gt;test NeCTAR Ubuntu 12.10 (Quantal) or&lt;/li&gt;
&lt;li&gt;NeCTAR Ubuntu 13.04 (Raring) ??&lt;/li&gt;
&lt;li&gt;TODO FIX issues with python audio lab&lt;/li&gt;
&lt;li&gt;TODO swapfile&lt;/li&gt;
&lt;li&gt;TODO Mount the larger storage&lt;/li&gt;
&lt;li&gt;TODO add swapfile&lt;/li&gt;
&lt;li&gt;TODO fix R install etc&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The Github code (in the gh-pages branch) and published as &lt;a href=&quot;http://ivanhanigan.github.io/pumilio-bushfm/&quot;&gt;a report at this link&lt;/a&gt;.  The Source Code is available to Clone or Fork at &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm&quot;&gt;This Github Repo&lt;/a&gt; and is at a stage that most of the installation and configuration is documented to a point where a test sound file has successfully been uploaded.&lt;/p&gt;

&lt;h4&gt;HELP WANTED&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Any researcher at an Australia university could follow the instructions on their own Nectar Cloud VM.&lt;/li&gt;
&lt;li&gt;If anybody out there is interested in bioacoustics, R, Linux or web data archives please help&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Background to pumilio, bushfm and this project&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/birdcombined.png&quot; alt=&quot;birdcombined.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&quot;pumilio%20project&quot;&gt;http://pumilio.sourceforge.net/&lt;/a&gt; is by Luis J. Villanueva-Rivera and Bryan C. Pijanowski. (2012. Pumilio: A Web-Based Management System for Ecological Recordings. Bulletin of the Ecological Society of America 93: 71-81. doi: 10.1890/0012-9623-93.1.71)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;Bush-fm%20project&quot;&gt;http://www.bush.fm/&lt;/a&gt; aims to provide the research community with a portal to national scale acoustic sensor data repositories, and a suite of tools to perform analysis and reporting on these data.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;(Images from http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0000M4)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>really-useful-r-upcase-string</title>
   <link href="http://ivanhanigan.github.com/2013/11/really-useful-r-upcase-string/"/>
   <updated>2013-11-17T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/really-useful-r-upcase-string</id>
   <content type="html">&lt;p&gt;Here is a really useful R snippet from  &lt;a href=&quot;http://stackoverflow.com/a/6364905&quot;&gt;http://stackoverflow.com/a/6364905&lt;/a&gt; with a minor modification to allow differnt splits&lt;/p&gt;

&lt;h4&gt;Code:r-upcase-string&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(&quot;The&quot;, &quot;quick&quot;, &quot;Brown&quot;, &quot;fox/lazy dog&quot;)

simpleCap &amp;lt;- function(x, tosplit = &quot; &quot;) {
  s &amp;lt;- strsplit(x, tosplit)[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep=&quot;&quot;, collapse=tosplit)
}
sapply(x, simpleCap)
sapply(x, simpleCap, tosplit = &quot;/&quot;)
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>pumilio-bushfm-test-dev-prod</title>
   <link href="http://ivanhanigan.github.com/2013/11/pumilio-bushfm-test-dev-prod/"/>
   <updated>2013-11-15T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/pumilio-bushfm-test-dev-prod</id>
   <content type="html">&lt;h1&gt;Testing the pumilio-bushfm-test-dev-prod build process, in an Open Notebook&lt;/h1&gt;

&lt;h4&gt;Aims:&lt;/h4&gt;

&lt;p&gt;It was suggested I could document the pumilio test build as an OpenNotebook.
I imagined that I could link this blog to github repo and doco hosted on gh-pages.&lt;/p&gt;

&lt;h4&gt;Methods:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;For ie the emacs html output now goes to &lt;a href=&quot;http://ivanhanigan.github.io/pumilio-bushfm/&quot;&gt;http://ivanhanigan.github.io/pumilio-bushfm/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;then collaborators can clone/fork &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm&quot;&gt;https://github.com/ivanhanigan/pumilio-bushfm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and comment/complain at &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm/wiki&quot;&gt;https://github.com/ivanhanigan/pumilio-bushfm/wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Results:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I did a test build a month ago on an old laptop sitting around, then rebuilt on the Nectar cloud&lt;/li&gt;
&lt;li&gt;Unfortunately I didn't realise that the Nectar VM had mounted /var on to the smaller root partition (the 40GB 2nd disk is on /mnt).&lt;/li&gt;
&lt;li&gt;then when I tried to upload a big sound file it broke :-(&lt;/li&gt;
&lt;li&gt;I did a bit of reading and whilst I began thinking I'd just need to move the mysql datadir via&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;sudo nano /etc/mysql/my.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;BUT&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;it actually looks like there is a WAV and MP3 file under /var/www/pumilio-2...&lt;/li&gt;
&lt;li&gt;so I think I can &lt;a href=&quot;http://askubuntu.com/questions/39536/how-can-i-store-var-on-a-separate-partition&quot;&gt;just remount /var&lt;/a&gt; onto the larger /mnt secondary disk.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>India Part 1. Society for Natal Effects on Health (SNEHA) Suicide Prevention, Chennai India</title>
   <link href="http://ivanhanigan.github.com/2013/11/india-p1-sneha-suicide-prevention-chennai-india/"/>
   <updated>2013-11-15T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/india-p1-sneha-suicide-prevention-chennai-india</id>
   <content type="html">&lt;h4&gt;Introduction&lt;/h4&gt;

&lt;p&gt;I was grateful to recieve the &lt;a href=&quot;http://students.anu.edu.au/scholarships/gc/cur/dom/bhati.php&quot;&gt;Bhati Family Travel Grant&lt;/a&gt; which allowed me to travel to Chennai, India in October to meet with experts on Drought and Suicide there.  I wanted to go to India to help me with my work on &lt;a href=&quot;http://www.pnas.org/content/109/35/13950&quot;&gt;Farmer Suicide in Australia&lt;/a&gt; primarily because I'd read about the excellent researchers there and also the large magnitude of the problem with farmer suicide and drought (it is in the news media quite regularly).&lt;/p&gt;

&lt;!-- Secondarily, I wanted to come back because I'd visited in 2005 for 3 weeks and was overwhelmed by the experience, possibly we tried to do to much and also we were mostly on the busy tourist track.  I wanted to see if a more local approach might give me a better experience of India.
--&gt;


&lt;p&gt;I arrived on 2nd October and the airport and streets were pretty quiet.  I think because this is a public holiday in honour of Gandhi.&lt;/p&gt;

&lt;p&gt;I had previously arranged to meet&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Dr L Vijayakumar founder of &lt;a href=&quot;http://snehaindia.org/&quot;&gt;Society for Natal Effects on Health - SNEHA&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;Dr A Nambi+, Director of the Climate Change Program and Dr R Rukmani, Director of the Food Security Program at &lt;a href=&quot;http://www.mssrf.org/&quot;&gt;M S Swaminathan Research Foundation - MSSRF&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is my re-written notes of both meetings.&lt;/p&gt;

&lt;h4&gt;First Meeting: 3rd October, Dr Lakshmi Vijayakumar&lt;/h4&gt;

&lt;p&gt;Dr Lakshmi Vijayakumar is a WHO expert, a practicing psychiatrist and founder of the SNEHA suicide prevention organisation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/sneha-ad.jpg&quot; alt=&quot;sneha-ad.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When I contacted her to ask for a meeting she invited me to come to the headquarters of SNEHA in R A Puram, south central Chennai. The Autorickshaw driver took a couple of wrong turns but we got there OK after asking directions.  I also met Jyothi the SNEHA Director and several volunteers who work at the centre and befriend people who visit or phone with suicidal problems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/sneha-cropped.jpg&quot; alt=&quot;sneha-cropped.jpg&quot; /&gt;&lt;/p&gt;

&lt;!-- We met at 12 noon and we also had Dosai for lunch. --&gt;


&lt;!-- With us were Jyothi, Sumitra and a volunteer whose name I did not write down --&gt;


&lt;p&gt;I had asked to visit because I've been following her research on farmer suicide prevention measures.  I suggested that my research on Drought-specific causality might benefit from insights that Dr Vijayakumar could give me, and thankfully she agreed to give me her time. I was very pleased she mentioned that she found my research &quot;interesting and important&quot; which is great because that is just the way I feel about HER research.&lt;/p&gt;

&lt;p&gt;I reflected on my way to the meeting what I want to get out of this lesson and I felt that it was really important to get feedback on some of the directions I want to pursue in terms of a unified theoretical framework across environmental and social causes of suicide (generalised across diverse social situations like the differences between Australian and Indian Farmers).&lt;/p&gt;

&lt;p&gt;I also felt it was important that I might just sit and listen to any advice Dr Vijayakumar felt like giving me, so I asked what she thought were the most important priorities for farmer suicide research in India.
I have been reflecting on the influence of Dr Vijayakumar's words had on me when I heard them on a Radio report by Australian Journalist Michael Condon &lt;a href=&quot;www.google.com&quot;&gt;Ref&lt;/a&gt; when he came to India in 2009.&lt;/p&gt;

&lt;!-- TODO expand this. --&gt;


&lt;h4&gt;My Secret Agenda&lt;/h4&gt;

&lt;p&gt;I brought along a secret agenda which was to get an opinion of my preconceived thoughts about the topic.  Particularly the approach to a generalised framework of the work of my PhD supervisors Colin, Phil and Rohan for disentangling environmental from social impacts on health.  Especially this framework is for understanding tipping points in the farmer suicide system.  My second secret agenda was that if the framework I presented was thought OK, then what were the best next steps (plan for the future), in terms of communication in suicide prevention fields, actions in policy arenas and etc.&lt;/p&gt;

&lt;p&gt;First: I presented the general framework.  The essence of this is the &quot;Five Capitals&quot;:
- Financial
- Social
- Physical
- Natural
- Human&lt;/p&gt;

&lt;p&gt;Phil and Rohan have developed approaches aimed at Rural Livelihoods and Adaptive Capacity of Farmers.  Colin had extended this even to the concept of Human Carrying Capacity.  I think this framework especially useful for tipping points because the causes of such catastrophes are undoubtedly numerous, interacting and dynamic.&lt;/p&gt;

&lt;p&gt;So what did Lakshmi think?  She started by saying that it made good sense, that the main focus of suicide prevention work was focused on the biological, psychological and social domains.  But there are so many other factors.  Drought was clearly one.&lt;/p&gt;

&lt;p&gt;She got out a map of the recent suicide rates and showed me a region of elevated risk north-west of Tamil Nadu, south of Chatinargarth, and north of Kerala.  60 percent of India's farmer suicides are in these states.  Why are these 3 states high but TN is not?  All are semi-arid.&lt;/p&gt;

&lt;!-- TODO check this --&gt;


&lt;p&gt;All have similar societies, cultures, climates, agriculture, etc.  We tried fitting these to the five capitals framework.&lt;/p&gt;

&lt;!--
#### What do you think?
Dr Lakshmi said that this kind of thing did help to develop policy &quot;definitely&quot;.
I asked how much extra help this gave if we knew that pesticides, alcohol and gun control were so important.  She replied that those were medium term policy solutions but long-term policies needed to look at social dynamics, farming and a whole lot of things like status change (owning land used to be a status symbol but now is a burden).

#### What are the priorities?
What secondary (surrogate) priorities emerge around the main points?  If I bring questions to the lesson are they answered by the presenter?  If not why?

- Synthesis:
My new understanding of the topic.

- TODO Mind map of the pathway to completed suicide.
--&gt;


&lt;p&gt;&lt;/p&gt;

&lt;!-- &lt;ul&gt; --&gt;


&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 dev-table&lt;/a&gt;&lt;/li&gt; --&gt;


&lt;!-- &lt;/ul&gt; --&gt;


&lt;!-- &lt;/div&gt; --&gt;


&lt;!-- &lt;/div&gt; --&gt;




&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; dev-table&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;

&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;left&quot; /&gt;&lt;col class=&quot;left&quot; /&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;&lt;th scope=&quot;col&quot; class=&quot;left&quot;&gt;Capital&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;left&quot;&gt;Semi-arid suicide hotspot&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;left&quot;&gt;Tamil Nadu&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Financial&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;move from rice/millet/polyculture&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;remained polyculture&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;into cashcrops, cotton, chilli&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;with more pesticide, decreased access&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;to local money lender and more reliance&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;on international corporations&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;retained local lending,&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;more responsive to seasonal&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;conditions&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!-- + Unfortunately Dr Nambi was out of action with a back injury but I was very lucky to have made an alternative contact in Dr R Rukmani, Director of the Food Security Program at MSSRF who had also researched Farmer Suicides.
--&gt;

</content>
 </entry>
 
 <entry>
   <title>What Do Scientists Who Write Metadata Use To Do It? And Why?</title>
   <link href="http://ivanhanigan.github.com/2013/11/what-do-scientists-who-write-metadata-use-to-do-it-and-why/"/>
   <updated>2013-11-06T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/what-do-scientists-who-write-metadata-use-to-do-it-and-why</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;The extent to which scientists write metadata is probably lower than it ought to be&lt;/li&gt;
&lt;li&gt;The level of metadata written during science projects is probably described generally as 'bare-minimum' and &quot;the minimum needed for one-self to come back to and understand what one did&quot;&lt;/li&gt;
&lt;li&gt;It sometimes seems that even the bare minimum for one-self is not being kept very often&lt;/li&gt;
&lt;li&gt;I argue that the reasons for less-than-adequate metadata can be understood by looking at&lt;/li&gt;
&lt;li&gt;1) the culture of the scienctists displinary background via training&lt;/li&gt;
&lt;li&gt;2) the tools available and&lt;/li&gt;
&lt;li&gt;3) institutional  requirements to produce metadata (both about data or access to data)&lt;/li&gt;
&lt;li&gt;In my ongoing &lt;a href=&quot;http://ivanhanigan.github.io/2013/10/two-main-types-of-data-documentation-workflow/&quot;&gt;series of blog posts I am exploring the tools available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In this post I just wanted to start the discussion about discipline culture and institutional requirements.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Discipline Culture&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I trained in Geography in the age of GIS and this community uses metadata a lot&lt;/li&gt;
&lt;li&gt;Due to the prevalance of the digital map (collection of layers) which is a derivative data output&lt;/li&gt;
&lt;li&gt;Need to know the source of all the layers&lt;/li&gt;
&lt;li&gt;first law of GIS is &quot;garbage in, garbage out&quot;&lt;/li&gt;
&lt;li&gt;I was trained in the ANSLIC standard from the start&lt;/li&gt;
&lt;li&gt;ArcGIS has a tool called ArcCatalog which makes metadata easy to create and view&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Institutional Requirements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The ARC and NHMRC say they are going to require more metadata (and even data deposit)&lt;/li&gt;
&lt;li&gt;Restrictions on data access make it necessary to describe at least the metadata around provision agreements, licence, allowable access&lt;/li&gt;
&lt;li&gt;A supporting management level who value the metadata as research output (alongside a peer reviewed paper metadata pales in comparison)&lt;/li&gt;
&lt;li&gt;My old boss used to say &quot;Work Not Published Is Work Not Done&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;This reminds me of Approaches and Barriers to Reproducible Research&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;In 2011 BiostatMatt (Matt Shotwell) published &lt;a href=&quot;http://biostatmatt.com/uploads/shotwell-interface-2011.pdf&quot;&gt;a survey of biostatisticians&lt;/a&gt;
VUMC Dept. of Biostatistics to assess:&lt;/li&gt;
&lt;li&gt;the prevalence of fully scripted data analyses&lt;/li&gt;
&lt;li&gt;the prevalence of literate programming practices&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To assess the perceived barriers to reproducible research the also asked:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;What The biggest obstacle to always reproducibly scripting your work?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;pre&gt;&lt;code&gt;| Barrier                                                  | Staff | Faculty |
|----------------------------------------------------------+-------+---------|
| No signifcant obstacles.                                 |     8 |      10 |
| I havent learned how.                                    |     0 |       0 |
| It takes more time.                                      |     7 |       7 |
| It makes collaboration difficult (eg. file compatibility)|     4 |       2 |
| The software I use doesnt facilitate reproducibility.    |     0 |       0 |
| Its not always necessary for my work to be reproducible. |     2 |       0 |
| Other                                                    |     2 |       1 |
|----------------------------------------------------------+-------+---------|
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;So what about the Approaches and Barriers to Me Writing Metadata?&lt;/h3&gt;

&lt;p&gt;With a sample size of one I asked myself these questions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;| Q                                                  | A                                                                    |
|----------------------------------------------------+----------------------------------------------------------------------|
| Do I fully document data (to a metadata standard?) | Occasionally, using DDI for high value raw inputs and final products |
| Do I employ data documentation practices           | I use a tool I created to write minimal metadata occasionally        |
| What are the main barriers?                        | takes more time, The software doesnt facilitate, not always necessary|
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The tools need to help write metadata

&lt;ul&gt;
&lt;li&gt;the Institution needs to require metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Shotwell, M.S. and Alvarez, J.M. 2011. Approaches and Barriers to Reproducible Practices in Biostatistics.
http://biostatmatt.com/uploads/shotwell-interface-2011.pdf&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>librarians-and-python</title>
   <link href="http://ivanhanigan.github.com/2013/11/librarians-and-python/"/>
   <updated>2013-11-06T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/librarians-and-python</id>
   <content type="html">&lt;p&gt;I stumbled on these posts about python and IPython Notebook by &quot;Data Scientist Training for Librarians&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://altbibl.io/dst4l/exploratory-data-analysis-and-statistics-using-pandas-and-matplotlib/&quot;&gt;http://altbibl.io/dst4l/exploratory-data-analysis-and-statistics-using-pandas-and-matplotlib/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://altbibl.io/dst4l/pandas-munging-stats-and-visualization/&quot;&gt;http://altbibl.io/dst4l/pandas-munging-stats-and-visualization/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I;ve been wanting to learn more python.  I don't think it'll be ready for statistical modelling for a while, but I a want to be ready when it is.  You can get my ipython notebook file for this here: &lt;a href=&quot;/data/olive.ipynb&quot;&gt;olive.ipynb&lt;/a&gt;, but first run the following R code snippet to get the replication dataset 'olive.csv' into your home directory.&lt;/p&gt;

&lt;h4&gt;R Code: get the olive oil dataset&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&quot;pgmm&quot;)
require(pgmm)
data(olive)
names(olive) &amp;lt;- tolower(names(olive))
str(olive)
write.csv(olive, &quot;olive.csv&quot;, row.names = F)
# actualy home might be easier for ipython to access
write.csv(olive, &quot;~/olive.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;OK now reproduce the example&lt;/h3&gt;

&lt;p&gt;I quite like the histograms from the second example.  Here is the raw code extracted from the ipynb&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;%pylab inline
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.colors as colors

acidlist=['palmitic', 'palmitoleic', 'stearic', 'oleic', 'linoleic', 'linolenic', 'arachidic', 'eicosenoic']
dfsub=df[acidlist].apply(lambda x: x/100.0)
dfsub.head()

rkeys=[1,2,3]
rvals=['South','Sardinia','North']
rmap={e[0]:e[1] for e in zip(rkeys,rvals)}
rmap

fig, axes=plt.subplots(figsize=(10,20), nrows=len(acidlist), ncols=1) # sets up the framework for the graphs. Acidlist is defined elsewhere, and is a list of the acids we’re interested in.
i=0 # Sets a counter to 0

for ax in axes.flatten(): # Starts a loop to go through our plot and render each row

    acid=acidlist[i]
    seriesacid=df[acid] # creates seriesacid and sets it to df[acid], a list of the percent composition of the acid in the current iteration that’s in each olive oil.

    minmax=[seriesacid.min(), seriesacid.max()] # the minimum and maximum values plotted will be the minimum and maximum percentages that we find in the data

    for k,g in df.groupby('region'):  # starts a loop in the loop to plot the values by region
        style = {'histtype':'stepfilled', 'alpha':0.5, 'label':rmap[k], 'ax':ax}
        g[acid].hist(**style)

        ax.set_xlim(minmax)

        ax.set_title(acid)

        ax.grid(False)

        #construct legend
        ax.legend()
    i=i+1 # increments the counter, to move the loop on to the next acid.

    fig.tight_layout()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/acid.png&quot; alt=&quot;acid.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I am not sure how to do the transparency but the rest of it would make more sense to me with R&lt;/li&gt;
&lt;li&gt;Will try to reproduce in R for head-to-head shoot out.&lt;/li&gt;
&lt;li&gt;I also plan to look into the new &lt;a href=&quot;http://trestletechnology.net/2013/11/ace-code-editor-in-shiny-shinyace/&quot;&gt;shinyAce browser based R editor&lt;/a&gt; for similar work&lt;/li&gt;
&lt;li&gt;I'm still really enjoying Emacs Orgmode for this kind of functionality though and thoroughly recommend &lt;a href=&quot;http://kieranhealy.org/resources/emacs-starter-kit.html&quot;&gt;KJ Healy's starter kit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(I've installed and config thison Ubuntu, Windoze and Mac, but LaTeX and exporting R code can be tricky)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>handling-survey-data-with-r</title>
   <link href="http://ivanhanigan.github.com/2013/11/handling-survey-data-with-r/"/>
   <updated>2013-11-06T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/handling-survey-data-with-r</id>
   <content type="html">&lt;p&gt;R is generally very good for handling many different data types but&lt;/p&gt;

&lt;h3&gt;R has problems with survey data&lt;/h3&gt;

&lt;p&gt;This post is a stub about what packages Ive found with methods allowing to handle efficiently survey data: handle variable labels, values labels, and retrieve information about missing values&lt;/p&gt;

&lt;h4&gt;Base R:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;## Not run:
require(foreign)
analyte  &amp;lt;- read.spss(filename, to.data.frame=T) 
varslist &amp;lt;- as.data.frame(attributes(analyte)$variable.labels)
# this gives a pretty useful thing to use
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;While I was digging around in &lt;a href=&quot;http://mephisto.unige.ch/traminer&quot;&gt;TraMineR&lt;/a&gt; I found this link to Dataset, Emmanuel Rousseaux's package for handling, documenting and describing data sets of survey data.&lt;/p&gt;

&lt;h4&gt;Code:Dataset, a package for handling-survey-data-with-r&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;if(!require(Dataset)) install.packages(&quot;Dataset&quot;, repos=&quot;http://R-Forge.R-project.org&quot;);
require(Dataset)
data(dds)
str(dds)
# cool
description(dds$sexe)
# excellent!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;I'm sure there are plenty of other approaches.  I'll add them as I find them'&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Github And Reproducible Research Report Casestudy Hutchinson Drought Index</title>
   <link href="http://ivanhanigan.github.com/2013/11/github-and-reproducible-research-report-casestudy-hutchinson-drought-index/"/>
   <updated>2013-11-06T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/github-and-reproducible-research-report-casestudy-hutchinson-drought-index</id>
   <content type="html">&lt;h3&gt;Background&lt;/h3&gt;

&lt;p&gt;This is an example of using github for a Reproducible Research Report (RRR) using a casestudy of the  Hutchinson Drought Index.  The project is available under GNU licence at &lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex&quot;&gt;https://github.com/ivanhanigan/HutchinsonDroughtIndex&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Aims&lt;/h3&gt;

&lt;p&gt;This example will clone the repository from github and run the replication codes using the replication datasets.&lt;/p&gt;

&lt;h3&gt;Materials and Methods&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;assumes you are connected to the internet&lt;/li&gt;
&lt;li&gt;assumes you have git, a github account and ssh key set up&lt;/li&gt;
&lt;li&gt;assumes you have R and GDAL configured&lt;/li&gt;
&lt;li&gt;assumes BoM and ABS have kept their data in the locations I specified to download from&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Hutchinsons Drought Index&lt;/h3&gt;

&lt;p&gt;The Hutchinson Drought Index (or Drought Severity Index) is a climatic drought index
that was designed to reflect agricultural droughts using only rainfall data.&lt;/p&gt;

&lt;p&gt;The index was invented by Professor Mike Hutchinson at the ANU in 1992 (1) and
this project includes R codes written to describe the calculations
and also to download data (2,3) to play with.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;h4&gt;Replication Codes to run in the terminal&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:ivanhanigan/HutchinsonDroughtIndex.git ~/data/HutchinsonDroughtIndex
R
setwd(&quot;~/data/HutchinsonDroughtIndex&quot;)
source(&quot;HutchinsonDroughtIndex_go.r&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;WARNING this downloads 5.4MB from BoM and 19.9MB from ABS&lt;/li&gt;
&lt;li&gt;This runs the codes and produces graphs&lt;/li&gt;
&lt;li&gt;an alternative workflow would be to run the sweave file in the reports directory but I dont like that method as much and have set all the evaluation commands to false.&lt;/li&gt;
&lt;li&gt;it will run now that we have the graphs though, to create a report:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;setwd(&quot;reports&quot;)
Sweave(&quot;HutchinsonDroughtIndex_transformations_doc.Rnw&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;which creates a tex file that can create &lt;a href=&quot;/pdfs/HutchinsonDroughtIndex_transformations_doc.pdf&quot;&gt;this pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;but the simplest example I gave above will compute the index and create the graphs (one is shown below)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/CentralWestDrought8283.png&quot; alt=&quot;CentralWestDrought8283.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;So what actually happened?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;This project adheres to the &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;Reichian LCFD model of writing R code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In this the code is bundled into modules Func (tools), Load, Do, Clean (check).  These live in the src directory (following teh White ProjectTemplate model - well almost, He would put tools/func into lib)&lt;/li&gt;
&lt;li&gt;then a main.r (or go.r) script calls these modules&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;go.r&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt; source('src/HutchinsonDroughtIndex_tools.r')
 source('src/HutchinsonDroughtIndex_load.r')
 source('src/HutchinsonDroughtIndex_do.r')
 source('src/HutchinsonDroughtIndex_check.r')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The really interesting bit is &lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex/blob/master/src/HutchinsonDroughtIndex_tools_droughtIndex.r&quot;&gt;a tool written for this project&lt;/a&gt; that is called from within tools.r&lt;/p&gt;

&lt;p&gt;   source('src/HutchinsonDroughtIndex_tools_droughtIndex.r')&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;A keen reader would look in that script to find out exactly what the function does&lt;/li&gt;
&lt;li&gt;A keen author would push that function to an R package (a super keen bean would publish this on CRAN)&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Discussion&lt;/h3&gt;

&lt;h4&gt;Strengths:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;full codes are provided to reproduce the data access, manipulation and analysis&lt;/li&gt;
&lt;li&gt;the gory details are hidden from the casual user&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Weaknesses:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;the important details are hidden from the technical user&lt;/li&gt;
&lt;li&gt;the script depends on a bunch of things that might not be true&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;This is an example of a self-contained RRR&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Smith, D. I, Hutchinson, M. F, &amp;amp; McArthur, R. J. (1992) Climatic and
Agricultural Drought: Payments and Policy. (Centre for Resource and Environmental
Studies, Australian National University, Canberra, Australia).&lt;br/&gt;
http://fennerschool-research.anu.edu.au/spatio-temporal/publications/cres_paper1992.pdf&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bureau of Meteorology High Quality Monthly precipitation data
downloaded on 2012-01-09
from ftp://ftp.bom.gov.au/anon/home/ncc/www/change/HQmonthlyR/HQ_monthly_prcp_txt.tar&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Australian Bureau of Statistics Statistical Divisions 2006
downloaded on 2012-01-09
from http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1259.0.30.0022006?OpenDocument&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Financial support was provided by Professor Tony McMichael's
&quot;Australia Fellowship&quot; from the the National Health and Medical Research Council, via the
National Centre for Epidemiology and Population Health, Australian National University.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>climatic-drought-guestpost-by-luciana-porfirio</title>
   <link href="http://ivanhanigan.github.com/2013/11/climatic-drought-guestpost-by-luciana-porfirio/"/>
   <updated>2013-11-01T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/11/climatic-drought-guestpost-by-luciana-porfirio</id>
   <content type="html">&lt;h3&gt;Guest post by Luciana Porfirio with code contributions by Francis Markham&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Recently I encountered a student doing an analysis who was doing everything in excel, and I couldn't contain my mouth and say R would be better... but it isn't a simple task. So any here are some tips.&lt;/li&gt;
&lt;li&gt;There is a table with 62 years * 12 months of rain data in mm. We calculated the cumulative distribution using ecdf: empirical cumulative distribution function. So the table looks like this: Year Jan Feb Mach (...) each cell contains the cum dist.&lt;/li&gt;
&lt;li&gt;We also got the number of months per year with less than X mm of rain. But he needs to know how many months  are between the drought months, regardless the year. So, I thought that converting the data into a ts was going to facilitate the task, but it doesn't, because now I don't' have columns and rows any longer.&lt;/li&gt;
&lt;li&gt;But I struggled to handle a ts object to do for example an ifelse.&lt;/li&gt;
&lt;li&gt;Luckily there are many many tricks about R, and with the code below we solved all his problems (and many months of work in excel).&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/data/raj_rain_data.csv&quot;&gt;Get the example data here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is how it looks like:&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;###############################################
require(reshape2)

###############################################
#read csv file with rain BoM data
dat = read.csv('raj_rain_data.csv')

fn &amp;lt;- apply(dat[,2:13], 2, ecdf) # equivalent to Excel's percentrank function, only for cols 2 to 13 but you need to apply the function to each month

#this fun does all the months at once
fn2 = data.frame(t(do.call(&quot;rbind&quot;, sapply(1:12, FUN = function(i) fn[[i]](dat[,i+1]), simplify = FALSE))))
colnames(fn2) = colnames(dat[,2:13])
fn2$year = dat$Year

#if the value is lower than 0.4, retrieves a 1, otherwise 0
fn2$drought = rowSums(ifelse(apply(fn2[,1:12], 2, FUN= function (x) x &amp;lt; 0.4)==TRUE, 1,0))

#if the value is lower than 0.1, retrieves a 1, otherwise 0
fn2$extreme = rowSums(ifelse(apply(fn2[,1:12], 2, FUN= function (x) x &amp;lt; 0.1)==TRUE, 1,0))

str(fn2)
names(fn2)

#ts didn't work - Francis suggested to melt the data.frame # Melt
#fn2 to tall data set
fn2.tall &amp;lt;- melt(fn2, id.vars=&quot;year&quot;)

# Get the year-month into date format
fn2.tall$date &amp;lt;- with(fn2.tall,
                             as.Date(paste(&quot;1&quot;, variable, year), &quot;%d %b %Y&quot;))

# Convert dates into months since 0 BC/AD (arbitrary, but doesn't
# matter)
#I'm not using the month.idx with Ivan's solution (remove)
fn2.tall$mnth.idx &amp;lt;- sapply(fn2.tall$date, function(x){
  12*as.integer(format(x, &quot;%Y&quot;)) + (as.integer(format(x, &quot;%m&quot;)) - 1)
})

# Sort by date
fn2.tall &amp;lt;- fn2.tall[order(fn2.tall$date),]


#Ivan's solution

x&amp;lt;-fn2.tall$value&amp;lt;0.4
xx &amp;lt;- (cumsum(!x) + 1) * x
x2&amp;lt;-(seq_along(x) - match(xx, xx) + 1) * x
fn2.tall$count&amp;lt;-x2

#counts the number of cases of drought
as.data.frame(table(fn2.tall$count))
###############################################
###############################################
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>quantum-gis-visualisations</title>
   <link href="http://ivanhanigan.github.com/2013/10/quantum-gis-visualisations/"/>
   <updated>2013-10-31T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/quantum-gis-visualisations</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;This is a quick post on Quantum GIS for spatial data visualisation&lt;/li&gt;
&lt;li&gt;it is also a follow up on &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/06/test-gislibrary/&quot;&gt;this post about area concordance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quantum GIS is getting pretty good but is still a bit tricky to make good looking maps&lt;/li&gt;
&lt;li&gt;QGIS can use &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/04/quantumgis-and-postgis/&quot;&gt;remote PostGIS geodatabases on the Cloud as the backend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code: use postgis to create area-concordance&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(devtools)
install_github(&quot;gisviz&quot;, &quot;ivanhanigan&quot;)
require(gisviz)
require(swishdbtools)
ch &amp;lt;- connect2postgres2(&quot;gislibrary&quot;)
# make a temporary tablename, to avoid clobbering
temp_table &amp;lt;- swish_temptable(&quot;gislibrary&quot;)
temp_table &amp;lt;- paste(&quot;public&quot;, temp_table$table, sep = &quot;.&quot;)
temp_table
# this is going to be public.foo11c7673416ea

sql &amp;lt;- postgis_concordance(conn = ch, source_table = &quot;abs_sla.nswsla91&quot;,
   source_zones_code = 'sla_id', target_table = &quot;abs_sla.nswsla01&quot;,
   target_zones_code = &quot;sla_code&quot;,
   into = temp_table, tolerance = 0.01,
   subset_target_table = &quot;cast(sla_code as text) like '105%'&quot;, 
   eval = F) 
cat(sql)
dbSendQuery(ch, sql)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;now connect to PostGIS using QGIS &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/2013/04/quantumgis-and-postgis/&quot;&gt;as described in this tute&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and add the layer to the map&lt;/li&gt;
&lt;li&gt;Style it how you like, I also added NSWSLA1991 over the top&lt;/li&gt;
&lt;li&gt;go into the &quot;new print composer&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/qgis-new-print-composer.png&quot; alt=&quot;qgis-new-print-composer.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/qgis-add-new-map.png&quot; alt=&quot;qgis-add-new-map.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;hit the export to image and viola&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/qgis-export-image.png&quot; alt=&quot;qgis-export-image.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Don't forget to clean up the database!&lt;/h3&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;dbSendQuery(ch, sprintf(&quot;drop table %s&quot;, temp_table))
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>notes-on-spatial-stats-meeting-with-sarunya-sujaritpong</title>
   <link href="http://ivanhanigan.github.com/2013/10/notes-on-spatial-stats-meeting-with-sarunya-sujaritpong/"/>
   <updated>2013-10-31T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/notes-on-spatial-stats-meeting-with-sarunya-sujaritpong</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;Yesterday I met with Sarunya Sujaritpong a PhD student working with &lt;a href=&quot;http://ivanhanigan.github.io/2013/10/spatially-structured-time-series-with-nmmaps/&quot;&gt;spatially structured time-series models as described previously&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Her supervisor Keith Dear has given me a lot of good stats advice in the past and one bit I keep thinking about is that a time series model can be fit for multiple spatial areal units of the same city and residual spatial auto-correlation in the errors should not be too much of a concern&lt;/li&gt;
&lt;li&gt;The problem would be if you get strong spatial autocorrelation of the residuals this indicates that the assumption of independent errors is violated and you will have tighter confidence intervals around the coefficients of interest than is really the case, inflating the signficance estimated for the relative risk&lt;/li&gt;
&lt;li&gt;The beta coefficient itself shouldn't be affected.&lt;/li&gt;
&lt;li&gt;So long as biostatisticians like Keith are comfortable with not addressing this issue in spatially structured time-series that is great but I see people are &lt;a href=&quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0043360&quot;&gt;starting to include this in their models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To date I've mostly seen it done in spatial (cross sectional) data analysis, not spatial times-series&lt;/li&gt;
&lt;li&gt;I'm preparing for the day when I might need to address this for a reviewer and would like to know what to do about it in case that happens&lt;/li&gt;
&lt;li&gt;So I asked Sarunya for a discussion about her research&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Sarunya's model is essentially like this&lt;/h3&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;fit &amp;lt;- glm(deaths ~ pollutant1 + pollutant2 + pollutant ... +
       ns(temp, df) + ns(humidity, df) +
       ns(time, df = 7*numYears) +
       SLA * ns(time, df = 2),
       data = analyte, family = poisson
       )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;SLA is Statistical Local Area (now called SA2, like a suburb)&lt;/li&gt;
&lt;li&gt;Sarunya explained that the research question was if the magnitude of the coeff on pollutant1 differed between this model and the old style of model where an entire city is used as the unit of analysis per day and exposure estimates are calculated as averages across several monitoring stations in the city&lt;/li&gt;
&lt;li&gt;turns out that this comparison is still valid EVEN IF THE STANDARD ERROR IS BIASED DUE TO RESIDUAL SPATIAL AUTOCORRELATION&lt;/li&gt;
&lt;li&gt;therefore this study avoids the issue by it's intentional design to compare betas not se&lt;/li&gt;
&lt;li&gt;Interestingly Sarunya explained that the stats theory suggests that even if the exposure precision is increased (exposure misclassification bias is decreased) the se on the beta will not be affected.&lt;/li&gt;
&lt;li&gt;this is fascinating in itself, but a separate issue for another post&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;So it looks like the extent a study might need to consider the issue of potential residual spatial autocorrelation depends alot on what questions are being asked and what inferences will be attempted from the results&lt;/li&gt;
&lt;li&gt;if the aim of the study is to estimate the magnitude AND CONFIDENCE INTERVALS of an exposure's relative risk (especially some novel exposure such as interstellar space dust across the suburbs) then the issue might become important to address.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;THANKS Sarunya!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>if-disease-incidence-varies-with-age-control-for-it</title>
   <link href="http://ivanhanigan.github.com/2013/10/if-disease-incidence-varies-with-age-control-for-it/"/>
   <updated>2013-10-31T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/if-disease-incidence-varies-with-age-control-for-it</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;today I was in a meeting where the discussion turned to maps of crude incidence rates, age-standardised rates and regression models controlling for age&lt;/li&gt;
&lt;li&gt;If disease incidence does not vary with age then there is not much point in controlling for this,&lt;/li&gt;
&lt;li&gt;apart from the work done in exploratory data analysis to ascertain whether or not this is the case&lt;/li&gt;
&lt;li&gt;I proposed that if you've done all the work on age-specific rates needed to test if incidence varies with age, then you might as well present age adjusted rates seeing as you've already done the work anyway&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;An example&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The project is highly confidential due to the nature of the data&lt;/li&gt;
&lt;li&gt;We;ll call the study location South Kingsland to protect the identity&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# get spatial data
require(swishdbtools) # github package, under
                      # swish-climate-impact-assessment

Tab1 &amp;lt;- read.csv(&quot;data/dataset.csv&quot;, sep= &quot;,&quot;, header = TRUE, stringsAsFactor = F)
Tab1 &amp;lt;- as.data.frame(table(Tab1$SLA))
nrow(Tab1)
write.csv(Tab1, &quot;data/SLA.csv&quot;, row.names = F)
load2postgres(&quot;data/SLA.csv&quot;, schema=&quot;ivan_hanigan&quot;, tablename=&quot;sla&quot;, ip = &quot;brawn.anu.edu.au&quot;,
              db = &quot;gislibrary&quot;, pguser = &quot;ivan_hanigan&quot;, printcopy = F)


ch  &amp;lt;- connect2postgres2(&quot;gislibrary&quot;)
sql_subset(ch, &quot;ivan_hanigan.sla&quot;, eval = T, limit = 10)


sql_subset(ch, &quot;abs_sla.qldsla01&quot;, select =  &quot;sla_name, st_y(st_centroid(geom))&quot;, eval = T, limit = 10)
dbSendQuery(ch,
  &quot;select sla_name, geom
  into southkingsland
  from abs_sla.qldsla01 t1
  join
  sla t2
  on t1.sla_name = t2.var1;
  alter table southkingsland add column gid serial primary key;
  &quot;)
# these are missing from spatial data
## 42                      West End                          &amp;lt;NA&amp;gt;
## 43                OVERSEAS-OTHER                          &amp;lt;NA&amp;gt;
## 44                  Yarrabah (S)                          &amp;lt;NA&amp;gt;
## 45     Rowes Bay-Belgian gardens                          &amp;lt;NA&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;visualise with QGIS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/southkingsland.png&quot; alt=&quot;southkingsland.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
require(plyr)

# load
Tab2 &amp;lt;- read.csv(&quot;data/dataset.csv&quot;, sep= &quot;,&quot;, header = TRUE, stringsAsFactor = F)
names(Tab2)
# [1] &quot;SLA&quot;       &quot;agegrp&quot;    &quot;Sex&quot;       &quot;district&quot;  &quot;dm&quot;        &quot;ninf&quot;     
# [7] &quot;popSLA&quot;    &quot;Month&quot;     &quot;Year&quot;      &quot;groupyear&quot;
test_sla  &amp;lt;- names(table(Tab2$SLA)[1])
subset(Tab2, SLA == test_sla &amp;amp; Year == 2001 &amp;amp; agegrp == 1 &amp;amp; Sex == &quot;MALE&quot;)
subset(Tab2, SLA == test_sla &amp;amp; Year == 2001 &amp;amp; agegrp == 1 &amp;amp; Sex == &quot;FEMALE&quot;)

# clean
summary(Tab2)
## remove any SLA with NA populations
Tab2 &amp;lt;- Tab2[which(!is.na(Tab2$popSLA)),]     

# do
## first total cases by annualised pop
analyte  &amp;lt;- ddply(Tab2, c(&quot;SLA&quot;, &quot;Year&quot;, &quot;agegrp&quot;, &quot;Sex&quot;), summarise,
                  ninf=sum(ninf),
                  popSLA=mean(popSLA)
                  )
#str(analyte)

#subset(analyte, SLA == test_sla)
subset(analyte, SLA == test_sla &amp;amp; Year == 2001 &amp;amp; agegrp == 1)

## now annual totals for study region
analyte2  &amp;lt;- ddply(analyte, c(&quot;Year&quot;, &quot;agegrp&quot;), summarise,
                  ninf=sum(ninf),
                  popSLA=sum(popSLA)
                  )
str(analyte2)

qc &amp;lt;- subset(analyte2,  Year == 2001)
qc
sum(qc$popSLA)

## now totals
qc  &amp;lt;- ddply(analyte2, c(&quot;Year&quot;), summarise,
             ninf=sum(ninf),
             popSLA=sum(popSLA)
             )
qc
sum(qc$ninf)
mean(qc$popSLA)

## totals by age
subset(analyte2,  agegrp == 1)
analyte3  &amp;lt;- ddply(analyte2, c(&quot;agegrp&quot;), summarise,
                  ninf=sum(ninf),
                  popSLA=mean(popSLA)
                  )
analyte3
sum(analyte3$popSLA)
analyte3$asr  &amp;lt;- (analyte3$ninf / analyte3$popSLA) * 1000
analyte3

png(&quot;graphs/south-kingsland-age-specific-rates.png&quot;) 
mp &amp;lt;- barplot(analyte3$asr, names.arg = analyte3$agegrp)
box()
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h3&gt;Plot Age Specific Rates&lt;/h3&gt;

&lt;p&gt;Great so with that bit of work we should have an idea of the variation of incidence by age&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/south-kingsland-age-specific-rates.png&quot; alt=&quot;south-kingsland-age-specific-rates.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Results and Discussion&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;It looks like the incidence does vary with age&lt;/li&gt;
&lt;li&gt;TODO make graphs per year (the scale of the above graph is wrong - decades in the numerator but annualised pops in the denominator)&lt;/li&gt;
&lt;li&gt;TODO make graphs by city, by sex&lt;/li&gt;
&lt;li&gt;age-standardised rates probably need different age categories, only 4 or 5?&lt;/li&gt;
&lt;li&gt;worth commenting on the liklihood that age is not important for time-series models at the city scale, unless age structure changes substantially over time (this is an analysis for the spatial pattern only)&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Clean up database!&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;dbSendQuery(ch, &quot;drop table sla&quot;)
dbSendQuery(ch, &quot;drop table southkingsland&quot;)
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>morpho-and-reml-boilerplate-streamline-the-process-of-metadata-entry</title>
   <link href="http://ivanhanigan.github.com/2013/10/morpho-and-reml-streamline-the-process-of-metadata-entry/"/>
   <updated>2013-10-29T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/morpho-and-reml-streamline-the-process-of-metadata-entry</id>
   <content type="html">&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Disentangle Things&lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 morpho-and-reml-boilerplate-streamline-the-process-of-metadata-entry&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Speed and Rigour&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 Analysts can often trade-off completeness of documentation for speed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4&quot;&gt;1.4 Librarians produce gold plated documentation and can take longer to produce this&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5&quot;&gt;1.5 An example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6&quot;&gt;1.6 Embracing Inaccuracy and Incompleteness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-7&quot;&gt;1.7 Aim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-8&quot;&gt;1.8 Step 1: load a simple example dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-9&quot;&gt;1.9 Step 2 create a function to deliver the minimal metadata object&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-10&quot;&gt;1.10 reml&lt;sub&gt;boilerplate&lt;/sub&gt;-code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-11&quot;&gt;1.11 reml&lt;sub&gt;boilerplate&lt;/sub&gt;-test-code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-12&quot;&gt;1.12 Results: This loads into Morpho with some errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-13&quot;&gt;1.13 Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; morpho-and-reml-boilerplate-streamline-the-process-of-metadata-entry&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1&lt;/span&gt; Background&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1&quot;&gt;


&lt;ul&gt;
&lt;li&gt;The Morpho/Metacat system is great for a data repository
&lt;/li&gt;
&lt;li&gt;Morpho also claims to be suitable for Ecologists to document their data
&lt;/li&gt;
&lt;li&gt;But in my experience it leaves a little to be desired in ease of use for both purposes
&lt;/li&gt;
&lt;li&gt;Specifically the speed that documentation can be entered into Morpho is slow
&lt;/li&gt;
&lt;li&gt;This post is a first attempt to create some boilerplate code to quickly generate EML metadata using REML.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2&lt;/span&gt; Speed and Rigour&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2&quot;&gt;

&lt;p&gt;As I noted in a previous post, there are [two types of data documentation workflow](&lt;a href=&quot;http://ivanhanigan.github.io/2013/10/two-main-types-of-data-documentation-workflow/&quot;&gt;http://ivanhanigan.github.io/2013/10/two-main-types-of-data-documentation-workflow/&lt;/a&gt;).  
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GUI
&lt;/li&gt;
&lt;li&gt;Programatic
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  
I also think there are two types of users with different motivations and constraints:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1) Data Analysts
&lt;/li&gt;
&lt;li&gt;2) Data Librarians
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3&lt;/span&gt; Analysts can often trade-off completeness of documentation for speed&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3&quot;&gt;

&lt;p&gt;In my view the Analysts group of users need a tool that will very rapidly document their data and workflow steps and can live with a bit less rigour in the quality of documentation.  Obviously this is not ideal but seems an inevitable trade-off needed to enable analysts to keep up the momentum of the data processing and modelling without getting distracted by tedious (and potentially unnecessary) data documentation tasks.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4&lt;/span&gt; Librarians produce gold plated documentation and can take longer to produce this&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4&quot;&gt;

&lt;p&gt;On the other hand the role of the Librarian group is to produce documentation to the best level possible (given time and resource constraints) the datasets and methodologies that lead to the creation of the datasets.  For that group Rigour will take precedence and there will be a trade-off in terms of the amount of time needed to produce the documentation.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5&lt;/span&gt; An example&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5&quot;&gt;

&lt;p&gt;As an example of the two different groups, an analyst working with weather data in Australia may want to specify that their variable &quot;temperature&quot; is the average of the daily maxima and minima, but might not need to specify that the observations were taken inside a Stevenson Screen, or even if they are in Celsius, Farenhiet or Kelvin.  They will be very keen to start the analysis to identify any associations between weather variables and the response variable they are investigating.   The data librarian on the other hand will be more likely to need to include this information so that the users of the temperature data do not mis-interpret it.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-6&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.6&lt;/span&gt; Embracing Inaccuracy and Incompleteness&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-6&quot;&gt;


&lt;ul&gt;
&lt;li&gt;I've been talking about this for a while got referred to this document by Ben Davies at the ANUSF
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[http://thedailywtf.com/Articles/Documentation-Done-Right.aspx](&lt;a href=&quot;http://thedailywtf.com/Articles/Documentation-Done-Right.aspx&quot;&gt;http://thedailywtf.com/Articles/Documentation-Done-Right.aspx&lt;/a&gt;)
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;It has this bit:
&lt;/li&gt;
&lt;/ul&gt;




&lt;pre class=&quot;src src-R&quot;&gt;  
   
Embracing Inaccuracy and Incompleteness 
    
The immediate answer to what&amp;#8217;s the right way to do documentation is
clear: produce the least amount of documentation needed to facilitate
the most understanding, and be very explicit about which documentation
is to be maintained and which is to be archived (i.e., read-only and
left to rot).
&lt;/pre&gt;


&lt;ul&gt;
&lt;li&gt;Roughly speaking, a full EML document produced by Morpho is a bit like a whole bunch of cruft that isnt needed and gets in the way (and is more confusing)
&lt;/li&gt;
&lt;li&gt;Whereas a minimal version Im thinking of covers almost all the generic entries providing the &quot;minimum amount of stuff to make it work right&quot;.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-7&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-7&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.7&lt;/span&gt; Aim&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-7&quot;&gt;


&lt;ul&gt;
&lt;li&gt;This experiment aims to speed up the creation of a minimal &quot;skeleton&quot; of metadata to a level that both the groups above can be comfortable with AS A FIRST STEP.
&lt;/li&gt;
&lt;li&gt;It is assumed that additional steps will then need to be taken to complete the documentation, but the automation of the first part of the process should shave off enough time to suit the purposes of both groups
&lt;/li&gt;
&lt;li&gt;It is an imperative that the quick-start creation of the metadata does not end up costing the documentor more time later on down the track if they need to go back to many of the elements for additional editing.
&lt;/li&gt;
&lt;/ul&gt;





&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-8&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-8&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.8&lt;/span&gt; Step 1: load a simple example dataset&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-8&quot;&gt;

&lt;p&gt;I've been using a [fictitious dataset from a Statistics Methodology paper by Ritschard 2006](&lt;a href=&quot;http://ivanhanigan.github.io/2013/10/test-data-for-classification-trees/&quot;&gt;http://ivanhanigan.github.io/2013/10/test-data-for-classification-trees/&lt;/a&gt;).  It will do as a first cut but when it comes to actually test this out it would be good to have something that would take a bit longer (so that the frustrations of using Morpho become very apparent).
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;  &lt;span style=&quot;color: #586e75;&quot;&gt;#### &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;R Code:&lt;/span&gt;
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;func&lt;/span&gt;
      &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(devtools)
      install_github(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;disentangle&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;ivanhanigan&quot;&lt;/span&gt;)
      &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(disentangle)
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;load&lt;/span&gt;
      fpath &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; system.file(
          file.path(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;extdata&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;civst_gend_sector_full.csv&quot;&lt;/span&gt;),
          package = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;disentangle&quot;&lt;/span&gt;
          )
      data_set &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(fpath)
      summary(data_set)
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;store it in the current project workspace&lt;/span&gt;
      write.csv(data_set, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data/civst_gend_sector_full.csv&quot;&lt;/span&gt;, row.names = F)
      



&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| divorced/widowed: 33 | female:132 | primary  :116 | Min.   : 128.9 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| married         :120 | male  :141 | secondary: 99 | 1st Qu.: 768.3 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| single          :120 | nil        | tertiary : 58 | Median : 922.8 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| nil                  | nil        | nil           | Mean   : 908.4 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| nil                  | nil        | nil           | 3rd Qu.:1079.1 |&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;| nil                  | nil        | nil           | Max.   :1479.4 |&lt;/span&gt;

&lt;/pre&gt;




&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-9&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-9&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.9&lt;/span&gt; Step 2 create a function to deliver the minimal metadata object&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-9&quot;&gt;

&lt;ul&gt;
&lt;li&gt;the package REML will create a EML metadata document quite easily
&lt;/li&gt;
&lt;li&gt;I will assume that a lot of the data elements are self explanatory and take column names and factor levels as the descriptions
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-10&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-10&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.10&lt;/span&gt; reml&lt;sub&gt;boilerplate&lt;/sub&gt;-code&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-10&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:reml_boilerplate&lt;/span&gt;
 
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;func&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(reml)) {
  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(devtools)
  install_github(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;reml&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;ropensci&quot;&lt;/span&gt;)
  } 
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(reml)

&lt;span style=&quot;color: #268bd2;&quot;&gt;reml_boilerplate&lt;/span&gt; &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;function&lt;/span&gt;(data_set, created_by = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;Ivan Hanigan &amp;lt;&lt;a href=&quot;mailto:ivanhanigan&amp;#64;gmail.com&quot;&gt;ivanhanigan&amp;#64;gmail.com&lt;/a&gt;&amp;gt;&quot;&lt;/span&gt;, data_dir = getwd(), titl = &lt;span style=&quot;color: #b58900;&quot;&gt;NA&lt;/span&gt;, desc = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;&quot;&lt;/span&gt;)
{

  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;essential&lt;/span&gt;
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(is.na(titl)) &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;stop&lt;/span&gt;(print(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;must specify title&quot;&lt;/span&gt;))
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;we can get the col names easily&lt;/span&gt;
  col_defs &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; names(data_set)
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;next create a list from the data&lt;/span&gt;
  unit_defs &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; list()
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:ncol(data_set))
    {
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;i = 4&lt;/span&gt;
      &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(is.numeric(data_set[,i])){
        unit_defs[[i]] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;numeric&quot;&lt;/span&gt;
      } &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;else&lt;/span&gt; {
        unit_defs[[i]] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; names(table(data_set[,i]))          
      }
    }
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;unit_defs&lt;/span&gt;
  
  ds &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; data.set(data_set,
                 col.defs = col_defs,
                 unit.defs = unit_defs
                 )
  &lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;str(ds)&lt;/span&gt;

  metadata  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; metadata(ds)
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;needs names&lt;/span&gt;
  &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:ncol(data_set))
    {
      &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;i = 4&lt;/span&gt;
      &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(is.numeric(data_set[,i])){
        names(metadata[[i]][[3]]) &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;number&quot;&lt;/span&gt;
      } &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;else&lt;/span&gt; {
        names(metadata[[i]][[3]]) &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; metadata[[i]][[3]]
      }
    }
  &lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;metadata&lt;/span&gt;
  oldwd &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; getwd()
  setwd(data_dir)
  eml_write(data_set, metadata,
            title = titl,  
            description = desc,
            creator = created_by
            )
  setwd(oldwd)
  sprintf(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;your metadata has been created in the '%s' directory&quot;&lt;/span&gt;, data_dir)
  }
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-11&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-11&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.11&lt;/span&gt; reml&lt;sub&gt;boilerplate&lt;/sub&gt;-test-code&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-11&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:reml_boilerplate-test&lt;/span&gt;

analyte &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data/civst_gend_sector_full.csv&quot;&lt;/span&gt;)
reml_boilerplate(
  data_set = analyte,
  created_by = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;Ivan Hanigan &amp;lt;&lt;a href=&quot;mailto:ivanhanigan&amp;#64;gmail.com&quot;&gt;ivanhanigan&amp;#64;gmail.com&lt;/a&gt;&amp;gt;&quot;&lt;/span&gt;,
  data_dir = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data&quot;&lt;/span&gt;,
  titl = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;civst_gend_sector_full&quot;&lt;/span&gt;,
  desc = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;An example, fictional dataset&quot;&lt;/span&gt;
  )

dir(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;data&quot;&lt;/span&gt;)
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-12&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-12&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.12&lt;/span&gt; Results: This loads into Morpho with some errors&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-12&quot;&gt;

&lt;ul&gt;
&lt;li&gt;Notably unable to import the data file
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
&lt;img src=&quot;/images/morpho-reml-boilerplate.png&quot; alt = &quot;morpho-reml-boilerplate.png&quot;&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Also &quot;the saved document is not valid for some reason&quot;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
&lt;img src=&quot;/images/morpho-reml-boilerplate2.png&quot; alt = &quot;morpho-reml-boilerplate2.png&quot;&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-13&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-13&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.13&lt;/span&gt; Conclusions&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-13&quot;&gt;

&lt;ul&gt;
&lt;li&gt;This needs testing
&lt;/li&gt;
&lt;li&gt;A real deal breaker is if the EML is not valid 
&lt;/li&gt;
&lt;li&gt;In some cases not having the data table included will be a deal breaker (ie KNB repositories designed for downloading complete data packs
&lt;/li&gt;
&lt;li&gt;A definite failure would be that even if it is quicker to get started if it takes a long time and is difficult to fix up it might increase the risk of misunderstandings.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>counting-number-of-consecutive-months-in-drought</title>
   <link href="http://ivanhanigan.github.com/2013/10/counting-number-of-consecutive-months-in-drought/"/>
   <updated>2013-10-29T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/counting-number-of-consecutive-months-in-drought</id>
   <content type="html">&lt;p&gt;I got this question today from a Drought researcher:&lt;/p&gt;

&lt;h4&gt;Question:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;There a table with 62 years * 12 months of rain data in mm. We
calculated the cumulative distribution using ecdf: empirical
cumulative distribution function. So the table looks like this:
Year Jan Feb Mach (…) each cell contains the cum dist.

We also got the number of months per year with less than X mm of
rain. But he needs to know how many months are between the
drought months, regardless the year. So, I thought that
converting the data into a ts was going to facilitate the task,
but it doesn’t, because now I don’t’ have columns and rows any
longer.

How do I handle a ts object to do for example an ifelse?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;I answered:&lt;/p&gt;

&lt;p&gt;This looks a lot like the Hutchinson Drought Index which I've coded up here&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex&quot;&gt;https://github.com/ivanhanigan/HutchinsonDroughtIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex/blob/master/src/HutchinsonDroughtIndex_tools_droughtIndex.r&quot;&gt;https://github.com/ivanhanigan/HutchinsonDroughtIndex/blob/master/src/HutchinsonDroughtIndex_tools_droughtIndex.r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Also there is a completely reproducible example with BoM rainfall data at &lt;a href=&quot;https://github.com/ivanhanigan/SuicideAndDroughtInNSW&quot;&gt;https://github.com/ivanhanigan/SuicideAndDroughtInNSW&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is the key bit for counting consecutive months below a threshold&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;x&amp;lt;-data$index&amp;lt;=-1
xx &amp;lt;- (cumsum(!x) + 1) * x
x2&amp;lt;-(seq_along(x) - match(xx, xx) + 1) * x
data$count&amp;lt;-x2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;
I got it from &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2007-June/134594.html&quot;&gt;https://stat.ethz.ch/pipermail/r-help/2007-June/134594.html&lt;/a&gt;
and looked in wonder at the structure and am amazed it works.&lt;/p&gt;

&lt;p&gt;I also really like the cumulative summing version a lot&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;data$sums&amp;lt;-as.numeric(0)
y &amp;lt;- ifelse(data$index &amp;gt;= -1, 0, data$index)
f &amp;lt;- data$index &amp;lt; -1
f &amp;lt;- (cumsum(!f) + 1) * f
z &amp;lt;- unsplit(lapply(split(y,f),cumsum),f)
data$sums &amp;lt;- z
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;
Several things I'd like to change.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I like the idea of using a wide dataset and ecdf rather than my loop thru months&lt;/li&gt;
&lt;li&gt;I'd like to look into Joe Wheatley;s approach &lt;a href=&quot;]http://joewheatley.net/20th-century-droughts/&quot;&gt;http://joewheatley.net/20th-century-droughts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I was defeated by Mike's request to make a different threshold for breaking a drought than starting a drought.  went back to a for loop&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>spatial-lag-and-timeseries-model-with-nmmaps</title>
   <link href="http://ivanhanigan.github.com/2013/10/spatial-lag-and-timeseries-model-with-nmmaps/"/>
   <updated>2013-10-28T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/spatial-lag-and-timeseries-model-with-nmmaps</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;Today I chatted with Phil Kokic at CSIRO Mathematics, Informatics and Statistics about the way the spatially lagged neighbours variable absorbs any residual spatial correlation in the errors&lt;/li&gt;
&lt;li&gt;We agreed that this is a pretty minimal attempt, not as good as a spatial error model but pretty easy to implement&lt;/li&gt;
&lt;li&gt;I've hacked together some very very ugly code to construct the lagged variable&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ivanhanigan.github.io/spatiotemporal-regression-models/#sec-3&quot;&gt;http://ivanhanigan.github.io/spatiotemporal-regression-models/#sec-3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There may be errors, it's been a long day, but I won't have a chance to check back on this till next week so I thought I'd put it out there as is.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Daintree Rainforest Observatory Climate Data from AWAP-GRIDS</title>
   <link href="http://ivanhanigan.github.com/2013/10/extract-weather-data-from-awap-grids/"/>
   <updated>2013-10-26T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/extract-weather-data-from-awap-grids</id>
   <content type="html">&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;AWAP GRIDS &lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Introduction and Methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 Material and Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 R-code-for-extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3&quot;&gt;3 Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-4&quot;&gt;4 R-code-for-comparison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-5&quot;&gt;5 Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; Introduction and Methods&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;


&lt;p&gt;
This is a work in progress.  It is a stub of an article I want to put together which shows how to use several online data repositories together as a showcase of the [Scientific Workflow and Integration Software for Health (SWISH) Climate Impact Assessments](&lt;a href=&quot;https://github.com/swish-climate-impact-assessment&quot;&gt;https://github.com/swish-climate-impact-assessment&lt;/a&gt;) project.
&lt;/p&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1&lt;/span&gt; Authors&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1&quot;&gt;

&lt;ul&gt;
&lt;li&gt;Ivan Hanigan and [Markus Nolf](&lt;a href=&quot;http://www.thinkoholic.com&quot;&gt;http://www.thinkoholic.com&lt;/a&gt;)
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2&lt;/span&gt; Background&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2&quot;&gt;


&lt;ul&gt;
&lt;li&gt;Markus Nolf offers this use case of the [SWISH EWEDB](&lt;a href=&quot;http://swish-climate-impact-assessment.github.io/&quot;&gt;http://swish-climate-impact-assessment.github.io/&lt;/a&gt;)
&lt;/li&gt;
&lt;li&gt;Markus is pulling together his  Daintree Rainforest Observatory (DRO) data into a manuscript for publication, and was looking for climate data from 2012 as well as long-term. 
&lt;/li&gt;
&lt;li&gt;More specifically, the annual precipitation and mean annual temperature for both 2012 and the 30-year mean.
&lt;/li&gt;
&lt;li&gt;The Australian Bureau of Meteorology has a nice rainfall dataset available at &lt;a href=&quot;http://www.bom.gov.au/climate/data/&quot;&gt;http://www.bom.gov.au/climate/data/&lt;/a&gt; (&quot;Cape Trib Store&quot; weather station), but it seems like the temperature records are patchy.
&lt;/li&gt;
&lt;li&gt;So it is advised to use the data the DRO collects its self
&lt;/li&gt;
&lt;li&gt;You need to apply through the [ASN SuperSite data portal](&lt;a href=&quot;http://www.tern-supersites.net.au/knb/&quot;&gt;http://www.tern-supersites.net.au/knb/&lt;/a&gt;) for access to the daily data for the DRO.
&lt;/li&gt;
&lt;li&gt;Note the use of the DRO met data will need to be properly cited as it is harder to keep
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;an AWS station running in the tropics for years than it is to collect most other data. 
The citation information is provided when you make a request to access the data.
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;The long term mean used by most DRO researchers is from the BOM station as we only have a short record from the station itself.  The offset is around 1000mm.
&lt;/li&gt;
&lt;li&gt;However what we want is mean annual temperatures but the BOM website seems to focus more on mean minimum and maximum temperatures.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3&lt;/span&gt; Material and Methods&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3&quot;&gt;


&lt;ul&gt;
&lt;li id=&quot;sec-1-3-1&quot;&gt;Baseline Climate Data 2012, Far North Queensland Rainforest Supersite, Cape Tribulation Node&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;We can use the data portal too see [the data file in question](&lt;a href=&quot;http://www.tern-supersites.net.au/knb/metacat/lloyd.238.13/html&quot;&gt;http://www.tern-supersites.net.au/knb/metacat/lloyd.238.13/html&lt;/a&gt;)
&lt;/li&gt;
&lt;li&gt;Application for access is via email
&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-3-2&quot;&gt;Extract mean annual temperatures at the BOM website&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;SWISH uses BoM data a fair bit and aims to streamline access to BoM data for extreme weather event analysis (which require long term average climatology to provide the baseline that extremes are measured against).
&lt;/li&gt;
&lt;li&gt;WRT to temperature most daily averages from BoM are calculated as average of maximum&lt;sub&gt;temperature&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;24&lt;/sub&gt;&lt;sub&gt;hours&lt;/sub&gt;&lt;sub&gt;after&lt;/sub&gt;&lt;sub&gt;9am&lt;/sub&gt;&lt;sub&gt;local&lt;/sub&gt;&lt;sub&gt;time&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;degrees&lt;/sub&gt; and minimum&lt;sub&gt;temperature&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;24&lt;/sub&gt;&lt;sub&gt;hours&lt;/sub&gt;&lt;sub&gt;before&lt;/sub&gt;&lt;sub&gt;9am&lt;/sub&gt;&lt;sub&gt;local&lt;/sub&gt;&lt;sub&gt;time&lt;/sub&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;sub&gt;degree&lt;/sub&gt; (only couple of hundred AWS provide hourly data to get the proper mean of 24 obs).
&lt;/li&gt;
&lt;li&gt;The Bureau of Meteorology has generated a range of gridded meteorological datasets for Australia as a contribution to the Australian Water Availability Project (AWAP). These include daily max and min temperature which you could use to generate daily averages, then calculate your long term averages from those?  
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.bom.gov.au/jsp/awap/&quot;&gt;http://www.bom.gov.au/jsp/awap/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Documentation is at &lt;a href=&quot;http://www.bom.gov.au/amm/docs/2009/jones.pdf&quot;&gt;http://www.bom.gov.au/amm/docs/2009/jones.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-1-3-3&quot;&gt;A workflow to download and process the public BoM weather grids.&lt;br/&gt;
&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; R-depends&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #7f9f7f;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7f9f7f;&quot;&gt;depends&lt;/span&gt;
install.packages(c(&lt;span style=&quot;color: #cc9393;&quot;&gt;'raster'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'rgdal'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'plyr'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'RODBC'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'RCurl'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'XML'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'ggmap'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'maptools'&lt;/span&gt;, &lt;span style=&quot;color: #cc9393;&quot;&gt;'spdep'&lt;/span&gt;))

&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;This workflow uses the open source R software with some of our custom written packages:
&lt;/li&gt;
&lt;/ul&gt;



&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;2&lt;/span&gt; R-code-for-extraction&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-2&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;name:r-code&lt;/span&gt;



&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;aim daily weather for any point location from online BoM weather grids&lt;/span&gt;

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;depends on some github packages&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(awaptools)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;http://swish-climate-impact-assessment.github.io/tools/awaptools/awaptools-downloads.html&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(swishdbtools)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools-downloads.html&lt;/span&gt;
&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(gisviz)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;http://ivanhanigan.github.io/gisviz/&lt;/span&gt;

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;and this from CRAN&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(raster)) install.packages(&lt;span style=&quot;color: #2aa198;&quot;&gt;'raster'&lt;/span&gt;); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(raster)

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;get weather data, beware that each grid is a couple of megabytes&lt;/span&gt;
vars &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;maxave&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;minave&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;totals&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;vprph09&quot;&lt;/span&gt;,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;vprph15&quot;&lt;/span&gt;) &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;,&quot;solarave&quot;) &lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;solar only available after 1990&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(measure &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; vars)
{
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;measure &amp;lt;- vars[1]&lt;/span&gt;
  get_awap_data(start = &lt;span style=&quot;color: #2aa198;&quot;&gt;'1960-01-01'&lt;/span&gt;,end = &lt;span style=&quot;color: #2aa198;&quot;&gt;'1960-01-02'&lt;/span&gt;, measure)
}

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;get location&lt;/span&gt;
locn &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; geocode(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;daintree rainforest&quot;&lt;/span&gt;)
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;this uses google maps API, better check this&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;lon       lat&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;1 145.4185 -16.17003&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;## &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;Treat data frame as spatial points&lt;/span&gt;
epsg &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; make_EPSG()
shp &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
                              proj4string=CRS(epsg$prj4[epsg$code %&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt;% &lt;span style=&quot;color: #2aa198;&quot;&gt;'4283'&lt;/span&gt;]))
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;now loop over grids and extract met data&lt;/span&gt;
cfiles &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt;  dir(pattern=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;grid$&quot;&lt;/span&gt;)

&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt; (i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; seq_len(length(cfiles))) {
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;i &amp;lt;- 1 ## for stepping thru&lt;/span&gt;
  gridname &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; cfiles[[i]]
  r &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; raster(gridname)
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;image(r) # plot to look at&lt;/span&gt;
  e &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; extract(r, shp, df=T)
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;str(e) ## print for debugging&lt;/span&gt;
  e1 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; shp
  e1@data$values &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; e[,2]
  e1@data$gridname &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; gridname
  &lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;write to to target file&lt;/span&gt;
  write.table(e1@data,&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;output.csv&quot;&lt;/span&gt;,
    col.names = i == 1, append = i&amp;gt;1 , sep = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;,&quot;&lt;/span&gt;, row.names = &lt;span style=&quot;color: #b58900;&quot;&gt;FALSE&lt;/span&gt;)
}

&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;further work is required to format the column with the gridname to get out the date and weather paramaters.&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;3&lt;/span&gt; Results&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-3&quot;&gt;

&lt;ul&gt;
&lt;li id=&quot;sec-3-1&quot;&gt;Results&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;Markus reports:
&lt;/li&gt;
&lt;li&gt;&quot;The R-script worked great once i had set a working directory that did not include spaces. (It may have been a different problem that got solved by changing the wd, but the important thing is it's running now.)&quot;
&lt;/li&gt;
&lt;li&gt;Markus downloaded 70+ GB of gridded weather data from the BoM website to his local computer
&lt;/li&gt;
&lt;li&gt;Also note there is another set of gridded data available from the BOM, which contains pre-computed longterm mean temps, [ready to be extracted with the script](&lt;a href=&quot;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&quot;&gt;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&lt;/a&gt;)
&lt;/li&gt;
&lt;li&gt;&quot;Using this file, I only needed to get the 2012 temp grids for a comparison of 2012 vs. 30-year data. I'm going to run the extraction of 1961-1990 data, just to be sure.&quot;
&lt;/li&gt;
&lt;li&gt;&quot;When we finished analysis of the long-term temperature from daily means found:
&lt;/li&gt;
&lt;li&gt;While the official, pre-computed long-term mean (i.e. 30-year grid file, analysed with your script) was 22.29 °C for the DRO coordinates (145.4494, -16.1041), the new value from daily means (i.e. daily minave and maxave averaged) is 24.91 °C.
&lt;/li&gt;
&lt;li&gt;We're not sure what causes this discrepancy, but thought we'd note that there is one.
&lt;/li&gt;
&lt;li&gt;For the manuscript, we ended up using the means obtained via BOM's method* to compare 1961-1990 values to 2012, both computed with the above script.
&lt;/li&gt;
&lt;li&gt;(* average of daily min/max temperature for each year, then averaged across the entire 30 year period)
&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li id=&quot;sec-3-2&quot;&gt;Dataset discrepancy&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;Following up on the interesting a difference between the two BoM datasets. 
&lt;/li&gt;
&lt;li&gt;One thing that might cause this might be if you are calculating the average of the annual averages ie sum(annavs)/30 or the average of all the daily averages as sum(dailyavs)/(30 * 365 or 366)?  the variance will differ by these methods.
&lt;/li&gt;
&lt;li&gt;looks like the 30 year dataset is the former:
&lt;/li&gt;
&lt;li&gt;&quot;Average annual temperatures (maximum, minimum or mean) are calculated by adding daily temperature values each year, dividing by the number of days in that year to get an average for that particular year. The average values for each year in a specified period (1961 to 1990) are added together and the final value is calculated by dividing by the number of years in the period (30 years in this case).&quot;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[metadata](&lt;a href=&quot;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&quot;&gt;http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;amp;period=#maps&lt;/a&gt;)
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Markus followed the BOM calculation method, and just compared it with two other approaches.
&lt;/li&gt;
&lt;li&gt;average of all 21914 values
&lt;/li&gt;
&lt;li&gt;average of yearly sum(min and max values per year)/(ndays*2)
&lt;/li&gt;
&lt;li&gt;average of yearly sum(daily average)/ndays)
&lt;/li&gt;
&lt;li&gt;where ndays = number of days per year.
&lt;/li&gt;
&lt;li&gt;Differences between these methods show only in the 6th decimal place, far from 2.62 degrees.
&lt;/li&gt;
&lt;/ul&gt;



&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-4&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;4&lt;/span&gt; R-code-for-comparison&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;This is Markus' comparison script &lt;/span&gt;
&lt;span style=&quot;color: #93a1a1;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;also see the formatted table csv, as well as the SWISH script's raw output csv&lt;/span&gt;

setwd(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;E:\\markus\\ph.d\\aus-daintree\\data-analysis\\climate&quot;&lt;/span&gt;)
climate &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;minmaxave-30year-daily.csv&quot;&lt;/span&gt;, sep=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;,&quot;&lt;/span&gt;, dec=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;.&quot;&lt;/span&gt;)

climate$year &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(climate$file,1,4)
climate$dailymean &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; (climate$maxave+climate$minave)/2
head(climate)


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;total average across all days and values&lt;/span&gt;
annmean &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; mean(c(climate$maxave,climate$minave))
annmean


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;daily means averaged by year, then total average&lt;/span&gt;
annmean1 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(1,2)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:30) {
        annmean1[i] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; mean(climate[climate$year==(i+1960),]$dailymean)
        &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;print(annmean1[i])&lt;/span&gt;
}
annmean1
mean(annmean1)


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;mean of all values per year, then total average&lt;/span&gt;
annmean2 &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(1,2)
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:30) {
        tmpdata &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; climate[climate$year==(i+1960),]
        annmean2[i] &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; (sum(tmpdata$maxave) + sum(tmpdata$minave))/(length(tmpdata$maxave)+length(tmpdata$minave))
        &lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;print(annmean2[i])&lt;/span&gt;
}
annmean2; mean(annmean2)


&lt;span style=&quot;color: #93a1a1;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #93a1a1;&quot;&gt;differences&lt;/span&gt;
annmean - mean(annmean1)
annmean - mean(annmean2)
mean(annmean1) - mean(annmean2)


&lt;/pre&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-5&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-5&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;5&lt;/span&gt; Discussion&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-5&quot;&gt;


&lt;ul&gt;
&lt;li&gt;Principal findings: Very convenient automated extraction of location-based time series data for the precise period that is requested.
&lt;/li&gt;
&lt;li&gt;Weaknesses (whole method, not your script): very long download time for daily grids (~11.000 grids = huge dataset, took several days in my case). Yearly grids would be beneficial (and I believe most others are also looking mainly for data on a yearly (or larger) scale).
&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
&lt;li id=&quot;sec-5-1&quot;&gt;Conclusion&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;Take home message: Seems like a perfect case of &quot;double-check the data using one and the same method&quot;.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>document-first-ask-questions-later</title>
   <link href="http://ivanhanigan.github.com/2013/10/document-first-ask-questions-later/"/>
   <updated>2013-10-25T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/document-first-ask-questions-later</id>
   <content type="html">&lt;p&gt;This post is just a short note about something I'm thinking of calling &quot;documentation-driven development&quot;.
It is based on the concept of &lt;a href=&quot;http://en.wikipedia.org/wiki/Test-driven_development&quot;&gt;&quot;test-driven development&quot;&lt;/a&gt;, and more recently:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://lamages.blogspot.in/2013/04/test-driven-analysis.html&quot;&gt;&quot;test-driven analysis&quot;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;or even &lt;a href=&quot;http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/&quot;&gt;&quot;Evidence-based Data Analysis&quot;&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;It is also a kind of a critique on the paradigm suggested by the BCCVL statement on &lt;a href=&quot;http://bccvl.org.au/blog/2013/08/20/just-in-time-metadata/&quot;&gt;&quot;Just-In-Time metadata&quot;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Anyway, it is a small thing but hopefully big things will grow.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A Great Intro 2 Logistic Regression</title>
   <link href="http://ivanhanigan.github.com/2013/10/challenger-logistic/"/>
   <updated>2013-10-18T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/challenger-logistic</id>
   <content type="html">&lt;p&gt;This is a great example of logistic regression,  because it is pretty simple but covers good ground.  I got it from Peter Caley;s R tutorial workbook from Charles Darwin School of Environmental Research.&lt;/p&gt;

&lt;p&gt;It is also a tragic example of the impact weather can have on health.&lt;br/&gt;
The colder it is the more likely the shuttle is to explode.&lt;/p&gt;

&lt;p&gt;The problem was with the failure rate (and number of) O-rings that failed (n.fail) related to the temperature (temp).&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#Load the data
#The following R code will construct the dataset
n.fail &amp;lt;- c(2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0)
temp &amp;lt;- c(53, 66, 68, 70, 75, 78, 57, 67, 69, 70, 75, 79, 58, 67, 70, 72, 76, 81, 63, 67, 70, 73, 76)
# there were 6 o rings for each of 23 attempts
total &amp;lt;- rep(6,23)
# probability of fail
p.fail &amp;lt;- n.fail/total
# Response = resp column bind them together  
resp &amp;lt;- cbind(n.fail, total-n.fail)

###########################################################################
# we can write text files easily once the data frame or matrix is in shape
data &amp;lt;- as.data.frame(cbind(resp,temp))
names(data) &amp;lt;- c('nfail','totalMinusNfail', 'temp')
# write.csv(data, 'learnR-logistic-data.csv', row.names=F)

###########################################################################
# and read it in again 
# data2 &amp;lt;- read.csv('learnR-logistic-data.csv')

################################################################
# name:learnR-logistic
png('images/pfail.png')
plot(temp, p.fail, pch=16, xlim=c(40,100), ylim=c(0,0.4))
title('A plot of the proportion failed by temperature')
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/pfail.png&quot; alt=&quot;pfail.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;###########################################################################
# newnode: linear
linear &amp;lt;- glm(resp ~ 1 + temp, family=binomial(link=logit))
summary(linear)
linearoutput &amp;lt;- summary(linear)
linearoutput$coeff

###########################################################################
# newnode: learnR-logistic
cf &amp;lt;- linearoutput$coeff
signif(cf[which(row.names(cf) == 'temp'),'Estimate'],2)

###########################################################################
# newnode: learnR-logistic
# write.csv(linearoutput$coeff,&quot;challengerOfails.csv&quot;)

###########################################################################
# newnode: learnR-logistic
 png('images/challengerLogistic.png')
 par(mfrow=c(2,2))
 plot(linear)
 dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/challengerLogistic.png&quot; alt=&quot;challengerLogistic.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;####################################################################
# newnode: learnR-logistic
dummy &amp;lt;- data.frame(temp=seq(20,100,1))
pred.prob &amp;lt;- predict.glm(linear, newdata=dummy, type=&quot;resp&quot;)
png('images/pfailfit.png')
plot(temp, p.fail, xlab=&quot;Launch Temperature (F)&quot;,
 ylab=&quot;Proportion Failing&quot;, pch=16, xlim=c(20,100), ylim=c(0,1.0))
lines(dummy$temp, pred.prob)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/pfailfit.png&quot; alt=&quot;pfailfit.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;####################################################################
resp &amp;lt;- as.data.frame(resp)
resp$fail &amp;lt;- ifelse(resp$n.fail &amp;gt; 0, 1, 0)
resp$temp &amp;lt;- temp

png('images/fail.png')
with(resp, plot(temp, fail, xlab=&quot;Launch Temperature (F)&quot;,ylab=&quot;Joint damage&quot;, pch=16, xlim=c(50,80), ylim=c(0,1.0))
     )
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/fail.png&quot; alt=&quot;fail.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;R Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;chal.logit &amp;lt;- glm(fail~temp,family=binomial, data = resp)
summary(chal.logit)$coeff

png('images/pfailfit2.png')
cx &amp;lt;- c(50:80/1)
cyhat &amp;lt;- coefficients(chal.logit)[c(1)] +
coefficients(chal.logit)[c(2)]*cx
cpihat &amp;lt;- exp(cyhat)/(1+exp(cyhat))
with(resp,plot(temp,fail,xlab=&quot;Temperature&quot;,ylab=&quot;Damage&quot;,
main=&quot;Incidence of Booster Field Joint Damage vs. Temperature&quot;, xlim = c(50,80))
     )
lines(cx,cpihat)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/pfailfit2.png&quot; alt=&quot;pfailfit2.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>spatially-structured-time-series-with-nmmaps</title>
   <link href="http://ivanhanigan.github.com/2013/10/spatially-structured-time-series-with-nmmaps/"/>
   <updated>2013-10-16T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/spatially-structured-time-series-with-nmmaps</id>
   <content type="html">&lt;p&gt;I will use the NMMAPSlite datasets for a simple example of what I
describe as &quot;Spatially Structured Timeseries&quot; as opposed to
&quot;Spatio-Temporal&quot; which I think more explicitly includes spatial
structure in the model.  &lt;a href=&quot;http://ivanhanigan.github.io/spatiotemporal-regression-models/&quot;&gt;See This Report&lt;/a&gt; for all the gory details.&lt;/p&gt;

&lt;h1&gt;R Codes&lt;/h1&gt;

&lt;!-- &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; --&gt;


&lt;!-- &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Strict//EN&quot; --&gt;


&lt;!--                &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd&quot;&gt; --&gt;


&lt;!-- &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;en&quot; xml:lang=&quot;en&quot;&gt; --&gt;


&lt;p&gt;&lt;head&gt;
&lt;title&gt;Spatiotemporal Regression Modelling&lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;Spatiotemporal Regression Modelling&quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-10-16T15:17+1100&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Spatiotemporal Regression Modelling&lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Core Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 Core Model Plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1&lt;/span&gt; Core Model&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #5F7F5F;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;name:core&lt;/span&gt;
&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;func&lt;/span&gt;
setwd(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;~/projects/spatiotemporal-regression-models/NMMAPS-example&quot;&lt;/span&gt;)
&lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;require&lt;/span&gt;(mgcv)
&lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;require&lt;/span&gt;(splines)

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;load&lt;/span&gt;
analyte &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;analyte.csv&quot;&lt;/span&gt;)

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;clean&lt;/span&gt;
analyte$yy &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; substr(analyte$date,1,4)
numYears&lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt;length(names(table(analyte$yy)))
analyte$date &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; as.Date(analyte$date)
analyte$time &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; as.numeric(analyte$date)
analyte$agecat &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; factor(analyte$agecat,
                          levels = c(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;under65&quot;&lt;/span&gt;,
                              &lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;65to74&quot;&lt;/span&gt;, &lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;75p&quot;&lt;/span&gt;),
                          ordered = &lt;span style=&quot;color: #7CB8BB;&quot;&gt;TRUE&lt;/span&gt;
                          )

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;do&lt;/span&gt;
fit &lt;span style=&quot;color: #BFEBBF; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; gam(cvd ~ s(tmax) + s(dptp) +
           city + agecat +
           s(time, k= 7*numYears, fx=T) +
           offset(log(pop)),
           data = analyte, family = poisson
           )

&lt;span style=&quot;color: #5F7F5F;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #7F9F7F;&quot;&gt;plot of response functions&lt;/span&gt;
png(&lt;span style=&quot;color: #CC9393;&quot;&gt;&quot;images/nmmaps-eg-core.png&quot;&lt;/span&gt;, width = 1000, height = 750, res = 150)
par(mfrow=c(2,3))
plot(fit, all.terms = &lt;span style=&quot;color: #7CB8BB;&quot;&gt;TRUE&lt;/span&gt;)
dev.off()


&lt;/pre&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;2&lt;/span&gt; Core Model Plots&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-2&quot;&gt;

&lt;p&gt;&lt;img src=&quot;/images/nmmaps-eg-core.png&quot;  alt=&quot;/images/nmmaps-eg-core.png&quot; /&gt;
&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>morpho-and-rfigshare</title>
   <link href="http://ivanhanigan.github.com/2013/10/morpho-and-rfigshare/"/>
   <updated>2013-10-14T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/morpho-and-rfigshare</id>
   <content type="html">&lt;p&gt;In this Case Study I will use Morpho to compare directly with reml.&lt;/p&gt;

&lt;h1&gt;Step one: Set up morpho&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Follow the instructions at the ASN SuperSite website and install Morpho 1.8 rather than latest version because it has technical issues that stop it from setting permissions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.tern-supersites.net.au/index.php/data/repository-tutorial&quot;&gt;Configure morpho&lt;/a&gt;.  (I will follow the ASN SuperSite instructions as a future Case Study will be to use their KNB Metacat service).&lt;/li&gt;
&lt;li&gt;Do not configure to connect to the Metacat repository, will need a password to be assigned by ASN data manager.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Step 2: Look at the REML created metadata using Morpho&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Morpho offers to open existing sets for modification.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code: get location of my example dataset&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(disentangle)
fpath &amp;lt;- system.file(file.path(&quot;extdata&quot;, &quot;civst_gend_sector.csv&quot;), package=&quot;disentangle&quot;)
fpath
dirname(fpath)
# [1] &quot;/home/ivan_hanigan/Rlibs/disentangle/extdata&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Morpho &gt; File &gt; import = civst_gend_sector_eml.xml&lt;/li&gt;
&lt;li&gt;(not the figshare_civst_gend_sector_eml.xml that was created when sending to figshare)&lt;/li&gt;
&lt;li&gt;Error encountered.  could not open metadata, open empty data package.  Offered to upgrade (unable to edit &gt; accepted)&lt;/li&gt;
&lt;li&gt;unable to display data, empty data package will be shown&lt;/li&gt;
&lt;li&gt;top menu &gt; Documentation &gt; Add/Edit ion

&lt;h1&gt;Step 3: Create new datasets with Morpho&lt;/h1&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>dc-uploader-and-ANU-DataCommons</title>
   <link href="http://ivanhanigan.github.com/2013/10/dc-uploader-and-ANU-DataCommons/"/>
   <updated>2013-10-13T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/dc-uploader-and-ANU-DataCommons</id>
   <content type="html">&lt;p&gt;In this post I use the tool produced at the ANU by the DataCommons team.  This requires Python3.&lt;/p&gt;

&lt;h1&gt;What does it do?&lt;/h1&gt;

&lt;p&gt;The script only creates new collection records. The functionality to edit records didn’t make it into the script as the expectation is that automated ingests will only require creation of new datasets to which files will be uploaded.&lt;/p&gt;

&lt;p&gt;Users can feel free to tweak the collection parameter file to their liking in the development environment until happy with the results.&lt;/p&gt;

&lt;h1&gt;Create the metadata.txt&lt;/h1&gt;

&lt;p&gt;You need to get the python scripts and conf file from the ANU DataCommons team.  Store these somewhere handy and move to that directory.&lt;/p&gt;

&lt;p&gt;change the anudc.conf: to test out the scripts by creating some sample records, please uncomment the “host” field in the file that points to dc7-dev2.anu.edu.au:8443 , and comment out the one that points to datacommons.anu.edu.au:8443.&lt;/p&gt;

&lt;p&gt;Also you get a different token in dev and prod servers for security reasons you cannot use the same token. Also, storing your username and password in plain text is not recommended and is to be used only for debugging purposes. Also, in my case I had to change the owner group to ‘5’ when creating records in dev. In prod, it’s 6.&lt;/p&gt;

&lt;p&gt;You can look int the &quot;Keys.txt&quot; file that contains the full list of values that can be specified in this metadata.txt file.&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;setwd(&quot;~/tools/dcupload&quot;)
sink(&quot;metadata.txt&quot;)
cat(&quot;
# This file, referred to as a collection parameter file, consists of
# data in key=value pairs. This data is sent to the ANU Data Commons
# to create a collection, establish relations with other records,
# and/or upload files to those collections.

# The metadata section consists of metadata for use in creation (not
# for modification) of record metadata in ANU Data Commons. The
# following fields are required for the creation of a record. The file
# Keys.txt contains the full list of values that can be specified in
# this file. Based on this information below, a collection record of
# type databaset with the title &quot;Test Collection 6/05/2013&quot; will be
# created owned by Meteorology and Health group.
[metadata]
type = Collection
subType = dataset
ownerGroup = 5
# 6 on production, 5 on dev
name = Civil Status, Gender and Activity Sector
briefDesc = An example, fictional dataset for Decision Tree Models
citationCreator = Ritschard, G. (2006). Computing and using the deviance with classification trees. In Compstat 2006 - Proceedings in Computational Statistics 17th Symposium Held in Rome, Italy, 2006.
email = ivan.hanigan@anu.edu.au
anzforSubject = 1601

# The relations section allows you to specify the relation this record
# has with other records in the system.  Currently relations with NLA
# identifiers is not supported.
[relations]
isOutputOf = anudc:123

# This section contains a line of the form 'pid = anudc:123' once a
# record has been created so executing the uploader script with the
# same collection parameter file doesnt create a new record with the
# same metadata.
[pid]
&quot;)
sink()

# run the dcload
system(&quot;python3 dcuploader.py -c metadata.txt&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;What happened?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Looking in the metadata.txt file it now has a pid like &quot;pid = test:3527&quot;&lt;/li&gt;
&lt;li&gt;And we have created a new record in our account on the DataCommons server.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;go to the website&lt;/h1&gt;

&lt;p&gt;Now go to &lt;a href=&quot;https://dc7-dev2.anu.edu.au:8443/DataCommons/&quot;&gt;the dev site&lt;/a&gt; and you can continue editing the record manually in the browser.&lt;/p&gt;

&lt;p&gt;Or if we have ironed out the wrinkles you could go straight to the production server at &lt;a href=&quot;https://datacommons.anu.edu.au:8443/DataCommons&quot;&gt;This Link&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Uploading the data&lt;/h1&gt;

&lt;p&gt;The dataset gets sent using a Java applet in the browser while you are manually editing the record using the browser.&lt;/p&gt;

&lt;h1&gt;Notes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After the records get created, the script tries to relate the record to other records as you’ve specified in the collection parameter file in the relations section. If you’re creating a record in dev2, you cannot relate it to a record in production because that record doesn’t exist in dev2. Remember that IDs for records in dev environments have the prefix “test:” while those in production have “anudc:”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, when you ran the script against production the created records were linked with the record with the ID anudc:123. I have now removed those relations. You might want to change that value in your metadata.txt file so the links are established to records that created records actually can be related to. Or for testing purposes, simply delete the entire [relations] section.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>reml-and-rfigshare-part-2</title>
   <link href="http://ivanhanigan.github.com/2013/10/reml-and-rfigshare-part-2/"/>
   <updated>2013-10-12T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/reml-and-rfigshare-part-2</id>
   <content type="html">&lt;p&gt;In the last post I explored the functionality of reml.
This time I will try to send data to figshare.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First follow &lt;a href=&quot;https://github.com/ropensci/rfigshare&quot;&gt;These Instructions&lt;/a&gt; to get rfigshare set up.  In particular store your figshare credentials in ~/.Rprofile&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:reml-and-rfigshare-part-2&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
require(devtools)
install_github(&quot;reml&quot;, &quot;ropensci&quot;)
require(reml)
install_github(&quot;rfigshare&quot;, &quot;ropensci&quot;)
require(rfigshare)
install_github(&quot;disentangle&quot;, &quot;ivanhanigan&quot;)
require(disentangle)
# load
fpath &amp;lt;- system.file(file.path(&quot;extdata&quot;,&quot;civst_gend_sector_eml.xml&quot;), package = &quot;disentangle&quot;)
setwd(dirname(fpath))
obj &amp;lt;- eml_read(fpath)
# clean
obj
# do

## STEP 1: find one of the preset categories
# available. We can ask the API for
# a list of all the categories:
list &amp;lt;- fs_category_list()
list[grep(&quot;Survey&quot;, list)]

## STEP 2: PUBLISH TO FIGSHARE
id &amp;lt;- eml_publish(fname,
                  description=&quot;Example EML
                    A fictional dataset&quot;,
                  categories = &quot;Survey results&quot;,
                  tags = &quot;EML&quot;,
                  destination=&quot;figshare&quot;
                  )
# there are several warnings
# but go to figshare and it has sent the metadata and data OK

# make public using either the figshare web interface, the
# rfigshare package (using fs_make_public(id)) or just by adding
# the argument visibility = TRUE to the above eml_publish
fs_make_public(id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Now these data are on figshare&lt;/h1&gt;

&lt;p&gt;Now I have published the data they are visible and have a DOI&lt;/p&gt;

&lt;iframe src=&quot;http://wl.figshare.com/articles/820158/embed?show_title=1&quot; width=&quot;568&quot; height=&quot;157&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

</content>
 </entry>
 
 <entry>
   <title>data-documentation-case-study-reml-and-rfigshare</title>
   <link href="http://ivanhanigan.github.com/2013/10/data-documentation-case-study-reml-and-rfigshare/"/>
   <updated>2013-10-12T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/data-documentation-case-study-reml-and-rfigshare</id>
   <content type="html">&lt;h4&gt;Case Study: reml-and-rfigshare&lt;/h4&gt;

&lt;p&gt;First we will look at the work of the ROpenSci team and the reml
package.  In the vignette they show how to publish data to figshare
using rfigshare package.  &lt;a href=&quot;http://figshare.com/&quot;&gt;figshare&lt;/a&gt; is a site
where scientists can share datasets/figures/code. The goals are to
encourage researchers to share negative results and make reproducible
research efforts user-friendly. It also uses a tagging system for
scientific research discovery. They give you unlimited public space
and 1GB of private space.&lt;/p&gt;

&lt;p&gt;Start by getting the reml package.&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
require(devtools)
install_github(&quot;reml&quot;, &quot;ropensci&quot;)
require(reml)
?eml_write
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;This is the Top-level API function for writing eml.  Help page is a bit sparse.  See &lt;a href=&quot;https://github.com/ropensci/reml&quot;&gt;This Link&lt;/a&gt; for more.  For eg &quot;for convenience, dat could simply be a data.frame and reml will launch it's metadata wizard to assist in constructing the metadata based on the data.frame provided. While this may be helpful starting out, regular users will find it faster to define the columns and units directly in the format above.&quot;&lt;/p&gt;

&lt;p&gt;Now load up the test data for classification trees I described in &lt;a href=&quot;/2013/10/test-data-for-classification-trees/&quot;&gt;This Post&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;install_github(&quot;disentangle&quot;, &quot;ivanhanigan&quot;) # for the data
                                             # described in prev post

# load
fpath &amp;lt;- system.file(file.path(&quot;extdata&quot;, &quot;civst_gend_sector.csv&quot;),
                     package = &quot;disentangle&quot;
                     )
civst_gend_sector &amp;lt;- read.csv(fpath)

# clean
str(civst_gend_sector)

# do
eml_write(civst_gend_sector,
          creator = &quot;Ivan Hanigan &amp;lt;ivanhanigan@gmail.com&amp;gt;&quot;)





# Starts up the wizard, a section is shown below.  The wizard
# prompts in the console and the user writes the answer.

# Enter description for column 'civil_status':
#  marriage status
# column civil_status appears to contain categorical data.
#  
# Categories are divorced/widowed, married, single
#  Please define each of the categories at the prompt
# define 'divorced/widowed':
# was once married
# define 'married':
# still married
# define 'single':
# never married

# TODO I don't really know what activity_sector is.  I assumed
# school because Categories are primary, secondary, tertiary.

# this created &quot;metadata.xml&quot; and &quot;metadata.csv&quot;
file.remove(c(&quot;metadata.xml&quot;,&quot;metadata.csv&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;
This was a very minimal data documentation effort.  A bit more detail would be better.  Because I would now need to re-write all that in the wizard I will take the advice of the help file that &quot;regular users will find it faster to define the columns and units directly in the format&quot;&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;ds &amp;lt;- data.set(civst_gend_sector,
               col.defs = c(&quot;Marriage status&quot;, &quot;sex&quot;, &quot;education&quot;, &quot;counts&quot;),
               unit.defs = list(c(&quot;was once married&quot;,&quot;still married&quot;,&quot;never married&quot;),
                   c(&quot;women&quot;, &quot;men&quot;),
                   c(&quot;primary school&quot;,&quot;secondary school&quot;,&quot;tertiary school&quot;),
                   c(&quot;persons&quot;))
               )
ds
# this prints the dataset and the metadata
# now run the EML function
eml_write(ds, 
          title = &quot;civst_gend_sector&quot;,  
          description = &quot;An example, fictional dataset for Decision Tree Models&quot;,
          creator = &quot;Ivan Hanigan &amp;lt;ivanhanigan@gmail.com&amp;gt;&quot;,
          file = &quot;inst/extdata/civst_gend_sector_eml.xml&quot;
          )
# this created the xml and csv with out asking anything
# but returned a
## Warning message:
## In `[&amp;lt;-.data.frame`(`*tmp*`, , value = list(civil_status = c(2L,  :
##   Setting class(x) to NULL;   result will no longer be an S4 object

# TODO investigate this?

# now we can access the local EML
obj &amp;lt;- eml_read(&quot;inst/extdata/civst_gend_sector_eml.xml&quot;)
obj 
str(dataTable(obj))
# returns an error
## Error in plyr::compact(lapply(slotNames(from), function(s) if (!isEmpty(slot(from,  (from attribute.R#300) : 
##   subscript out of bounds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h1&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;So this looks like a useful tool.  Next steps are to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;look at sending these data to figshare&lt;/li&gt;
&lt;li&gt;describe a really really REALLY simple workflow (3 lines? create metadata, eml_write, push to figshare)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>two-main-types-of-data-documentation-workflow</title>
   <link href="http://ivanhanigan.github.com/2013/10/two-main-types-of-data-documentation-workflow/"/>
   <updated>2013-10-11T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/two-main-types-of-data-documentation-workflow</id>
   <content type="html">&lt;p&gt;This post introduces a new series of blog posts in which I want to experiment with a few tools for data documentation, which I'll present as Case Studies.  This series of posts will be pitched to an audience mixture of data librarians and data analysts.&lt;/p&gt;

&lt;p&gt;Data documentation occurs in a spectrum from simple notes through to elaborate systems.  I've been working on a conceptual framework about how the actual process can be done in two distinct ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Graphical User Interface (GUI) solutions&lt;/li&gt;
&lt;li&gt;Programmatic (Scripted/Automagic) solutions&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think the GUI tools are in general pretty user friendly and useful
for simple projects with only a small number of datasets, but have a
major drawback for the challenge of heterogeneous data integration.  I
think the problem is expressed nicely &lt;a href=&quot;http://carlboettiger.info/2013/06/23/notes-on-leveraging-the-ecological-markup-language.html&quot;&gt;In This Post By Carl Boettiger&lt;/a&gt;  in reference to Morpho:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;looks like a rather useful if tedious tool for generating EML
files. Unfortunately, without the ability to script inputs or
automatically detect existing data structures, we are forced through
the rather arduous process of adding all metadata annotation each
time....&quot;&lt;/li&gt;
&lt;li&gt;&quot;...A package could also provide utilities to generate EML from R objects, leveraging the metadata implicit in R objects that is not present in a CSV (in which there is no built-in notion of whether  a column is numeric or character string, what missing value characters it uses, or really if it is consistent at all. Avoiding manual specification of these things makes the metadata annotation less tedious as well.&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Centralised Repository, Distributed Users&lt;/h1&gt;

&lt;p&gt;A key aspect of current approaches is the existence of a centralised data management system.  All the examples I consider include at least a metadata catalogue and some also include a data repository.  An additional feature sometimes exists for managing users permissions.&lt;/p&gt;

&lt;p&gt;The relationship between users and centralised services is a really complicated space, but essentially consists of the ability for users to create the documentation and push it (perhaps along with the data) to the metadata catalogue  and/or repository.  So given these assumptions I propose the following types of arrangement:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;user sends metadata to metadata catalogue&lt;/li&gt;
&lt;li&gt;user sends metadata and data to metadata catalogue and data repository&lt;/li&gt;
&lt;li&gt;user sends metadata and data and permissions information to metadata catalogue and data repository and permissions system.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The Case Studies I've identified that I want to explore are listed below, names follow the format 'client tool'-and-'data repository or metadata catalogue'-and-optionally-'permissions system':&lt;/p&gt;

&lt;h4&gt;Programmatic solutions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;reml-and-rfigshare&lt;/li&gt;
&lt;li&gt;reml-and-knb (when/if this becomes available)&lt;/li&gt;
&lt;li&gt;make_ddixml-and-ddiindex-and-orapus&lt;/li&gt;
&lt;li&gt;r2ddi-ddiindex&lt;/li&gt;
&lt;li&gt;dc-uploader-and-ANU-DataCommons&lt;/li&gt;
&lt;li&gt;dc-uploader-and-RDA&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Graphical User Interface solutions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;morpho-and-knb-metacat&lt;/li&gt;
&lt;li&gt;nesstar-publisher-and-nesstar-and-whatever-Steve-calls-the-ADA-permissions-system&lt;/li&gt;
&lt;li&gt;xmet-and-Australian-Spatial-Data-Directory&lt;/li&gt;
&lt;li&gt;sdmx-editor-and-sdmx-registry&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>wickhams-tidy-tools-only-get-you-90-pct-the-way</title>
   <link href="http://ivanhanigan.github.com/2013/10/wickhams-tidy-tools-only-get-you-90-pct-the-way/"/>
   <updated>2013-10-10T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/wickhams-tidy-tools-only-get-you-90-pct-the-way</id>
   <content type="html">&lt;h4&gt;Hadley Wickham's tidy tools&lt;/h4&gt;

&lt;p&gt;In this video at 8 mins 50 seconds he says &quot;these four tools do 90% of the job&quot;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;subset,&lt;/li&gt;
&lt;li&gt;transform,&lt;/li&gt;
&lt;li&gt;summarise, and&lt;/li&gt;
&lt;li&gt;arrange&lt;/li&gt;
&lt;li&gt;TODO I noticed &lt;a href=&quot;http://www.rstudio.com/training/curriculum/data-manipulation.html&quot;&gt;at the website for an Rstudio  course&lt;/a&gt; transform has been replaced by mutate as one of the &quot;four basic verbs of data manipulation&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;iframe src=&quot;//player.vimeo.com/video/33727555&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt; &lt;p&gt;&lt;a href=&quot;http://vimeo.com/33727555&quot;&gt;Tidy Data&lt;/a&gt; from &lt;a href=&quot;http://vimeo.com/user2150538&quot;&gt;Drew Conway&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;So I thought what's the other 10?  Here's a few contenders for my work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;merge&lt;/li&gt;
&lt;li&gt;reshape::cast and reshape::melt&lt;/li&gt;
&lt;li&gt;unlist&lt;/li&gt;
&lt;li&gt;t() transpose&lt;/li&gt;
&lt;li&gt;sprintf or paste&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;R-subset&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Filter rows by criteria
subset(airquality, Temp &amp;gt; 90, select = c(Ozone, Temp))

## NB This is a convenience function intended for use interactively.  For
## programming it is better to use the standard subsetting functions like
## ‘[’, and in particular the non-standard evaluation of argument
## ‘subset’ can have unanticipated consequences.

with(airquality,
     airquality[Temp &amp;gt; 90, c(&quot;Ozone&quot;, &quot;Temp&quot;)]
     )

# OR

airquality[airquality$Temp &amp;gt; 90,  c(&quot;Ozone&quot;, &quot;Temp&quot;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-transform&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# New columns that are functions of other columns       
df &amp;lt;- transform(airquality,
                new = -Ozone,
                Temp2 = (Temp-32)/1.8
                )
head(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-mutate&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(plyr)
# same thing as transform
df &amp;lt;- mutate(airquality, new = -Ozone, Temp = (Temp - 32) / 1.8)    
# Things transform can't do
df &amp;lt;- mutate(airquality, Temp = (Temp - 32) / 1.8, OzT = Ozone / Temp)

# mutate is rather faster than transform
system.time(transform(baseball, avg_ab = ab / g))
system.time(mutate(baseball, avg_ab = ab / g))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-summarise&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# New data.frame where columns are functions of existing columns
require(plyr)    
df &amp;lt;- ddply(.data = airquality,
            .variables = &quot;Month&quot;,
            .fun = summarise,
            tmax = max(Temp),
            tav = mean(Temp),
            ndays = length(unique(Day))
            )
head(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Passing variables to ddply for summary&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Notice how the name of the variable Temp doesn't need quotes?
# this means that you need to hard code the names
# But if you want to pass variables to this inside a function we need a
# different approach.

summarise_df  &amp;lt;- function(x, by, var1, var2, var3)
  {
    data_out &amp;lt;- ddply(x,
                      by,
                      function(df) return(
                        c(
                          tmax = max(df[,var1]),
                          tav = mean(df[,var2]),
                          ndays = length(unique(df[,var3]))
                          )
                        )
                      )
    return(data_out)
  }

df2 &amp;lt;- summarise_df(x = airquality, by = &quot;Month&quot;,
                   var1 = &quot;Temp&quot;, var2 = &quot;Temp&quot;, var3 = &quot;Day&quot;
                   )

head(df2)
all.equal(df,df2)
# TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Another alternative, if we want to pass the dataset as string too&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;summarise_df2  &amp;lt;- function(x, by, var1, var2, var3)
  {
    data_out &amp;lt;- eval(
      parse(
        text =
        sprintf(
          &quot;ddply(.data = %s,
            .variables = '%s',
            .fun = summarise,
            tmax = max(%s),
            tav = mean(%s),
            ndays = length(unique(%s))
            )&quot;, x, by, var1, var2, var3
          )
        )
      )
    return(data_out)
  }

df3 &amp;lt;- summarise_df2(x = &quot;airquality&quot;, by = &quot;Month&quot;,
                     var1 = &quot;Temp&quot;, var2 = &quot;Temp&quot;, var3 = &quot;Day&quot;
                     )
head(df3)
all.equal(df, df3)
# TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;R-arrange&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Re-order the rows of a data.frame
df &amp;lt;- arrange(airquality, Temp, Ozone)
head(df)
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>test-data-for-classification-trees</title>
   <link href="http://ivanhanigan.github.com/2013/10/test-data-for-classification-trees/"/>
   <updated>2013-10-10T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/test-data-for-classification-trees</id>
   <content type="html">&lt;h4&gt;A fictitious sample dataset&lt;/h4&gt;

&lt;p&gt;For discussion, I'll use a fictional example dataset that I'm using to work through some statistical theory related to Classification and Regression Trees (CART).
In the motivating example use case we are interested in predicting the civil status (married, single, divorced/widowed) of individuals from their sex (male, female) and sector of activity (primary, secondary, tertiary). The data set is composed of 273 cases.&lt;/p&gt;

&lt;p&gt;The data (and related statistical theory) come from:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ritschard, G. (2006). Computing and using the deviance with classification trees. In Compstat 2006 - Proceedings in Computational Statistics 17th Symposium Held in Rome, Italy, 2006. Retrieved from &lt;a href=&quot;http://mephisto.unige.ch/pub/publications/gr/ritschard_compstat06.pdf&quot;&gt;This Link&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ritschard, G., Pisetta, V., &amp;amp; Zighed, D. (2008). Inducing and evaluating classification trees with statistical implicative criteria. Statistical Implicative Analysis. Studies in Computational Intelligence Volume 127, pp 397-419. Retrieved from &lt;a href=&quot;http://mephisto.unige.ch/pub/publications/gr/ritsch-pisetta-zighed_bookGras_rev.pdf&quot;&gt;This Link&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# copy and paste the data from the PDF (Table 1 in both papers)
civst_gend_sector  &amp;lt;- read.csv(textConnection(
    &quot;civil_status gender activity_sector number_of_cases
         married   male         primary              50
         married   male       secondary              40
         married   male        tertiary               6
         married female         primary               0
         married female       secondary              14
         married female        tertiary              10
          single   male         primary               5
          single   male       secondary               5
          single   male        tertiary              12
          single female         primary              50
          single female       secondary              30
          single female        tertiary              18
divorced/widowed   male         primary               5
divorced/widowed   male       secondary               8
divorced/widowed   male        tertiary              10
divorced/widowed female         primary               6
divorced/widowed female       secondary               2
divorced/widowed female        tertiary               2
&quot;),sep = &quot;&quot;)

# save this to my personal R utilities package &quot;disentangle&quot; 
# for use later when I am exploring functions
dir.create(&quot;inst/extdata&quot;, recursive=T)
write.csv(civst_gend_sector, &quot;inst/extdata/civst_gend_sector.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;That is fine and good, we can use the case weights option to include number of cases but sometimes we want to use one row per person.
In the next chunk of code I;ll reformat the data, and also add another fictitious variable called income and contrive an example where a certain group earns less based on their activity sector.&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;df &amp;lt;- as.data.frame(matrix(NA, nrow = 0, ncol = 3))
for(i in 1:nrow(civst_gend_sector))
    {
    #    i &amp;lt;- 1
        n &amp;lt;- civst_gend_sector$number_of_cases[i]
        if(n == 0) next
        for(j in 1:n)
            {
              df &amp;lt;- rbind(df, civst_gend_sector[i,1:3])              
            }

    }

df$income  &amp;lt;- rnorm(nrow(df), 1000,200)
# Let us say secondary men earn less
df$income[df$gender == &quot;male&quot; &amp;amp; df$activity == &quot;secondary&quot;]  &amp;lt;- df$income[df$gender == &quot;male&quot; &amp;amp; df$activity == &quot;secondary&quot;] - 500
str(df)
# save this for use later
write.csv(df, &quot;inst/extdata/civst_gend_sector_full.csv&quot;, row.names = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Motivating reason for using these data&lt;/h4&gt;

&lt;p&gt;Classification and Regression Tree models (also referred to as Decision Trees) are one of the building blocks of data mining and a great tool for Exploratory Data Analysis.&lt;/p&gt;

&lt;p&gt;I've mostly used Regression Trees in the past but recently got some work with social science data where Classification Trees were needed.  I wanted to assess the deviance as well as the misclassification error rate for measuring the descriptive power of the tree.  While this is a easy with Regression Trees it became obvious that it was not so easy with Classification Trees.  This is because Classification Trees are most often evaluated by means of the error rate. The problem with the error rate is that it is not that helpful for assessing the descriptive capacity of the tree.&lt;/p&gt;

&lt;p&gt;For example if we look at the reduction in deviance between the Null model and the fitted tree we can say that the tree explains about XYZ% of the variation. We can also test if this is a statistically significant reduction based on a chi-squared test.&lt;/p&gt;

&lt;p&gt;Consider this example from page 310 of Hastie, T., Tibshirani, R., &amp;amp; Friedman, J. (2001). The elements of statistical learning. 2nd Edition:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;in a two-class problem with 400 observations in each class (denote this by (400, 400))&lt;/li&gt;
&lt;li&gt;suppose one split created nodes (300, 100) and (100, 300),&lt;/li&gt;
&lt;li&gt;the other created nodes (200, 400) and (200, 0).&lt;/li&gt;
&lt;li&gt;Both splits produce a misclassification rate of 0.25, but the second split produces a pure node and is probably preferable.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;During the course of my research to try to identify the best available method to implement in my analysis I found a useful series of papers by Ritschard, with a worked example using SPSS.  I hope to translate that to R in the future, but the first thing I did was grab the example data used in several of those papers out of the PDF.  So seeing as this was a public dataset (I use a lot of restricted data) and because I want to be able to use it to demonstrate the use of any R functions I find or write... I thought would publish it properly.&lt;/p&gt;

&lt;h4&gt;The Tree Model&lt;/h4&gt;

&lt;p&gt;So just before we leave Ritschard and the CART method, let's just fit the model.  Let's also install my R utilities package &quot;disentangle&quot;, to test that we can access the data from it.&lt;/p&gt;

&lt;p&gt;In this analysis the civil status is the outcome (or response or decision or dependent) variable, while sex and activity sector are the predictors (or condition or independent variables).&lt;/p&gt;

&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
require(rpart)
require(partykit) 
require(devtools)
install_github(&quot;disentangle&quot;, &quot;ivanhanigan&quot;)

# load
fpath &amp;lt;- system.file(file.path(&quot;extdata&quot;, &quot;civst_gend_sector.csv&quot;),
                     package = &quot;disentangle&quot;
                     )
civst_gend_sector &amp;lt;- read.csv(fpath)

# clean
str(civst_gend_sector)

# do
fit &amp;lt;- rpart(civil_status ~ gender + activity_sector,
             data = civst_gend_sector, weights = number_of_cases,
             control=rpart.control(minsplit=1))
# NB need minsplit to be adjusted for weights.
summary(fit)

# report
dir.create(&quot;images&quot;)
png(&quot;images/fit1.png&quot;, 1000, 480)
plot(as.party(fit))
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;The Result&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/fit1.png&quot; alt=&quot;fit1.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>simple-example-using-nmmaps</title>
   <link href="http://ivanhanigan.github.com/2013/10/simple-example-using-nmmaps/"/>
   <updated>2013-10-10T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/simple-example-using-nmmaps</id>
   <content type="html">&lt;p&gt;I will use the NMMAPSlite datasets for a simple example of what I am trying to do.&lt;/p&gt;

&lt;!-- begin_src R :session *R* :tangle NMMAPS-example/NMMAPS-example-code.r :exports none :eval no --&gt;


&lt;h4&gt;Code: get nmmaps data&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# func
if(!require(NMMAPSlite)) install.packages('NMMAPSlite');require(NMMAPSlite)
require(mgcv)
require(splines)

######################################################
# load  
setwd('data')
initDB('data/NMMAPS') # this requires that we connect to the web,
                      # so lets get local copies
setwd('..')
cities &amp;lt;- getMetaData('cities')
head(cities)
citieslist &amp;lt;- cities$cityname
# write out a few cities for access later
for(city_i in citieslist[sample(1:nrow(cities), 9)])
{
 city &amp;lt;- subset(cities, cityname == city_i)$city
 data &amp;lt;- readCity(city)
 write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
 row.names = F, sep = ',')
}
# these are all tiny, go some big ones
for(city_i in c('New York', 'Los Angeles', 'Madison', 'Boston'))
{
 city &amp;lt;- subset(cities, cityname == city_i)$city
 data &amp;lt;- readCity(city)
 write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
 row.names = F, sep = ',')
}

######################################################
# now we can use these locally
dir(&quot;data&quot;)
city &amp;lt;- &quot;Chicago&quot;
data &amp;lt;- read.csv(sprintf(&quot;data/%s.csv&quot;, city), header=T)
str(data)
data$yy &amp;lt;- substr(data$date,1,4)
data$date &amp;lt;- as.Date(data$date)
######################################################
# check
par(mfrow=c(2,1), mar=c(4,4,3,1))
with(subset(data[,c(1,15:25)], agecat == '75p'),
  plot(date, tmax)
 )
with(subset(data[,c(1,4,15:25)], agecat == '75p'),
        plot(date, cvd, type ='l', col = 'grey')
        )
with(subset(data[,c(1,4,15:25)], agecat == '75p'),
        lines(lowess(date, cvd, f = 0.015))
        )
# I am worried about that outlier
data$date[which(data$cvd &amp;gt; 100)]
# [1] &quot;1995-07-15&quot; &quot;1995-07-16&quot;

######################################################
# do standard NMMAPS timeseries poisson GAM model
numYears&amp;lt;-length(names(table(data$yy)))
df &amp;lt;- subset(data, agecat == '75p')
df$time &amp;lt;- as.numeric(df$date)
fit &amp;lt;- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = poisson)
# plot of response functions
par(mfrow=c(2,2))
plot(fit)
dev.off()

######################################################
# some diagnostics
summary(fit)
# note the R-sq.(adj) =   0.21
gam.check(fit)
# note the lack of a leverage plot.  for that we need glm

######################################################
# do same model as glm
fit2 &amp;lt;- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
# plot responses
par(mfrow=c(2,2))
termplot(fit2, se =T)
dev.off()

# plot prediction
df$predictedCvd &amp;lt;- predict(fit2, df, 'response')
# baseline is given by the intercept
fit3 &amp;lt;- glm(cvd ~ 1, data = df, family = poisson)
df$baseline &amp;lt;-  predict(fit3, df, 'response')
with(subset(df, date&amp;gt;=as.Date('1995-01-01') &amp;amp; date &amp;lt;= as.Date('1995-07-31')),
 plot(date, cvd, type ='l', col = 'grey')
        )
with(subset(df, date&amp;gt;=as.Date('1995-01-01') &amp;amp; date &amp;lt;= as.Date('1995-07-31')),
        lines(date,predictedCvd)
        )
with(subset(df, date&amp;gt;=as.Date('1995-01-01') &amp;amp; date &amp;lt;= as.Date('1995-07-31')),
 lines(date,baseline)
        )
######################################################
# some diagnostics
# need to load a function to calculate poisson adjusted R squared
# original S code from
# The formula for pseudo-R^2 is taken from G. S. Maddalla,
# Limited-dependent and Qualitative Variables in Econometrics, Cambridge:Cambridge Univ. Press, 1983. page 40, equation 2.50.
RsquaredGlm &amp;lt;- function(o) {
 n &amp;lt;- length(o$residuals)
 ll &amp;lt;- logLik(o)[1]
 ll_0 &amp;lt;- logLik(update(o,~1))[1]
 R2 &amp;lt;- (1 - exp(((-2*ll) - (-2*ll_0))/n))/(1 - exp( - (-2*ll_0)/n))
 names(R2) &amp;lt;- 'pseudo.Rsquared'
 R2
 }
RsquaredGlm(fit2)
# 0.51
# the difference is presumably due to the arguments about how to account for unexplainable variance in the poisson distribution?

# significance of spline terms
drop1(fit2, test='Chisq')
# also note AIC. best model includes all of these terms
# BIC can be computed instead (but still labelled AIC) using
drop1(fit2, test='Chisq', k = log(nrow(data)))

# diagnostic plots
par(mfrow=c(2,2))
plot(fit2)
dev.off()
# note high leverage plus residuals points are labelled
# leverage doesn't seem to be too high though which is good
# NB the numbers refer to the row.names attribute which still refer to the original dataset, not this subset
df[row.names(df) %in% c(9354,9356),]$date
# as suspected [1] &quot;1995-07-15&quot; &quot;1995-07-16&quot;

######################################################
# so lets re run without these obs
df2 &amp;lt;- df[!row.names(df) %in% c(9354,9356),]
# to avoid duplicating code just re run fit2, replacing data=df with df2
# tmax still significant but not so extreme
# check diagnostic plots again
par(mfrow=c(2,2))
plot(fit2)
dev.off()
# looks like a well behaved model now.

# if we were still worried about any high leverage values we could identify these with
df3 &amp;lt;- na.omit(df2[,c('cvd','pm10tmean','tmax','dptp','time')])
df3$hatvalue &amp;lt;- hatvalues(fit2)
df3$res &amp;lt;- residuals(fit2, 'pearson')
with(df3, plot(hatvalue, res))
# this is the same as the fourth default glm diagnostic plot, which they label x-axis as leverage
summary(df3$hatvalue)
# gives us an idea of the distribution of hat values
# decide on a threshold and look at it
hatThreshold &amp;lt;- 0.1
with(subset(df3, hatvalue &amp;gt; hatThreshold), points(hatvalue, res, col = 'red', pch = 16))
abline(0,0)
segments(hatThreshold,-2,hatThreshold,15)
dev.off()

fit3 &amp;lt;- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = subset(df3, hatvalue &amp;lt; hatThreshold), family = poisson)
par(mfrow=c(2,2))
termplot(fit3, se = T)
# same same
plot(fit3)
# no better

# or we could go nuts with a whole number of ways of estimating influence
# check all influential observations
infl &amp;lt;- influence.measures(fit2)
# which observations 'are' influential
inflk &amp;lt;- which(apply(infl$is.inf, 1, any))
length(inflk)


######################################################
# now what about serial autocorrelation in the residuals?

par(mfrow = c(2,1))
with(df3, acf(res))
with(df3, pacf(res))
dev.off()



######################################################
# just check for overdispersion
fit &amp;lt;- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = quasipoisson)
summary(fit)
# note the Scale est. = 1.1627
# alternatively check the glm
fit2 &amp;lt;- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = quasipoisson)
summary(fit2)
# (Dispersion parameter for quasipoisson family taken to be 1.222640)
# this is probably near enough to support a standard poisson model...

# if we have overdispersion we can use QAIC (A quasi- mode does not have a likelihood and so does not have an AIC,  by definition)
# we can use the poisson model and calculate the overdispersion
fit2 &amp;lt;- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
1- pchisq(deviance(fit2), df.residual(fit2))

# QAIC, c is the variance inflation factor, the ratio of the residual deviance of the global (most complicated) model to the residual degrees of freedom
c=deviance(fit2)/df.residual(fit2)
QAIC.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1)
QAIC.1

# Actually lets use QAICc which is more conservative about parameters,
QAICc.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1) + 2*(length(coef(fit2)) + 1)*(length(coef(fit2)) + 1 + 1)/(nrow(na.omit(df[,c('cvd','pm10tmean','tmax','dptp','time')]))- (length(coef(fit2))+1)-1)
QAICc.1


######################################################
# the following is old work, some may be interesting
# such as the use of sinusoidal wave instead of smooth function of time


# # sine wave
# timevar &amp;lt;- as.data.frame(names(table(df$date)))
# index &amp;lt;- 1:length(names(table(df$date)))
# timevar$time2 &amp;lt;- index / (length(index) / (length(index)/365.25))
# names(timevar) &amp;lt;- c('date','timevar')
# timevar$date &amp;lt;- as.Date(timevar$date)
# df &amp;lt;- merge(df,timevar)

# fit &amp;lt;- gam(cvd ~ s(tmax) + s(dptp) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
# summary(fit)
# par(mfrow=c(3,2))
# plot(fit, all.terms = T)
# dev.off()

# # now just explore the season fit
# fit &amp;lt;- gam(cvd ~ sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
# yhat &amp;lt;- predict(fit)
# head(yhat)

# with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(15,55)))
# lines(df[,'date'],exp(yhat),col='red')


# # drop1(fit, test= 'Chisq')

# # drop1 only works in glm?
# # fit with weather variables, use degrees of freedom estimated by gam
# fit &amp;lt;- glm(cvd ~ ns(tmax,8) + ns(dptp,2) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
# drop1(fit, test= 'Chisq')
# # use plot.glm for diagnostics
# par(mfrow=c(2,2))
# plot(fit)
# par(mfrow=c(3,2))
# termplot(fit, se=T)
# dev.off()

# # cyclic spline, overlay on prior sinusoidal
# with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(0,55)))
# lines(df[,'date'],exp(yhat),col='red')

# df$daynum &amp;lt;- as.numeric(format(df$date, &quot;%j&quot;))
# df[360:370,c('date','daynum')]
# fit &amp;lt;- gam(cvd ~ s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
# yhat2 &amp;lt;- predict(fit)
# head(yhat2)

# lines(df[,'date'],exp(yhat2),col='blue')


# par(mfrow=c(1,2))
# plot(fit)


# # fit weather with season
# fit &amp;lt;- gam(cvd ~ s(tmax) + s(dptp) +
  # s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
# par(mfrow=c(2,2))
# plot(fit)

# summary(fit)
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>reviewing-lessons</title>
   <link href="http://ivanhanigan.github.com/2013/10/reviewing-lessons/"/>
   <updated>2013-10-05T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/10/reviewing-lessons</id>
   <content type="html">&lt;h4&gt;Introduction&lt;/h4&gt;

&lt;p&gt;This post is an attempt to put together a standard framework for reviewing lessons.  I've been to numerous workshops, master classes, tutorials and lectures and always take ad hoc notes that I generally re-write and re-organise afterwards.  I hope to develop a clearer methodology for reviewing what I learnt in each lesson.&lt;/p&gt;

&lt;h4&gt;Context&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When:
Either just the date and time or include more context like the broader event such as a conference, season, public holidays.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Who:
The presenter will give a bio at the start so make notes, especially to identify what disciplinary perspective they come from.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Where:
Includes the address, city, lecture theatre.  These are cues for memory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why:
Both why is the presenter here talking and also why am I hear listening.  It is important to critically reflect on what I want to get out of this lesson.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What:
What is this lesson all about?  This might start with a synopsis overview and will probably move on to a sequence of notes as the presenter's narrative unfolds, and my thoughts on the topic evolve.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Three Stages: Thesis, Antithesis, Synthesis&lt;/h4&gt;

&lt;p&gt;A guiding principle I use for writing the 'What' section is the three stages: thesis, antithesis, synthesis.  I am not a philosopher so I don't know the proper use of these concepts in that discipline, but for me they are useful to structure my notes as I go through the process of a lesson.  Here is how I think of these stages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Thesis:
This is where I might pick out the key topics that are being presented, and write down my prior knowledge and preconceptions about the topic.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Antithesis:
What's the main message(s) of the presenter?  What are their priorities?  What secondary (surrogate) topics emerge around the main points?  If I bring questions to the lesson are they answered by the presenter?  If not why?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Synthesis:
My new understanding of the topic.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Other tools&lt;/h4&gt;

&lt;p&gt;Other things I use are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mind maps: a central topic with a spiderweb of links extending out in a circle.&lt;/li&gt;
&lt;li&gt;TODO&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>spatially-structured-timeseries-vs-spatiotemporal-modelling</title>
   <link href="http://ivanhanigan.github.com/2013/09/spatially-structured-timeseries-vs-spatiotemporal-modelling/"/>
   <updated>2013-09-26T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/spatially-structured-timeseries-vs-spatiotemporal-modelling</id>
   <content type="html">&lt;p&gt;&lt;head&gt;
&lt;title&gt;Spatiotemporal Regression Modelling&lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=iso-8859-1&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;Spatiotemporal Regression Modelling&quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-09-26T10:18+1000&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;script type=&quot;text/javascript&quot; src=&quot;http://orgmode.org/mathjax/MathJax.js&quot;&gt;
/**
 *
 * @source: http://orgmode.org/mathjax/MathJax.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 * Copyright (C) 2012-2013  MathJax
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 */

/*
@licstart  The following is the entire license notice for the
JavaScript code below.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code below is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code below.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: [&quot;MMLorHTML.js&quot;], jax: [&quot;input/TeX&quot;],
            jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;],
        extensions: [&quot;tex2jax.js&quot;,&quot;TeX/AMSmath.js&quot;,&quot;TeX/AMSsymbols.js&quot;,
                     &quot;TeX/noUndefined.js&quot;],
        tex2jax: {
            inlineMath: [ [&quot;\\(&quot;,&quot;\\)&quot;] ],
            displayMath: [ ['$$','$$'], [&quot;\\[&quot;,&quot;\\]&quot;], [&quot;\\begin{displaymath}&quot;,&quot;\\end{displaymath}&quot;] ],
            skipTags: [&quot;script&quot;,&quot;noscript&quot;,&quot;style&quot;,&quot;textarea&quot;,&quot;pre&quot;,&quot;code&quot;],
            ignoreClass: &quot;tex2jax_ignore&quot;,
            processEscapes: false,
            processEnvironments: true,
            preview: &quot;TeX&quot;
        },
        showProcessingMessages: true,
        displayAlign: &quot;center&quot;,
        displayIndent: &quot;2em&quot;,

        &quot;HTML-CSS&quot;: {
             scale: 100,
             availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;],
             preferredFont: &quot;TeX&quot;,
             webFont: &quot;TeX&quot;,
             imageFont: &quot;TeX&quot;,
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    &quot;MML&quot;,
                 Firefox: &quot;MML&quot;,
                 Opera:   &quot;HTML&quot;,
                 other:   &quot;HTML&quot;
             }
        }
    });
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Spatiotemporal Regression Modelling&lt;/h1&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Spatially Structured Timeseries Vs Spatiotemporal Modelling&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 Spatially Structured Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Spatiotemporal modelling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; Spatially Structured Timeseries Vs Spatiotemporal Modelling&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;In my last post about &lt;a href=&quot;http://ivanhanigan.github.io/2013/09/reflections-bob-haining-update&quot;&gt;spatiotemporal regression modelling&lt;/a&gt; I mentioned that I am mostly interested in &quot;spatially structured time-series models&quot; rather than spatial models at a single point in time. By this I mean that we have several neighbouring areal units observed over a period of time. In this framework the general methods of time series modelling are used to control for temporal autocorrelation. However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
&lt;/p&gt;
&lt;p&gt;
I want to expand more on this topic because I want to be clear that the organisation of the material I am aiming to bring to this notebook topic is not aimed at purely spatial regression models &lt;a href=&quot;https://geodacenter.asu.edu/spatial-lag-and&quot;&gt;(there is a lot of material and tools out there already for that)&lt;/a&gt;.  I am trying with these notes to document my learning steps toward integrating spatial methods with time-series methods to allow me to practice (and understand) spatiotemporal regression modelling.
&lt;/p&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1&lt;/span&gt; Spatially Structured Time Series&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1&quot;&gt;

&lt;p&gt;In &lt;a href=&quot;http://www.pnas.org/content/early/2012/08/08/1112965109.full.pdf+html&quot;&gt;my most successful previous attempt to conduct a spatiotemporal analysis of Suicide and Droughts&lt;/a&gt; I built on my knowledge of time-series regression models from single-city air pollution studies where the whole city is the unit of analysis and the temporal variation is modelled with controlling techniques for temporal autocorrelation.  These techniques are also valid for multi-city studies because it is pretty safe to assume the cities are all independent at each time point.  I structured my study by Eleven large zones (Census Statistical Divisions) of NSW and assumed each of these would vary over time independent of each other, and I fitted a zone-specific time trend and cycle. This is what I call &quot;spatially structured time-series&quot; modelling.  
&lt;/p&gt;
&lt;p&gt;
I justify using this model in this case because aggregating up to these very large regions will diminish the possibility of spatial autocorrelation and because Droughts vary over large spatial zones too, we will not suffer from exposure misclassificaiton bias.
&lt;/p&gt;
&lt;p&gt;
So this model is a simple time-series regression (with trend and seasonality) and an additional term for spatial Zone.
&lt;/p&gt;


\begin{eqnarray*}
        log({\color{red} O_{ijk}})  &amp; = &amp; s({\color{red}ExposureVariable})  + {\color{blue} OtherExplanators}  \\
        &amp; &amp;   + AgeGroup_{i} + Sex_{j} \\
        &amp; &amp;   + {\color{blue} SpatialZone_{k}}  \\
        &amp; &amp;  + sin(Time \times 2 \times \pi) + cos(Time \times 2 \times \pi) \\
        &amp; &amp;  + Trend \\
        &amp; &amp;   + offset({\color{blue} log(Pop_{ijk})})\\
\end{eqnarray*}

&lt;p&gt;
Where:&lt;br/&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\({\color{red}O_{ijk}}\) = Outcome (counts) by Age\(_{i}\), Sex\(_{j}\) and SpatialZone\(_{k}\) &lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;{\color{red}ExposureVariable} = Data with {\color{red}Restrictive Intellectual Property~(IP)} &lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;{\color{blue}OtherExplanators} = Other {\color{blue}Less Restricted}  Explanatory variables &lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;s( ) = penalized regression splines &lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;\({\color{blue} SpatialZone_{k}}\)  = {\color{blue} Less Restricted} data representing the \(SpatialZone_{k}\)  &lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;Trend = Longterm smooth trend(s) &lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;\({\color{blue}Pop_{ijk}}\) = interpolated Census populations, by time in each group&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2&lt;/span&gt; &lt;span class=&quot;todo TODO&quot;&gt;TODO&lt;/span&gt; Spatiotemporal modelling&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2&quot;&gt;

&lt;p&gt;In contrast to the above model for modelling exposures that have fine resolution spatial variation (such as air pollution) the exposure misclassification effect of aggregating up to very large spatial zones will conteract the benefits of avoiding spatially autocorrelated errors and this might be unacceptable for certain research questions.  Therefore it is important to move toward a spatiotemporal regression model that replaces the \(SpatialZone_{k}\) term with a more spatial error or spatial lag approach.
&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>reflections-bob-haining-update</title>
   <link href="http://ivanhanigan.github.com/2013/09/reflections-bob-haining-update/"/>
   <updated>2013-09-25T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/reflections-bob-haining-update</id>
   <content type="html">&lt;!-- &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; --&gt;


&lt;!-- &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Strict//EN&quot; --&gt;


&lt;!--                &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd&quot;&gt; --&gt;


&lt;!-- &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;en&quot; xml:lang=&quot;en&quot;&gt; --&gt;


&lt;p&gt;&lt;head&gt;&lt;/p&gt;

&lt;!-- &lt;title&gt;spatiotemporal &lt;/title&gt; --&gt;


&lt;p&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;spatiotemporal &quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-09-25T14:46+1000&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;script type=&quot;text/javascript&quot; src=&quot;http://orgmode.org/mathjax/MathJax.js&quot;&gt;
/**
 *
 * @source: http://orgmode.org/mathjax/MathJax.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 * Copyright (C) 2012-2013  MathJax
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 */

/*
@licstart  The following is the entire license notice for the
JavaScript code below.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code below is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code below.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: [&quot;MMLorHTML.js&quot;], jax: [&quot;input/TeX&quot;],
            jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;],
        extensions: [&quot;tex2jax.js&quot;,&quot;TeX/AMSmath.js&quot;,&quot;TeX/AMSsymbols.js&quot;,
                     &quot;TeX/noUndefined.js&quot;],
        tex2jax: {
            inlineMath: [ [&quot;\\(&quot;,&quot;\\)&quot;] ],
            displayMath: [ ['$$','$$'], [&quot;\\[&quot;,&quot;\\]&quot;], [&quot;\\begin{displaymath}&quot;,&quot;\\end{displaymath}&quot;] ],
            skipTags: [&quot;script&quot;,&quot;noscript&quot;,&quot;style&quot;,&quot;textarea&quot;,&quot;pre&quot;,&quot;code&quot;],
            ignoreClass: &quot;tex2jax_ignore&quot;,
            processEscapes: false,
            processEnvironments: true,
            preview: &quot;TeX&quot;
        },
        showProcessingMessages: true,
        displayAlign: &quot;center&quot;,
        displayIndent: &quot;2em&quot;,

        &quot;HTML-CSS&quot;: {
             scale: 100,
             availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;],
             preferredFont: &quot;TeX&quot;,
             webFont: &quot;TeX&quot;,
             imageFont: &quot;TeX&quot;,
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    &quot;MML&quot;,
                 Firefox: &quot;MML&quot;,
                 Opera:   &quot;HTML&quot;,
                 other:   &quot;HTML&quot;
             }
        }
    });
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;!-- &lt;h1 class=&quot;title&quot;&gt;spatiotemporal &lt;/h1&gt; --&gt;


&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Update on reflections from Bob Haining's Lecture&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-1&quot;&gt;1.1 CART Tree analysis that addresses the (potential)spatial autocorrelation problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-2&quot;&gt;1.2 Modeling with control for spatial autocorrelation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-3&quot;&gt;1.3 The Spatial Error Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-4&quot;&gt;1.4 The Spatial Lag Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-5&quot;&gt;1.5 Spatially Lagged Independent Variable(s)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1-6&quot;&gt;1.6 Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;1&lt;/span&gt; Update on reflections from Bob Haining's Lecture&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;&lt;a href=&quot;http://ivanhanigan.github.io/2013/04/reflections-bob-haining/&quot;&gt;Earlier this year&lt;/a&gt; Prof Bob Haining from the Geography Department Cambridge visited and gave us a great lecture on spatial regression.
&lt;/p&gt;
&lt;p&gt;
This Tuesday at the &lt;a href=&quot;http://gis-forum.github.io&quot;&gt;GIS Forum&lt;/a&gt; we were lucky to be joined by statistician Phil Kokic from CSIRO who had heard we'd be discussing spatial autocorrelation (Phil is my PhD supervisor). Here are some quick notes I made:
&lt;/p&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-1&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-1&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.1&lt;/span&gt; CART Tree analysis that addresses the (potential)spatial autocorrelation problem&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-1&quot;&gt;

&lt;p&gt;We started off the discussion with an assessment of the approach described in this post &lt;a href=&quot;http://thebiobucket.blogspot.com.au/2012/03/classification-trees-allowing-for.html&quot;&gt;Classification Trees and Spatial Autocorrelation&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
I've been thinking more and more about decision trees/CART/random forest methods for selecting a subset of relevant variables (and interations) for use in GLM or GAM model construction.  In a perfect world I'd have data on the main predictor I wanted to model and enough data about all the relevant other predictors (especially confounding or modifying variables) to ensure I get a 'well behaved model'. But with all the data around and so many potentially plausible relationships one might choose to include we need a way to narrow down these to just include the most important covariates, confounders and interactions.  CART or some variation on it seems a good way to do this, but is prone to the potential problem of spatially correlated errors too.
&lt;/p&gt;
&lt;p&gt;
The idea from that blog post is:
&lt;/p&gt;
&lt;p&gt;
&quot;compute the classification tree, calculate residuals and use it for a Mantel-test and Mantel correlograms.
The Mantel correlograms test differrences in dissimilarities of
the residuals across several spatial distances and thus enable you to detect lag-distances where possible spatial autocorrelation vanishes.
&amp;hellip;If encounter autocorrelation&amp;hellip; try to use subsamples of the data avoiding resampling within the lag-distance..&quot;
&lt;/p&gt;
&lt;p&gt;
I think the workflow would be to
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fit the classification tree (Question: best to use all the data or with a sample like using cross-validation)
&lt;/li&gt;
&lt;li&gt;get the residuals and visually assess the lagged distances plot provided by the Mantel correlogram.  Decide on a threshold (Question: is there an objective way to do this?).
&lt;/li&gt;
&lt;li&gt;Sample from the data and select out from this sample only data from pairs with distances greater than the threshold (have to keep one out of each close pair or else we'd only be getting data from the sparsely sampled parts of our study region).
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
We all agreed this sounded OK, but only avoids the problem of spatial autocorrelation (and loses data).
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-2&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-2&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.2&lt;/span&gt; Modeling with control for spatial autocorrelation&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-2&quot;&gt;

&lt;p&gt;So we all agreed we'd prefer if our model can control for spatial autocorrelation.  I confessed that I'd always found the GeoBUGS tutorial and other tutorials about Bayesian methods for this very difficult and would really like a &quot;Simple&quot; way to make the problem go away.  So first we briefly reviewed Prof Hainings 3 equations again:
&lt;/p&gt;
&lt;p&gt;
NOTE: THE FOLLOWING IDEAS WORK BEST FOR AREAL DATA.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-3&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-3&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.3&lt;/span&gt; The Spatial Error Model&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-3&quot;&gt;




\(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \eta_{i}\)

&lt;p&gt;
Where:
&lt;/p&gt;
&lt;p&gt;
\(\eta_{i}\) = Spatially autocorrelated errors.
&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-4&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-4&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.4&lt;/span&gt; The Spatial Lag Model&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-4&quot;&gt;




\(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \rho(Neighbours Y_{ij}) + e_{i}\)

&lt;p&gt;
Where:
&lt;/p&gt;
&lt;p&gt;
\(\rho_(Neighbours Y_{ij})\) = is an additional explanatory variable which is the value of the dependent variable in neighbouring areas. 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-5&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-5&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.5&lt;/span&gt; Spatially Lagged Independent Variable(s)&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-5&quot;&gt;




\(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2L} X_{2ij} + e_{i}\)

&lt;p&gt;
Where:
&lt;/p&gt;
&lt;p&gt;
\(\beta_{2L} X_{2ij}\) = is the independent variable X2 that is spatially lagged.
&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-1-6&quot; class=&quot;outline-4&quot;&gt;
&lt;h4 id=&quot;sec-1-6&quot;&gt;&lt;span class=&quot;section-number-4&quot;&gt;1.6&lt;/span&gt; Discussion&lt;/h4&gt;
&lt;div class=&quot;outline-text-4&quot; id=&quot;text-1-6&quot;&gt;

&lt;ul&gt;
&lt;li&gt;Phil agreed with Bob that the spatial error model is the best, spatial lag model is OK and spatially lagged covariates not so great.
&lt;/li&gt;
&lt;li&gt;For spatial error model fitting Phil suggested looking at R packages spBayes and spTimer.
&lt;/li&gt;
&lt;li&gt;I pointed out that I am mostly interested in &quot;spatially structured time-series models&quot; rather than spatial models at a single point in time.  By this I mean that we have several neighbouring areal units observed over a period of time.  In this framework the general methods of time series modelling are used to control for temporal autocorrelation.  However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
&lt;/li&gt;
&lt;li&gt;I asked that if spatial lag is OK (and it seems easier to fit into the time-series model framework) how can I check to know if it has done the trick?  If this were purely a spatial model we could check for spatial autocorrelation in the residuals just as they described in the CART blog above, but here we have many maps we could make (one every time point), and our spatial autocorrelation measure would surely vary a lot over time.  SO would a simple way just be to asses the effect on the Standard Error on beta1 (our primary interest) and if it is bigger but still significant we can be reassured that our result isn't affected? Or perhaps we should assess the beta on the lagged variable, for instance is a significant p-value on the lagged Beta an indication that it is capturing the unmeasured spatial associations represented by the neighbourhood variable?  
&lt;/li&gt;
&lt;li&gt;If it hadn't done the trick Nerida pointed out this might be because the Neighbourhoods are actually not appropriately represented by the first order neighbours and therefore more neighbours could be included, like moving out several concentric circles to wider and wider neighbourhoods
&lt;/li&gt;
&lt;li&gt;Nasser and Phil pointed out that the lagged variable (the outcome in the neighbours) includes an element of the exposure variables, and said that it would be difficult to 'unpack' what that part of the model meant.
&lt;/li&gt;
&lt;li&gt;so it looks like there is no simple answer and spatial error model is still preferred.
&lt;/li&gt;
&lt;/ul&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>workflow-flowcharts-update</title>
   <link href="http://ivanhanigan.github.com/2013/09/workflow-flowcharts-update/"/>
   <updated>2013-09-23T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/workflow-flowcharts-update</id>
   <content type="html">&lt;h1&gt;Workflow flowcharts - Update&lt;/h1&gt;

&lt;p&gt;A while back I posted about &lt;a href=&quot;/2013/07/worflow-flowcharts/&quot;&gt;my work with the Rgraphviz toolbox&lt;/a&gt; toward a wrapper function that will allow me to track the connections between chunks of my code as I write it.
This update includes notes from a discussion I had with Keith about this.&lt;/p&gt;

&lt;h4&gt;The basic use case is described by this code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;require(disentangle)
# Decide on the first step and create the starting node of the flowchart 
nodes &amp;lt;- newnode(name=&quot;NAME&quot;, inputs=&quot;INPUT&quot;, newgraph=T)
# FirstStep.
# Comment: do a bit of work
NAME &amp;lt;- myWorkFunction(INPUT)
# Decide on next step
nodes &amp;lt;- newnode(name=&quot;OUTPUT&quot;, inputs=c(&quot;NAME&quot;,&quot;ANOTHER THING&quot;))
# Do the second step
OUTPUT &amp;lt;- mySecondFunction(NAME, ANOTHER_THING)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/workflow-flowchart1.png&quot; alt=&quot;workflow-flowchart1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So as the scripted workflow develops, so does the flow chart. The alternative I described previously is to make a list of all the steps in a Balzerian Filelist Table.&lt;/p&gt;

&lt;h4&gt;Keith's comments:&lt;/h4&gt;

&lt;p&gt;Keith gave me some great comments on that post.  I'll just jot them down here for now but will have to come back and re-write parts of my function and the descriptive blog post to address these at another time.&lt;/p&gt;

&lt;p&gt;First section:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I said that multiple datasets used in an analysis could be broken down into discrete data packages.  Keith asked if I thought they &quot;should be&quot;.&lt;/li&gt;
&lt;li&gt;questioned whether the data warehouse at my work really is &quot;Big Data&quot;&lt;/li&gt;
&lt;li&gt;I need to cite the graphviz and Rgraphviz software better&lt;/li&gt;
&lt;li&gt;the function currently requires users to speciify newgraph = T to start, and fails to do anything if &quot;nodes&quot; object doesn't exist.  This is uneccessary.  should make it create &quot;nodes&quot; if doesn't exist. Then the newgraph argument is only needed to delete an old graph and start again.&lt;/li&gt;
&lt;li&gt;is it required that any of these actually be new?  if not then &quot;newnode&quot; seems a poor name for the function&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;From section &quot;Code:adding nodes&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the line where output is the name is confusing.  Keith felt that output was not the name of the newnode (but it really is).&lt;/li&gt;
&lt;li&gt;the name of the nodes object is hardcoded to be &quot;nodes&quot;.  therefore don't need to give it as an argument&lt;/li&gt;
&lt;li&gt;proposed better syntac: newnode(name= &quot;ANOTHER THING&quot;, output = &quot;OUTPUT&quot;) which I think will also work?&lt;/li&gt;
&lt;li&gt;wouldn't it be great if links could be labelled? maybe better to focus on links rather than nodes? this seems more logical because links are transitions and where the action is, not at the nodes.&lt;/li&gt;
&lt;li&gt;it might be helpful to review common actions to see what graph structures each implies.&lt;/li&gt;
&lt;li&gt;RE lab-book from Chemistry: most people would not be familiar with how lab-books are written.&lt;/li&gt;
&lt;li&gt;can newnode build circuits/cycles?  can it add links between existing nodes?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Section after &quot;shows this result&quot;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;tablexyz -&gt; FileA -&gt; Input1 should be one step because fileA is not used again. ditto fileB.&lt;/li&gt;
&lt;li&gt;analysisResults is a different kind of thing. this is confusing.&lt;/li&gt;
&lt;li&gt;why do I have to type &quot;nodes &amp;lt;-&quot; every time?  if it has to have this name It could be hidden&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Under example two, &quot;Step three&quot;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;huh? does input1 have &quot;id&quot;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&quot;the result&quot; graph is  a bit dense and hard to follow.  Perhaps simpler labels?  ie instead of FileA just &quot;A&quot;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>introduction-to-ons-theory-and-practice</title>
   <link href="http://ivanhanigan.github.com/2013/09/introduction-to-ons-theory-and-practice/"/>
   <updated>2013-09-23T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/introduction-to-ons-theory-and-practice</id>
   <content type="html">&lt;p&gt;The reasons why ONS is so appealing to me can be broken into two parts.  The first is about the problems which it solves, the second is about the benefits it might bring.&lt;/p&gt;

&lt;h4&gt;The Problems&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Identifying errors (either from miscalculations or from methodological mistakes)&lt;/li&gt;
&lt;li&gt;Uncovering fraud&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;The Benefits&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Sharing interests and skills&lt;/li&gt;
&lt;li&gt;Quickly finding out about new discoveries and failures&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;These benefits come from the enhanced potential for theoretical discussions and sharing ideas.  This is especially valuable around difficult theories, unknown issues or esoteric theories that are known only to a specialist in a field.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>using-orgmode-and-jekyll-for-open-notebook</title>
   <link href="http://ivanhanigan.github.com/2013/09/using-orgmode-and-jekyll-for-open-notebook/"/>
   <updated>2013-09-22T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/using-orgmode-and-jekyll-for-open-notebook</id>
   <content type="html">&lt;h1&gt;Using Orgmode and Jekyll for Open Notebook&lt;/h1&gt;

&lt;p&gt;Orgmode is a great notebook tool because it allows the coding, evaluation and documentation all in one.  I also want to use it to send the documentation to my blog as an Open Notebook.&lt;/p&gt;

&lt;p&gt;If starting again I'd look into this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://orgmode.org/worg/org-tutorials/org-jekyll.html&quot;&gt;http://orgmode.org/worg/org-tutorials/org-jekyll.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;But as it is I already put a lot of work into configuring a jekyll blog I cloned from Scott Chamberlain over at ROpenSci and I will just use orgmode to publish the posts related to each project, tagged as 'categories'.&lt;/p&gt;

&lt;p&gt;But here is a problem I just found out how to solve.  For a long time I thought that because github disabled ruby plugins that the automatic generate categories index pages was broken.  Luckily Charlie Park has written up the following solution and this seems to have worked for me today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://charliepark.org/tags-in-jekyll/&quot;&gt;http://charliepark.org/tags-in-jekyll/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;http://charliepark.org/jekyll-with-plugins/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Cheers!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>transformational-adaptation</title>
   <link href="http://ivanhanigan.github.com/2013/09/transformational-adaptation/"/>
   <updated>2013-09-22T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/transformational-adaptation</id>
   <content type="html">&lt;p&gt;Energymark is about transformational adaptations as opposed to incremental adaptation..&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>toward-a-unified-ecology-dataset</title>
   <link href="http://ivanhanigan.github.com/2013/09/toward-a-unified-ecology-dataset/"/>
   <updated>2013-09-20T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/toward-a-unified-ecology-dataset</id>
   <content type="html">&lt;h1&gt;Ecology datasets&lt;/h1&gt;

&lt;p&gt;These are my notes from a meeting that Kathryn and I had with a group of Ecologists at the ANU (primarily Luciana Porforio and Nasreen Khan).  We asked them to discuss how they search for and use Ecology datasets, especially how to best package up the parts of an ecological field data collection (ie weather, vegetation, biodiversity, soils, topography etc).&lt;/p&gt;

&lt;p&gt;Lu started off the discussion by stating that the most important thing to acknowledge is that every ecologist will start off with a main research question and then search for data that will address their specific research question.  It is difficult to work from a 'top-down' perspective that hopes to pre-empt the range of possible questions.  Lu felt that it may therefore be best to just keep all the data together in the biggest bundle that is possible and the end user can pick it apart once downloaded.&lt;/p&gt;

&lt;p&gt;We explained that LTERN datasets can be quite expansive with many dimensions and it seemed preferable to at least untangle the main 'themes' for packaging up.&lt;/p&gt;

&lt;p&gt;Nasreen pointed out that there is always a protocol for how data are collected and this should give the data collection it's structure.  However I felt that ecology collections are so diverse they have been made (by necessity) very flexible and specific to the needs of the individual plot network.  Therefore generalisations across data collections are very hard to make (apart from easy things like &quot;weather&quot; or &quot;aboveground dead biomass&quot;).&lt;/p&gt;

&lt;h1&gt;Toward a Unified Ecology&lt;/h1&gt;

&lt;p&gt;I always fall back on the text book &quot;Toward a Unified Ecology: Timothy F. H. Allen, Thomas W. Hoekstra 1992&quot;.  I wondered if it can guide us?  On pages 42-53 they describe the following framework and use the image below (the letters in the middle disc correspond to the criteria ie O = Organism).  In this framework it is possible to summarise ANY ecological study as they ALWAYS incorporate these scale-independent criteria:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Organism: genetic integrity, discrete body, autonomy from other organisms&lt;/li&gt;
&lt;li&gt;Population: relative similarity within the group&lt;/li&gt;
&lt;li&gt;Community: inter-species competition, interference, mutualism&lt;/li&gt;
&lt;li&gt;Ecosystem: biotic and abiotic interactions&lt;/li&gt;
&lt;li&gt;Landscape: spatial structure/contiguity&lt;/li&gt;
&lt;li&gt;Biome: characteristic physiognomy, disturbance and climate&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I thought that if these dimensions were identified in a data collection first then they might become the discrete packages by which each plot network publishes their collection?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/datadoco-layercake.png&quot; alt=&quot;datadoco-layercake.png&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Aekos&lt;/h1&gt;

&lt;p&gt;Over at &lt;a href=&quot;http://www.aekos.org.au/why_aekos#diversity&quot;&gt;AEKOS&lt;/a&gt; they have a similar conceptual framework&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Observations can range from that of individual organisms and
interactions, through to populations, communities, ecosystems and
across broad global landscapes.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;This is an open issue.  More discussions are needed internally for the Data Custodians.&lt;/p&gt;

&lt;p&gt;Lu also pointed out that the end users are the key stakeholders and perhaps more input from them (via surveys and workshops?) is needed?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>energymark</title>
   <link href="http://ivanhanigan.github.com/2013/09/energymark/"/>
   <updated>2013-09-19T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/energymark</id>
   <content type="html">&lt;h1&gt;Energymark&lt;/h1&gt;
</content>
 </entry>
 
 <entry>
   <title>pumilio-bushfm-test-dev-prod</title>
   <link href="http://ivanhanigan.github.com/2013/09/pumilio-bushfm-test-dev-prod/"/>
   <updated>2013-09-18T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/pumilio-bushfm-test-dev-prod</id>
   <content type="html">&lt;h1&gt;Testing the pumilio-bushfm-test-dev-prod build process, in an Open Notebook&lt;/h1&gt;

&lt;h4&gt;Aims:&lt;/h4&gt;

&lt;p&gt;It was suggested I could document the pumilio test build as an OpenNotebook.
I imagined that I could link this blog to github repo and doco hosted on gh-pages.&lt;/p&gt;

&lt;h4&gt;Methods:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;For ie the emacs html output now goes to &lt;a href=&quot;http://ivanhanigan.github.io/pumilio-bushfm/&quot;&gt;http://ivanhanigan.github.io/pumilio-bushfm/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;then collaborators can clone/fork &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm&quot;&gt;https://github.com/ivanhanigan/pumilio-bushfm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and comment/complain at &lt;a href=&quot;https://github.com/ivanhanigan/pumilio-bushfm/wiki&quot;&gt;https://github.com/ivanhanigan/pumilio-bushfm/wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Results:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;I did a test build a month ago on an old laptop sitting around, then rebuilt on the Nectar cloud&lt;/li&gt;
&lt;li&gt;Unfortunately I didn't realise that the Nectar VM had mounted /var on to the smaller root partition (the 40GB 2nd disk is on /mnt).&lt;/li&gt;
&lt;li&gt;then when I tried to upload a big sound file it broke :-(&lt;/li&gt;
&lt;li&gt;I did a bit of reading and whilst I began thinking I'd just need to move the mysql datadir via&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Code:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;sudo nano /etc/mysql/my.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;


&lt;h4&gt;BUT&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;it actually looks like there is a WAV and MP3 file under /var/www/pumilio-2...&lt;/li&gt;
&lt;li&gt;so I think I can &lt;a href=&quot;http://askubuntu.com/questions/39536/how-can-i-store-var-on-a-separate-partition&quot;&gt;just remount /var&lt;/a&gt; onto the larger /mnt secondary disk.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Starting my Open Notebook Science Blog</title>
   <link href="http://ivanhanigan.github.com/2013/09/ons/"/>
   <updated>2013-09-13T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/09/ons</id>
   <content type="html">&lt;p&gt;Many examples are emerging of scientists who are transitioning to a
much more open model of research.  This is in part externally driven
by funding bodies (such as the Aussie Research Council asking for deposit of funded data and papers) and journals
(&lt;a href=&quot;http://www.nature.com/ng/journal/v45/n5/full/ng.2621.html&quot;&gt;ie. Nature journals removing length restrictions on Methods sections.&lt;/a&gt;). Also the increased value being placed on transparency of reproducible analysis to safeguard against error and fraud is becoming an internal driver within science communities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Open_Notebook_Science&quot;&gt;Open Notebook Science&lt;/a&gt;
(ONS) style is an extreme of transparent approaches to research.
According to the wikipedia page it is the &quot;practice of making the
entire primary record of a research project publicly available online
as it is recorded&quot;.&lt;/p&gt;

&lt;p&gt;That's pretty extreme!  In my view a lot of stuff in the research project should probably be archived quickly and left to rot.&lt;/p&gt;

&lt;p&gt;I like the range of options available.  I think I'll go for &lt;a href=&quot;http://onsclaims.wikispaces.com/&quot;&gt;SCD or &quot;Seclected Content / Delayed&quot;&lt;/a&gt; and show their image below.  In this model a portion of the open notebook and associated supporting raw data are available after some delay. I'll try to use this blog for weekly updates on progress for each project, and provide links off my 'Open Notebook' and 'Software' Tabs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ONS-SCD.png&quot; alt=&quot;ONS-SCD.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Worflow flowcharts</title>
   <link href="http://ivanhanigan.github.com/2013/07/worflow-flowcharts/"/>
   <updated>2013-07-31T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/07/worflow-flowcharts</id>
   <content type="html">&lt;h2&gt;What is the issue&lt;/h2&gt;

&lt;p&gt;Most people seem to collect multiple datasets together in a single spot that can be split into 2 or more separate data packages.  I think this is a natural set up from an analysts perspective, where the results of multiple steps accumulate as 'stepping stones' toward the file they end up analysing.&lt;/p&gt;

&lt;p&gt;I was first taught GIS by Isabelle Balzer at Ecowise Environmental Services in Canberra.  She showed me the method of keeping a table (sticky-taped to the desk!) of all the files and transformations that were going on. This was a method that didn't allow any multitasking!  I call this the 'Balzerian Method' (I am sure others used it before Isabelle, but I think Balzerian is a great word).&lt;/p&gt;

&lt;p&gt;I think the data wharehouse at my work is an example, and probably we'll find the key challenge for big data will be for analysts to disentangle their own filing systems.&lt;/p&gt;

&lt;p&gt;In my experience the way people store research data is often one (or a couple, or all) of these three types:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a database with heaps of tables and views&lt;/li&gt;
&lt;li&gt;a directory (and sub-directories) with heaps of files&lt;/li&gt;
&lt;li&gt;a spreadsheet workbook with heaps of sheets (and links to other workbooks)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I am developing a tool based on the open source graphviz softawre. The tool I am developing addresses the challenge of graphing the links between these sequential steps.&lt;/p&gt;

&lt;h4&gt;Code:introducing newnode&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# NB this only works easily on linux
require(devtools)
install_github(&quot;disentangle&quot;, &quot;ivanhanigan&quot;)
require(disentangle)
# the core of the tool is Rgraphviz, I just built a wrapper function
# to add newnodes to a graph of nodes
# always start with (newgraph = T) because the newnode function ADDS
# nodes to a graph, unless told otherwise, and fails if no 'nodes'
# object exists
nodes  &amp;lt;- newnode(name=&quot;NAME&quot;,inputs=&quot;INPUT&quot;,outputs=&quot;OUTPUT&quot;, newgraph = T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/newnode1.png&quot; alt=&quot;images/newnode1.png&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Code:adding nodes&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# now we can add nodes, and we can pass multiple inputs or outputs
nodes  &amp;lt;- newnode(name=&quot;OUTPUT&quot;,inputs=c(&quot;NAME&quot;,&quot;ANOTHER THING&quot;))
# outputs are optional
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/newnode2.png&quot; alt=&quot;images/newnode2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It can be used in two or three ways.&lt;/p&gt;

&lt;h2&gt;Example one, the composite view:&lt;/h2&gt;

&lt;p&gt;So if there is a Balzerian filelist table available, convert it to a spreadsheet.  This is als similar to a labbook from Chemistry but follows a very rigid structure: NAME,        INPUTS,           OUTPUTS,         DESCRIPTION.  The first method I'll show will take one of these tables and map out the steps in the workflow.&lt;/p&gt;

&lt;h4&gt;Code: Composite Worflow Files List&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#    so if there is a Balzerian filelist table available,
# either make a spreadsheet with names, inputs and outputs 
# fileslist &amp;lt;- read.csv(&quot;exampleFilesList.csv&quot;, stringsAsFactors = F)
# or 
filesList &amp;lt;- read.csv(textConnection(
'NAME,        INPUTS,           OUTPUTS,         DESCRIPTION
FileA,        TableXYZ,         Input1,          Transformed variable
FileB,        TableABC,         Input2,          Collapsed dimensions
analysisFile, &quot;Input1,Input2&quot;,  analysisResults, Merged inputs and analysed
'), stringsAsFactors = F, strip.white = T)
filesList

for(i in 1:nrow(filesList))
{
  nodes &amp;lt;- newnode(name = filesList[i,&quot;NAME&quot;],
                   inputs = strsplit(filesList$INPUTS, &quot;,&quot;)[[i]],
                   outputs = strsplit(filesList$OUTPUTS, &quot;,&quot;)[[i]],
                   newgraph = (i == 1)
  )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;shows this result&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/fileRelationships.png&quot; alt=&quot;fileRelationships.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Example two, tracking the steps while analysing data:&lt;/h2&gt;

&lt;p&gt;Structure a script into sections and document each section before evaluating the code to execute the step.  This works well with orgmode/ESS, Sweave or knitr style workflows.
For example:&lt;/p&gt;

&lt;h4&gt;Code: Ad Hoc Files Lists Flowcharts&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#### step one ####
nodes &amp;lt;- newnode(name=&quot;FileA&quot;, inputs=&quot;TableXYZ&quot;, outputs=&quot;Input1&quot;,
                 newgraph =T) # this is required to tell newnode to
                              # start a new graph, rather than add to
                              # the nodes
FileA  &amp;lt;- read.table(&quot;TableXYZ.txt&quot;)
Input1 &amp;lt;- log(FileA$columnZ)

#### step two ####
nodes &amp;lt;- newnode(name=&quot;FileB&quot;, inputs=&quot;TableABC&quot;, outputs=&quot;Input2&quot;)
FileB  &amp;lt;- read.table(&quot;TableABC.txt&quot;)
Input2 &amp;lt;- ddply(FileB, &quot;id&quot;, summarise,
                duration = max(year) - min(year),
                nteams = length(unique(team)))

#### step three ####
nodes &amp;lt;- newnode(name=&quot;analysisFile&quot;, inputs=c(&quot;Input1&quot;,&quot;Input2&quot;),
                 outputs=&quot;analysisResults&quot;)
analysisFile  &amp;lt;- merge(Input1, Input2, by=&quot;id&quot;)
analysisResults  &amp;lt;- lm(y ~ duration + nteams, data = analysisFile)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Example three: visualising relationships&lt;/h2&gt;

&lt;p&gt;It is not aimed at visualising the linked structure of a tree or semi-lattice but can be used in such a way but changing the nodename and inputs concept to parent/child relationships.&lt;/p&gt;

&lt;p&gt;As an example I'll describe how a list of database tables might be displayed as a tree. I am a great fan of Josh Reich due to his &lt;a href=&quot;http://stackoverflow.com/a/1434424&quot;&gt;LCFD workflow&lt;/a&gt;, and I also like his work on the &lt;a href=&quot;https://www.simple.com/&quot;&gt;Simple Bank&lt;/a&gt; so when I stumbled on this &lt;a href=&quot;http://blog.i2pi.com/post/52812976752/joshs-postgresql-database-conventions&quot;&gt;blog post&lt;/a&gt; in which he says:&lt;/p&gt;

&lt;p&gt;&quot;Show me your flowchart and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won’t usually need your flowchart; it’ll be obvious.&quot;&lt;/p&gt;

&lt;p&gt;I was switched on and I started thinking about how the graphVis tool could be used to describe a list of tables and views from a database.&lt;/p&gt;

&lt;p&gt;Say that two groups studied the same file TableXYZ with different inputs.  One of these groups wrote a seminal paper in the field, while their rivals wrote an inferior paper with a different result.  Imagine now a subsequent group who gathered the data from the previous work into the following database tables and conducted a replication study, with a new sensitivity analysis to explain why the original two papers produced different results.&lt;/p&gt;

&lt;p&gt;Let's assume this database has all the data from all the groups in it and we want to get a pictorial view so we can disentangle which files belong to which study.  First get the following list of tables as INPUTS, grouping them by 'NAME' will give the tree structure and showing their results as OUTPUTS allows the subsequent replication study to use them as inputs and assume the position at the bottom of the flowchart.&lt;/p&gt;

&lt;h4&gt;Code: database tables and different studies&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;filesList &amp;lt;- read.csv(textConnection(
'NAME                 ,             INPUTS         , OUTPUTS
The Seminal Study     ,              FileA         , 
The Seminal Study     ,              FileB         , 
The Seminal Study     ,       analysisFile         , 
The Seminal Study     ,           TableXYZ         , 
The Seminal Study     ,           TableABC         , 
The Seminal Study     ,      Input1,Input2         ,
The Seminal Study     ,             Input1         , 
The Seminal Study     ,             Input2         , 
The Seminal Study     ,      The Seminal Study     , analysisResults 
The Inferior Rivals   ,                FileC       , 
The Inferior Rivals   ,        analysisFileX       , 
The Inferior Rivals   ,             TableXYZ       , 
The Inferior Rivals   ,               InputX       , 
The Inferior Rivals   ,    The Inferior Rivals     , analysisResultsX       
The Replication Study ,    &quot;Input1,Input2,TableXYZ&quot;,  analysisResultsR     
The Replication Study ,    &quot;Input1,InputX,TableXYZ&quot;,  sensitivityResult 
'), stringsAsFactors = F, strip.white = T)

for(i in 1:nrow(filesList))
{
  nodes &amp;lt;- newnode(name = filesList[i,&quot;NAME&quot;],
                   inputs = strsplit(filesList$INPUTS, &quot;,&quot;)[[i]],
                   outputs = strsplit(filesList$OUTPUTS, &quot;,&quot;)[[i]],
                   newgraph = (i == 1)
  )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;the result&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/filesRelationships2.png&quot; alt=&quot;filesRelationships2.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>animated-maps</title>
   <link href="http://ivanhanigan.github.com/2013/07/animated-maps/"/>
   <updated>2013-07-30T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/07/animated-maps</id>
   <content type="html">&lt;h1&gt;Animated maps to allow exploration of alternate levels of 'jitter'&lt;/h1&gt;

&lt;p&gt;In a &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22672028&quot;&gt;previous project&lt;/a&gt; we published a map of point locations that had been 'jittered', ie adding random noise to the latitude and longitude.  We did this by testing out a few maps and deciding on one that we thought protected privacy adequately whilst not destroying the spatial pattern we wished to display (evocatively).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/jitter/Figure%202_FINAL.jpg&quot; alt=&quot;Figure 2_FINAL.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I always wondered about a way to interactively do this and I think the animation package might do the trick, with the ability to step thru levels of jittering with the pause, fwd and back buttons.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/jitter/index.html&quot;&gt;Clink here for the same data shown in a new animation&lt;/a&gt;.&lt;/p&gt;

&lt;h1&gt;Reference&lt;/h1&gt;

&lt;p&gt;Vally, H., Peel, M., Dowse, G. K., Cameron, S., Codde, J. P., Hanigan, I., &amp;amp; Lindsay, M. D. a. (2012). Geographic Information Systems used to describe the link between the risk of Ross River virus infection and proximity to the Leschenault estuary, WA. Australian and New Zealand Journal of Public Health, 36(3), 229–235. doi:10.1111/j.1753-6405.2012.00869.x&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>spatio-temporal-animations</title>
   <link href="http://ivanhanigan.github.com/2013/06/spatio-temporal-animations/"/>
   <updated>2013-06-06T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/06/spatio-temporal-animations</id>
   <content type="html">&lt;h2&gt;Space time animations are cool&lt;/h2&gt;

&lt;p&gt;This is a graphic I made a few years ago of temperature in Sydney.  It is a GAM with smoothing splines on longitude, latitude and time (t):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;require(mgcv)
fit &amp;lt;- gam(temp ~ te(lon,lat,t,d=c(2,1),bs=c(&quot;tp&quot;,&quot;cr&quot;)), data=jan06)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/button/spacetimegamSydney3hrTemp.gif&quot; alt=&quot;spacetimegamSydney3hrTemp.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/button/index.html&quot;&gt;Clink here for an example of where I think this kind of animation should go now&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The addition of stop/play/fwd/reverse buttons adds potential for exploratory insights.&lt;/p&gt;

&lt;p&gt;The secret to getting the play/pause/next buttons is to insert graphing code between:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;saveHTML(..., outdir = getwd())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately the rest of the code that accompanies the graphic above is too specific to my workflow that it is not reproducible.  That was back before (during the time that) I became obsessed with the prospect of &lt;a href=&quot;http://swish-climate-impact-assessment.github.io/&quot;&gt;creating reproducible data analysis workflows&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Complex Model Selection (or Data Dredging)</title>
   <link href="http://ivanhanigan.github.com/2013/05/data-dredging/"/>
   <updated>2013-05-09T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/05/data-dredging</id>
   <content type="html">&lt;p&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;en&quot; xml:lang=&quot;en&quot;&gt;
&lt;head&gt;&lt;/p&gt;

&lt;!--&lt;title&gt;data dredging&lt;/title&gt;--&gt;


&lt;p&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;data dredging&quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-05-09 &quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;ivan hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;script type=&quot;text/javascript&quot; src=&quot;http://orgmode.org/mathjax/MathJax.js&quot;&gt;
/**
 *
 * @source: http://orgmode.org/mathjax/MathJax.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 * Copyright (C) 2012-2013  MathJax
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 */

/*
@licstart  The following is the entire license notice for the
JavaScript code below.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code below is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code below.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: [&quot;MMLorHTML.js&quot;], jax: [&quot;input/TeX&quot;],
            jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;],
        extensions: [&quot;tex2jax.js&quot;,&quot;TeX/AMSmath.js&quot;,&quot;TeX/AMSsymbols.js&quot;,
                     &quot;TeX/noUndefined.js&quot;],
        tex2jax: {
            inlineMath: [ [&quot;\\(&quot;,&quot;\\)&quot;] ],
            displayMath: [ ['$$','$$'], [&quot;\\[&quot;,&quot;\\]&quot;], [&quot;\\begin{displaymath}&quot;,&quot;\\end{displaymath}&quot;] ],
            skipTags: [&quot;script&quot;,&quot;noscript&quot;,&quot;style&quot;,&quot;textarea&quot;,&quot;pre&quot;,&quot;code&quot;],
            ignoreClass: &quot;tex2jax_ignore&quot;,
            processEscapes: false,
            processEnvironments: true,
            preview: &quot;TeX&quot;
        },
        showProcessingMessages: true,
        displayAlign: &quot;center&quot;,
        displayIndent: &quot;2em&quot;,

        &quot;HTML-CSS&quot;: {
             scale: 100,
             availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;],
             preferredFont: &quot;TeX&quot;,
             webFont: &quot;TeX&quot;,
             imageFont: &quot;TeX&quot;,
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    &quot;MML&quot;,
                 Firefox: &quot;MML&quot;,
                 Opera:   &quot;HTML&quot;,
                 other:   &quot;HTML&quot;
             }
        }
    });
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;Toward Automated Model Selection&lt;/h1&gt;

&lt;!--
&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Data Dredging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 Compulsory inclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3&quot;&gt;3 AIC vs BIC vs LRTests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
--&gt;
&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;!--&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; Data Dredging&lt;/h2&gt;--&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;&lt;a href=&quot;http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did-environmental-factors-influencing-the-velocity-of-blueton&quot;&gt;The Bluetongue paper&lt;/a&gt; we've been discussing at the &lt;a href=&quot;http://gis-forum.github.io/study.html&quot;&gt;ANU GIS forum&lt;/a&gt;  correctly points out that with the &quot;the large number of candidate variables &amp;hellip; a huge number of models could be considered.&quot;
&lt;/p&gt;
&lt;p&gt;
They go on to say that:
&lt;/p&gt;
&lt;p&gt;
&quot;Thus, for practical reasons, we &amp;hellip; isolate independently for each of  three thematic sets of variables (host-, meteorological- and landscape-related covariates) a combination of variables best fitting the data.&quot;
&lt;/p&gt;
&lt;p&gt;
I don't really get this.  Why not fit the huge number of models (RAM and disk speed permitting) and let AIC or BIC  sift out any combinations that perform well?  For example in a simple instance with four explanatory variables and no interactions the rich model would be:
&lt;/p&gt;


\(Y_{i} = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \beta_{3} X_{3} + \beta_{4} X_{4}\)

&lt;p&gt;
Lu, Sonya and I are working on a function to do all the possible combos (interaction terms are possible to include too).  So far we have this code and a link to an old &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2007-January/124023.html&quot;&gt;Hadley Wickham quote&lt;/a&gt;.
That's from 2007, and still haven't herd about any tool developed that really does this. 
&lt;/p&gt;
&lt;p&gt;
So far we have this code:
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #268bd2;&quot;&gt;combos&lt;/span&gt;  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;function&lt;/span&gt;(yvar, xvars, compulsory = &lt;span style=&quot;color: #b58900;&quot;&gt;NA&lt;/span&gt;)
  {
    formlas &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #b58900;&quot;&gt;NULL&lt;/span&gt;
    &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(j &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; length(xvars):1)
      {
        combns &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; combn(xvars, j)
        &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:ncol(combns))
          {
            terms2include &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; combns[,i]
            &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!is.na(compulsory[1]))
              {
                terms2include  &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(terms2include, compulsory)
              }
            formla &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; reformulate(terms2include,                                  
                                  response = yvar
                                  )
            formlas &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; c(formlas,formla)     
          }
      }
    &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;return&lt;/span&gt;(formlas)
  }
&lt;/pre&gt;


&lt;p&gt;
The resulting list of candidate models are:
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;formlas &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; combos(yvar = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;deaths&quot;&lt;/span&gt;,
                  xvars = c(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x1&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x2&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x3&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x4&quot;&lt;/span&gt;)
                  )
paste(formlas)

&lt;/pre&gt;



&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2 + x3 + x4&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2 + x3&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2 + x4&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x3 + x4&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2 + x3 + x4&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x3&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x4&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2 + x3&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2 + x4&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x3 + x4&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x3&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x4&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;2&lt;/span&gt; Compulsory inclusions&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-2&quot;&gt;

&lt;p&gt;In some instances you may want to include a variable in all models so the compulsory option can be used.  For example in spatio-temporal models we could include a term for zone and time while assessing the mix of explanatory variables:
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;formlas &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; combos(yvar = &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;deaths&quot;&lt;/span&gt;,
                  xvars = c(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x1&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x2&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x3&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;x4&quot;&lt;/span&gt;),
                  compulsory = c(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;zone&quot;&lt;/span&gt;, &lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;ns(time, df = 3)&quot;&lt;/span&gt;)
                  )
paste(formlas)

&lt;/pre&gt;



&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2 + x3 + x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2 + x3 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2 + x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x3 + x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2 + x3 + x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x2 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x3 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2 + x3 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2 + x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x3 + x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x1 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x2 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x3 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;deaths ~ x4 + zone + ns(time, df = 3)&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;




&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;3&lt;/span&gt; AIC vs BIC vs LRTests&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-3&quot;&gt;

&lt;p&gt;Well now we get into a sort of philosophical debate on how to rank all these models.  That'll have to wait for another day.
&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Pioz et al 2012 model selection</title>
   <link href="http://ivanhanigan.github.com/2013/04/pioz-et-al-2012-model-selection/"/>
   <updated>2013-04-18T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/04/pioz-et-al-2012-model-selection</id>
   <content type="html">&lt;p&gt;In the &lt;a href=&quot;http://gis-forum.github.io/study.html&quot;&gt;GIS forum SPDEP study group&lt;/a&gt; we've been discussing the Bluetongue paper &lt;a href=&quot;http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did-environmental-factors-influencing-the-velocity-of-blueton&quot;&gt;http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I'd like to know more about the the Lagrange Multiplier tests and Francis
raised the &lt;a href=&quot;http://ivanhanigan.github.io/2013/04/reflections-bob-haining/#comment-864167749&quot;&gt;seminal Anselin 1988 paper for that&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But in this post I just wanted to summarise their model selection procedure in a flow diagram&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/pioz_modelling.png&quot; alt=&quot;pioz_modelling.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Suicide And Drought Evidence From Literature</title>
   <link href="http://ivanhanigan.github.com/2013/04/suicide-and-drought-evidence-from-literature/"/>
   <updated>2013-04-17T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2013/04/suicide-and-drought-evidence-from-literature</id>
   <content type="html">&lt;p&gt;This essay evolved as I was asked to write something for the &lt;a href=&quot;http://www.hmndp.org/&quot;&gt;High-level Meeting on National Drought Policy (HMNDP)&lt;/a&gt; in Geneva with &lt;a href=&quot;http://www.hmndp.org/presentations/12.03-HMNDP-Parallel5-Howden.pdf&quot;&gt;Mark Howden, Steven Crimp and Bryson Bates&lt;/a&gt;.  It is a summary of material Colin Butler and I wrote for our paper with Phil Kokic and Mike Hutchinson &lt;a href=&quot;http://www.pnas.org/cgi/content/abstract/1112965109v1&quot;&gt;on Suicide and Drought published in PNAS last year&lt;/a&gt;.&lt;/p&gt;

&lt;h1&gt;Suicide and Drought&lt;/h1&gt;

&lt;h2&gt;By Ivan Hanigan and Colin Butler&lt;/h2&gt;

&lt;p&gt;There has been substantial public interest within Australia in recent decades of the putative relationship between drought and rural mental health, including suicide. The topic has frequently been raised by the media, by rural politicians and by mental health support groups [1]. There have also been recent media reports in India indicating substantial concerns about drought and rural suicide in that country, including [2] from May 2012.&lt;/p&gt;

&lt;p&gt;The number of studies that have examined the relationship between suicide and drought is limited. However, many papers explore links between suicide and climate variables other than drought (such as temperature) and there are two major reviews papers available of the literature on climatic influences on suicide [3, 4]. Some climatic variables related to dryness have been studied for example Preti (1998) [5] found higher suicide rates in drier towns in Italy. But we make the point that very few studies have investigated the &quot;dryer than average conditions&quot; that is drought specifically.&lt;/p&gt;

&lt;p&gt;There are several mechanisms through which unusually low rainfall, especially if exacerbated by increased soil dryness due to higher temperatures may increase the suicide rate. First, droughts increase the financial stress on farmers and farming communities (even if partially compensated by drought relief welfare payments). Such difficulty may occur in conjunction with other economic stresses, such as rising interest rates, falling commodity prices, or an unfavourable foreign exchange rate.
In the broader economic system, reduced rainfall can depress economic activity in rural towns. In some regions the entire economy may be affected. Rural downturns can accelerate migration to metropolitan areas; weakening and stressing social support systems and lessening social interaction. In some cases rural depopulation may pass a tipping point, leading to an ongoing loss of critical services, such as hospitals, schools and doctors.
Second, there can be a great psychological toll following environmental degradation [6] and this may be acute during droughts linked with decisions and actions to sell or kill starving animals or to destroy orchards and vineyards, which in some cases were painstakingly accumulated over generations. Such loss, and even the apprehension of loss, undoubtedly places a burden on the mental health of farmers and their families. This mourning may not be confined to farmers but extend to other sections of the community likely to be impoverished by long-term environmental degradation. The experience of seeing suffering wild plants and animals, or parched urban parks and gardens, and contemplation of their loss is likely to be extremely painful for some individuals.&lt;/p&gt;

&lt;h1&gt;Evidence from literature&lt;/h1&gt;

&lt;p&gt;As mentioned the number of studies that have examined the relationship between suicide and drought is limited. One analysis of annual suicide rates in NSW found an association between suicide and year-to-year decline in annual rainfall between 1964 and 2001 [7]. In that study a decrease of 300mm of rain was associated with an increase in suicide rate of about 8% above the mean annual rate.
Another study of NSW, for the period 1901-1998, found an association between suicide and drought. That study focused on the association of conservative government and suicide [8]. The authors argued conservative government programmes (or perceived prospects under a government) might influence suicide directly, or that a correlated increase in anomie (decreased inclusiveness of society) and lowering social capital enhances risk of suicide in vulnerable individuals. The authors controlled for drought (among other things), and found that drought years were associated with an increased suicide risk of about 7% for men and 15% for women, across the whole population.
A third study [9] found no association between drought and suicide in Victoria but was based on only 7 years data (2001-2007) and did not stratify the population of the state into regions which may introduce bias in the exposure estimates of drought affected people.&lt;/p&gt;

&lt;p&gt;In contrast a longer running study did find an association using 38 years of data (1970-2007) to explore potential drought effects, especially on farmers and farm workers [10]. The drought exposures were calculated from climatic data for 11 subregions of New South Wales, and stratified by rural/urban region, age and sex. A strong association was observed in rural males aged 10-49. Surprisingly the suicide risk decreased in rural females aged over 30.&lt;/p&gt;

&lt;p&gt;This study provides clear evidence to support the hypothesis that male farmers, farm workers and farming families are at risk of depression and suicide due to droughts. The resulting statistical model estimated that around 9 % of rural suicides in males aged 30-49 were due to drought over the entire study period. This estimate is an average over the course of the 38 years of the study, as the majority of years are not droughts - the percentage is much greater than 9 % in the actual drought years, since these are episodic and confined to a distinct minority of years.
The statistical model also controlled for other well-known trends in suicide data, including that times of unusually high maximum temperatures increased suicide risk, that there was a increased risk in spring and early summer, and that there was a marked drop in suicide rates over the last decade.&lt;/p&gt;

&lt;h1&gt;Discussion&lt;/h1&gt;

&lt;p&gt;These studies from Australia offer some lessons for policy makers. The results identify a suite of contributing factors that influence suicide drawn from the environmental, social and political context of life in Australia, which drought is a part of. In particular these results help isolate the most critical times of risk, so that the best use of resources might be made. This includes provision of counselling services to target vulnerable people and get them help, both during droughts, at times with hotter than average maximum temperatures and during the dangerous spring period.
Other policy implications from this finding support broadening investment in research into gender specific drought effects rather than purely climate and economic focused research into drought impacts.&lt;/p&gt;

&lt;h1&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;[1]   Australian Broadcasting Commision (ABC) News. Drought lifts suicide rates: Kennett, 2006.  http://www.abc.net.au/news/2006-10-13/drought-lifts-suicide-rates-kennett/1285734&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[2]   P Sarathi Biswas. Alcohol, drought lead to farmer’s suicide. Daily News and Analysis, 2012. http://www.dnaindia.com/pune/report_alcohol-drought-lead-to-farmers-suicide_1688976&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[3] PG Dixon and AJ Kalkstein. Climate-suicide relationships: A research
problem in need of geographic methods and cross-disciplinary perspectives.
Geography Compass , 3(6):1{14, 2009.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[4] E A Deisenhammer. Weather and suicide: the present state of knowledge
on the association of meteorological factors with suicidal behaviour. Acta
Psychiatrica Scandinavica , 108(6):402{409, 2003.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[5] Antonio Preti. The infleuence of clima te on suicidal behaviour in Italy.
Psychiatry Res , 78(1-2):9{19, 1998.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[6]   PC Speldewinde, A Cook, P Davies, and P Weinstein. A relationship between environmental degradation and mental health in rural Western Australia. Health and Place, 15, 2009.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[7]   N Nicholls, CD Butler, and IC Hanigan. Inter-annual rainfall variations and suicide in New South Wales, Australia, 1964-2001. International Journal of Biometeorology, 50(3), 2006.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[8]   A Page, S Morrell, and R Taylor. Suicide and political regime in New South Wales and Australia during the 20th century. Journal of Epidemiology and Community Health, 56, 2002.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[9]   Robyn Guiney. Farming suicides during the Victorian drought: 2001-2007. The Australian Journal of Rural Health, 20(1):11–5, February 2012.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[10]  I. C. Hanigan, C. D. Butler, P. N. Kokic, and M. F. Hutchinson. Suicide and drought in New South Wales, Australia, 1970-2007. Proceedings of the National Academy of Sciences, pages 1112965109–, August 2012.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Timeseries with Spatial Lag</title>
   <link href="http://ivanhanigan.github.com/2013/04/timeseries-with-spatial-lag/"/>
   <updated>2013-04-06T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/04/timeseries-with-spatial-lag</id>
   <content type="html">&lt;p&gt;&lt;head&gt;
&lt;title&gt;adjacency example &lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=iso-8859-1&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;adjacency example &quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-09-24T21:01+1000&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;Ivan Hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;script type=&quot;text/javascript&quot; src=&quot;http://orgmode.org/mathjax/MathJax.js&quot;&gt;
/**
 *
 * @source: http://orgmode.org/mathjax/MathJax.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 * Copyright (C) 2012-2013  MathJax
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 */

/*
@licstart  The following is the entire license notice for the
JavaScript code below.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code below is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code below.
*/
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: [&quot;MMLorHTML.js&quot;], jax: [&quot;input/TeX&quot;],
            jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;],
        extensions: [&quot;tex2jax.js&quot;,&quot;TeX/AMSmath.js&quot;,&quot;TeX/AMSsymbols.js&quot;,
                     &quot;TeX/noUndefined.js&quot;],
        tex2jax: {
            inlineMath: [ [&quot;\\(&quot;,&quot;\\)&quot;] ],
            displayMath: [ ['$$','$$'], [&quot;\\[&quot;,&quot;\\]&quot;], [&quot;\\begin{displaymath}&quot;,&quot;\\end{displaymath}&quot;] ],
            skipTags: [&quot;script&quot;,&quot;noscript&quot;,&quot;style&quot;,&quot;textarea&quot;,&quot;pre&quot;,&quot;code&quot;],
            ignoreClass: &quot;tex2jax_ignore&quot;,
            processEscapes: false,
            processEnvironments: true,
            preview: &quot;TeX&quot;
        },
        showProcessingMessages: true,
        displayAlign: &quot;center&quot;,
        displayIndent: &quot;2em&quot;,

        &quot;HTML-CSS&quot;: {
             scale: 100,
             availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;],
             preferredFont: &quot;TeX&quot;,
             webFont: &quot;TeX&quot;,
             imageFont: &quot;TeX&quot;,
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    &quot;MML&quot;,
                 Firefox: &quot;MML&quot;,
                 Opera:   &quot;HTML&quot;,
                 other:   &quot;HTML&quot;
             }
        }
    });
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;h1 class=&quot;title&quot;&gt;adjacency example &lt;/h1&gt;


&lt;hr/&gt;

&lt;div id=&quot;table-of-contents&quot;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id=&quot;text-table-of-contents&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 Load some test data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-3&quot;&gt;3 spdep calculates neighbours&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-4&quot;&gt;4 plot these&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-5&quot;&gt;5 function to return adjacency list as a dataframe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#sec-6&quot;&gt;6 test-adjacency df&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;I've got a timeseries model I am fitting to a city dataset with about 45 zones.
The data are daily, stratified by Zone, Age and Sex.
Following on from learning about spatiallly correlated errors I want to see if the Standard Error on the estimated \(\beta_{1}\) from the timeseries model is affected.
&lt;/p&gt;
&lt;p&gt;
I think the simplest option is to use the spatial lag model, which can be fitted with just adding a term that is the average of the set of each Zone's neighbours outcome level on each day. For this I need to find the list of each region's neighbours.  Then I'll use this to assign each zone/day/age group their neighbours values and then collapse that to get their daily means.
&lt;/p&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;2&lt;/span&gt; Load some test data&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-2&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;we have access to a classic dataset for studying spatial dependence&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;in the spdep package&lt;/span&gt;
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(spdep))    install.packages(spdep); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(spdep)     
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(rgdal))    install.packages(rgdal); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(rgdal) 
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(maptools)) install.packages(maptools); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(maptools) 
&lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(!&lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(maps))     install.packages(maps); &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;require&lt;/span&gt;(maps) 
fn &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; system.file(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;etc/shapes/eire.shp&quot;&lt;/span&gt;, package=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;spdep&quot;&lt;/span&gt;)[1]
prj &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; CRS(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;+proj=utm +zone=30 +units=km&quot;&lt;/span&gt;)
eire &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; readShapeSpatial(fn, ID=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;names&quot;&lt;/span&gt;, proj4string=prj)
str(eire)
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;reproject into a better coordinate system&lt;/span&gt;
eire &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; spTransform(eire, CRS(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;+proj=longlat +datum=WGS84&quot;&lt;/span&gt;))
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;check out the attributes&lt;/span&gt;
head(eire@data)

&lt;/pre&gt;


&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;right&quot; /&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;A&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;towns&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;pale&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;size&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;ROADACC&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;OWNCONS&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;POPCHG&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;RETSALE&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;right&quot;&gt;INCOME&lt;/th&gt;&lt;th scope=&quot;col&quot; class=&quot;left&quot;&gt;names&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;34.2&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0.12&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;1087&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;3664&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;8.6&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;97&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;2962&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;7185&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;29.68&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0.01&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;2133&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;5000&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;15&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;69&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;4452&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9459&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;26.54&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0.01&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;535&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;4321&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;19&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;78&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;3460&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;12435&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Clare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;23.92&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0.03&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;1476&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;4118&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;90&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;28402&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;65901&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;27.91&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0.03&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;989&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;7500&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;27&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;75&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;7478&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;17626&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Donegal&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;right&quot;&gt;32.79&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;0.61&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;1&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;18105&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;3078&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;9.4&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;142&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;89424&lt;/td&gt;&lt;td class=&quot;right&quot;&gt;164631&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Dublin&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;3&lt;/span&gt; spdep calculates neighbours&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-3&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;nb &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; poly2nb(eire)
str(nb)
&lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;List of 26&lt;/span&gt;
nb[[1]]
&lt;span style=&quot;color: #586e75;&quot;&gt;#&lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;[1]  9 10 11 25 26&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;So this returns the set of index values for each area's neighbours&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;I'd prefer to read their names&lt;/span&gt;
eire[[&lt;span style=&quot;color: #2aa198;&quot;&gt;'names'&lt;/span&gt;]][1]
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;&amp;gt; [1] Carlow&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;so therefore the neighbours of area 1 &quot;Carlow&quot; are in the first&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;element of the list&lt;/span&gt;
eire[[&lt;span style=&quot;color: #2aa198;&quot;&gt;'names'&lt;/span&gt;]][nb[[1]]]
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;&amp;gt; [1] Kildare  Kilkenny Laoghis  Wexford  Wicklow&lt;/span&gt;

&lt;/pre&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-4&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-4&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;4&lt;/span&gt; plot these&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-4&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:plot these&lt;/span&gt;
png(&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;images/Fig1.png&quot;&lt;/span&gt;)
plot(eire)
plot(nb, coordinates(eire), add=&lt;span style=&quot;color: #b58900;&quot;&gt;TRUE&lt;/span&gt;, pch=&lt;span style=&quot;color: #2aa198;&quot;&gt;&quot;.&quot;&lt;/span&gt;, lwd=2)
map.scale(ratio = F)
box()
dev.off()

&lt;/pre&gt;


&lt;p&gt;
&lt;img src=&quot;/images/Fig1.png&quot;  alt=&quot;images/Fig1.png&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-5&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-5&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;5&lt;/span&gt; function to return adjacency list as a dataframe&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-5&quot;&gt;

&lt;p&gt;I THINK I actually want this as a dataframe so I can merge it with the master table of outcome data.
&lt;/p&gt;



&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:adjacency_df&lt;/span&gt;
&lt;span style=&quot;color: #268bd2;&quot;&gt;adjacency_df&lt;/span&gt; &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;function&lt;/span&gt;(NB, shp, zone_id)
  {
    adjacencydf &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; as.data.frame(matrix(&lt;span style=&quot;color: #b58900;&quot;&gt;NA&lt;/span&gt;, nrow = 0, ncol = 2))
    &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;for&lt;/span&gt;(i &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;in&lt;/span&gt; 1:length(NB))
    {
      &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;if&lt;/span&gt;(length(shp[[zone_id]][NB[[i]]]) == 0) &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;next&lt;/span&gt;
      adjacencydf &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; rbind(
                           adjacencydf,
                           cbind(
                                 as.character(shp[[zone_id]][i]),
                                 as.character(shp[[zone_id]][NB[[i]]])
                                 )
                           )
    }
    &lt;span style=&quot;color: #859900; font-weight: bold;&quot;&gt;return&lt;/span&gt;(adjacencydf)
  }

&lt;/pre&gt;


&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-6&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-6&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;6&lt;/span&gt; test-adjacency df&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-6&quot;&gt;




&lt;pre class=&quot;src src-R&quot;&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;################################################################&lt;/span&gt;
&lt;span style=&quot;color: #586e75;&quot;&gt;# &lt;/span&gt;&lt;span style=&quot;color: #586e75;&quot;&gt;name:adjacency_df&lt;/span&gt;
adj &lt;span style=&quot;color: #268bd2; font-weight: bold;&quot;&gt;&amp;lt;-&lt;/span&gt; adjacency_df(NB = nb, shp = eire, zone_id = &lt;span style=&quot;color: #2aa198;&quot;&gt;'names'&lt;/span&gt;)
adj  
&lt;/pre&gt;


&lt;table border=&quot;2&quot; cellspacing=&quot;0&quot; cellpadding=&quot;6&quot; rules=&quot;groups&quot; frame=&quot;hsides&quot;&gt;
&lt;colgroup&gt;&lt;col class=&quot;left&quot; /&gt;&lt;col class=&quot;left&quot; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Monaghan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Clare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Clare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Clare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kerry&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Donegal&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Dublin&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Dublin&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Dublin&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Clare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Mayo&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kerry&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kerry&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Dublin&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Donegal&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Sligo&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Clare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kerry&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Louth&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Louth&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Monaghan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Mayo&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Mayo&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Mayo&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Sligo&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Dublin&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Louth&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Monaghan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Monaghan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Monaghan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Louth&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Monaghan&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Mayo&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Sligo&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Sligo&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Leitrim&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Sligo&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Mayo&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Sligo&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Clare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Galway&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Laoghis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Limerick&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cork&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Tipperary&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Cavan&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Longford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Meath&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Offaly&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Westmeath&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Roscommon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kilkenny&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Waterford&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Carlow&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Dublin&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Kildare&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=&quot;left&quot;&gt;Wicklow&lt;/td&gt;&lt;td class=&quot;left&quot;&gt;Wexford&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reflections on Spatial Regression Class with Prof Bob Haining</title>
   <link href="http://ivanhanigan.github.com/2013/04/reflections-bob-haining/"/>
   <updated>2013-04-05T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2013/04/reflections-bob-haining</id>
   <content type="html">&lt;p&gt;&lt;head&gt;
&lt;title&gt;Reflections on a class with Prof Bob Haining&lt;/title&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=iso-8859-1&quot;/&gt;
&lt;meta name=&quot;title&quot; content=&quot;Reflections on a class with Prof Bob Haining&quot;/&gt;
&lt;meta name=&quot;generator&quot; content=&quot;Org-mode&quot;/&gt;
&lt;meta name=&quot;generated&quot; content=&quot;2013-04-05&quot;/&gt;
&lt;meta name=&quot;author&quot; content=&quot;ivan hanigan&quot;/&gt;
&lt;meta name=&quot;description&quot; content=&quot;&quot;/&gt;
&lt;meta name=&quot;keywords&quot; content=&quot;&quot;/&gt;&lt;/p&gt;



&lt;script type=&quot;text/javascript&quot;&gt;
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = &quot;code-highlighted&quot;;
     elem.className   = &quot;code-highlighted&quot;;
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;script type=&quot;text/javascript&quot; src=&quot;http://orgmode.org/mathjax/MathJax.js&quot;&gt;
&lt;!--/*--&gt;&lt;![CDATA[/*&gt;&lt;!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: [&quot;MMLorHTML.js&quot;], jax: [&quot;input/TeX&quot;],
            jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;],
        extensions: [&quot;tex2jax.js&quot;,&quot;TeX/AMSmath.js&quot;,&quot;TeX/AMSsymbols.js&quot;,
                     &quot;TeX/noUndefined.js&quot;],
        tex2jax: {
            inlineMath: [ [&quot;\\(&quot;,&quot;\\)&quot;] ],
            displayMath: [ ['$$','$$'], [&quot;\\[&quot;,&quot;\\]&quot;], [&quot;\\begin{displaymath}&quot;,&quot;\\end{displaymath}&quot;] ],
            skipTags: [&quot;script&quot;,&quot;noscript&quot;,&quot;style&quot;,&quot;textarea&quot;,&quot;pre&quot;,&quot;code&quot;],
            ignoreClass: &quot;tex2jax_ignore&quot;,
            processEscapes: false,
            processEnvironments: true,
            preview: &quot;TeX&quot;
        },
        showProcessingMessages: true,
        displayAlign: &quot;center&quot;,
        displayIndent: &quot;2em&quot;,

        &quot;HTML-CSS&quot;: {
             scale: 100,
             availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;],
             preferredFont: &quot;TeX&quot;,
             webFont: &quot;TeX&quot;,
             imageFont: &quot;TeX&quot;,
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    &quot;MML&quot;,
                 Firefox: &quot;MML&quot;,
                 Opera:   &quot;HTML&quot;,
                 other:   &quot;HTML&quot;
             }
        }
    });
/*]]&gt;*///--&gt;
&lt;/script&gt;


&lt;p&gt;&lt;/head&gt;
&lt;body&gt;&lt;/p&gt;

&lt;div id=&quot;preamble&quot;&gt;

&lt;/div&gt;




&lt;div id=&quot;content&quot;&gt;
&lt;!-- &lt;h1 class=&quot;title&quot;&gt;Reflections on a class with Prof Bob Haining&lt;/h1&gt; --&gt;


&lt;!-- &lt;div id=&quot;table-of-contents&quot;&gt; --&gt;
&lt;!-- &lt;h2&gt;Table of Contents&lt;/h2&gt; --&gt;
&lt;!-- &lt;div id=&quot;text-table-of-contents&quot;&gt; --&gt;
&lt;!-- &lt;ul&gt; --&gt;
&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-1&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt; --&gt;
&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-2&quot;&gt;2 The Spatial Error Model&lt;/a&gt;&lt;/li&gt; --&gt;
&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-3&quot;&gt;3 The Spatial Lag Model&lt;/a&gt;&lt;/li&gt; --&gt;
&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-4&quot;&gt;4 Spatially Lagged Independent Variable(s)&lt;/a&gt;&lt;/li&gt; --&gt;
&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-5&quot;&gt;5 Discussion&lt;/a&gt; --&gt;
&lt;!-- &lt;ul&gt; --&gt;
&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-5-1&quot;&gt;5.1 How to decide which model to fit?&lt;/a&gt;&lt;/li&gt; --&gt;
&lt;!-- &lt;/ul&gt; --&gt;
&lt;!-- &lt;/li&gt; --&gt;
&lt;!-- &lt;li&gt;&lt;a href=&quot;#sec-6&quot;&gt;6 Conclusion&lt;/a&gt;&lt;/li&gt; --&gt;
&lt;!-- &lt;/ul&gt; --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!-- &lt;/div&gt; --&gt;

&lt;div id=&quot;outline-container-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;

&lt;p&gt;I recently attended a class on spatial regression with Prof Bob
Haining.  He described the issue of spatially correlated errors and
the problems this poses in spatial regression.
&lt;/p&gt;
&lt;p&gt;
The key issue is that spatial data often violates the assumption in
regression models that the errors are independent. 
&lt;/p&gt;
&lt;p&gt;
A simple regression model applied to spatial data based on ZONES:
&lt;/p&gt;


\(Y_{i} = \beta_{0} + ZONE_{i} + \beta_{1} X_{1i} + e_{i}\)

&lt;p&gt;
But with spatial data it is likely that the errors are spatially
correlated.  This is likely to mean the point estimate of beta 1 is OK
but the Standard Error is wrong.
&lt;/p&gt;
&lt;p&gt;
This might be due to the scale of the study units, which may not
capture the variation of exposure and outcome adequately. Or there
might be unmeasured explanatory variables that have not been accounted
for.
&lt;/p&gt;
&lt;p&gt;
I am mostly concerned with EXPLANATORY modelling in which a particular
exposure of interest is to be assessed.  Examples include a weather
variable (temperature), an air pollutant (PM10) or some measure of
socio-economic deprivation in an area (SEIFA scores in Australian
census data).  In these models I tend to include a number of
'nuisance' parameters to control for confounding; or interaction terms
to account for effect modification.  In this type of model the
performance of the model over-all is not that important, I just want
to control for the most important confounders so that my estimate of
the exposure of interest is as rigorous as possible.
&lt;/p&gt;
&lt;p&gt;
Therefore the problem that spatially correlated errors pose for these models is
slightly different to that which affects models aimed at PREDICTION: I
am not concerned so much with the model's fit to the data, rather the
confidence around the point-estimate of the parameter for the exposure
of interest.
&lt;/p&gt;
&lt;p&gt;
Simplistically I took away the following messages:
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-2&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;2&lt;/span&gt; The Spatial Error Model&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-2&quot;&gt;

&lt;p&gt;So we could model allowing for correlated errors:
&lt;/p&gt;


\(Y_{i} = \beta_{0} + ZONE_{i} + \beta_{1} X_{1i} + \eta_{i}\)

&lt;p&gt;
Where:
&lt;/p&gt;
&lt;p&gt;
\(\eta_{i}\) = Spatially autocorrelated errors.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-3&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;3&lt;/span&gt; The Spatial Lag Model&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-3&quot;&gt;

&lt;p&gt;Or we could include a term for the neighbours, thus absorbing the correlated errors:
&lt;/p&gt;


\(Y_{i} = \beta_{0} + ZONE_{i} + \beta_{1} X_{1i} + \rho(Neighbours Y_{ij}) + e_{i}\)

&lt;p&gt;
Where:
&lt;/p&gt;
&lt;p&gt;
\(\rho_(Neighbours Y_{ij})\) = is an additional explanatory variable which is the value of the dependent variable in neighbouring areas. 
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-4&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-4&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;4&lt;/span&gt; Spatially Lagged Independent Variable(s)&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-4&quot;&gt;

&lt;p&gt;This is almost a variation of the spatial lag model, except that we
include a term for the exposure variable in the neighbours, and
therefore 'smooth' the effect of the exposure from what was observed
in any area to make it relevant to it's neighbours as well:
&lt;/p&gt;


\(Y_{i} = \beta_{0} + ZONE_{i} + \beta_{1} X_{1i} + \beta_{2L} X_{2ij} + e_{i}\)

&lt;p&gt;
Where:
&lt;/p&gt;
&lt;p&gt;
\(\beta_{2L} X_{2ij}\) = is the independent variable X2 that is spatially lagged.
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-5&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-5&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;5&lt;/span&gt; Discussion&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-5&quot;&gt;


&lt;/div&gt;

&lt;div id=&quot;outline-container-5-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-5-1&quot;&gt;&lt;span class=&quot;section-number-3&quot;&gt;5.1&lt;/span&gt; How to decide which model to fit?&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-5-1&quot;&gt;

&lt;p&gt;So the burning question is how to choose between the various spatial
models? Prof Haining had some suggestions, but he noted that sometimes
two could be equally appropriate.  He suggested that the spatial lag
model makes the strong assumption that there is a relationship between
the outcome in a neighbouring area with the index zone.  This suggests
some kind of contagion or dispersion effect.  He was not keen to fit
this model in circumstances where the causal mechanism did not support
such a relationship, suggesting the spatially weighted error model was
more suited, but that &quot;in practice they often give the same result&quot;.
&lt;/p&gt;
&lt;p&gt;
In my situation where I am not concerned with the actual
autocorrelation but with tightening up the standard error on my
exposure of interest, I think I might plead forgiveness and try
fitting the spatial lag model as it seems easier.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div id=&quot;outline-container-6&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-6&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;6&lt;/span&gt; Conclusion&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-6&quot;&gt;

&lt;p&gt;Stay tuned.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>software-ism</title>
   <link href="http://ivanhanigan.github.com/2012/09/software-ism/"/>
   <updated>2012-09-15T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2012/09/software-ism</id>
   <content type="html">&lt;p&gt;I am a huge fan of the R language for statistics and graphics.&lt;/p&gt;

&lt;p&gt;I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).&lt;/p&gt;

&lt;p&gt;In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.&lt;/p&gt;

&lt;p&gt;I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.&lt;/p&gt;

&lt;p&gt;For example a simple analysis in Rstudio using the 'R Markdown document' is below.&lt;/p&gt;

&lt;p&gt;You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the &lt;strong&gt;Knit HTML&lt;/strong&gt; button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;summary(cars)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt; speed &lt;/th&gt;
&lt;th&gt; dist &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Min.   : 4.0 &lt;/td&gt;
&lt;td&gt; Min.   :  2.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; 1st Qu.:12.0 &lt;/td&gt;
&lt;td&gt; 1st Qu.: 26.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Median :15.0 &lt;/td&gt;
&lt;td&gt; Median : 36.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Mean   :15.4 &lt;/td&gt;
&lt;td&gt; Mean   : 42.98  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; 3rd Qu.:19.0 &lt;/td&gt;
&lt;td&gt; 3rd Qu.: 56.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Max.   :25.0 &lt;/td&gt;
&lt;td&gt; Max.   :120.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;hr /&gt;

&lt;p&gt;You can also embed plots, for example:&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;plot(cars)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/images/unnamed-chunk-2.png&quot; alt=&quot;plot of chunk unnamed-chunk-2&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;I hope we can work toward a kind of 'tower of babel'.&lt;/h2&gt;

&lt;p&gt;name: software-ism
layout: post
title: software-ism
date: 2012-09-15
categories:&lt;/p&gt;

&lt;h2&gt;- software&lt;/h2&gt;

&lt;p&gt;I am a huge fan of the R language for statistics and graphics.&lt;/p&gt;

&lt;p&gt;I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).&lt;/p&gt;

&lt;p&gt;In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.&lt;/p&gt;

&lt;p&gt;I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.&lt;/p&gt;

&lt;p&gt;For example a simple analysis in Rstudio using the 'R Markdown document' is below.&lt;/p&gt;

&lt;p&gt;You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the &lt;strong&gt;Knit HTML&lt;/strong&gt; button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;summary(cars)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt; speed &lt;/th&gt;
&lt;th&gt; dist &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Min.   : 4.0 &lt;/td&gt;
&lt;td&gt; Min.   :  2.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; 1st Qu.:12.0 &lt;/td&gt;
&lt;td&gt; 1st Qu.: 26.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Median :15.0 &lt;/td&gt;
&lt;td&gt; Median : 36.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Mean   :15.4 &lt;/td&gt;
&lt;td&gt; Mean   : 42.98  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; 3rd Qu.:19.0 &lt;/td&gt;
&lt;td&gt; 3rd Qu.: 56.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt; Max.   :25.0 &lt;/td&gt;
&lt;td&gt; Max.   :120.00  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>The Ecological Fallacy is Itself a Fallacy</title>
   <link href="http://ivanhanigan.github.com/2012/07/the-ecological-fallacy-is-itself-a-fallacy/"/>
   <updated>2012-07-09T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2012/07/the-ecological-fallacy-is-itself-a-fallacy</id>
   <content type="html">&lt;h2&gt;The Ecological Fallacy&lt;/h2&gt;

&lt;p&gt;The term 'Ecological fallacy' is used in Epidemiology and some other disciplines (such as Sociology) to refer to &lt;a href=&quot;http://www.springer.com/medicine/book/978-1-4419-1004-2&quot; title=&quot;Marc Gellman, J. Rick Turner. Encyclopedia of Behavioral Medicine Springer, 2012.&quot;&gt;improperly inferring a causal association (or lack of association) at an individual-level based on a group-level relationship&lt;/a&gt;. This use of the word ecological is at odds with the alternative use of the word in the discipline of Ecology.  &lt;a href=&quot;http://cup.columbia.edu/book/978-0-231-06919-9/toward-a-unified-ecology&quot; title=&quot;Timothy F. H. Allen, Thomas W. Hoekstra. Toward a unified ecology. Columbia University Press, 1993.&quot;&gt;Ecological methods from Ecology are inherently multi-scaled&lt;/a&gt; and address precisely the issue of this cross-scale inferential fallacy.&lt;/p&gt;

&lt;p&gt;I argue that a broader understanding of ecological methods by non-Ecologists would be a start towards better understanding between the disciplines.  The inclusion of real ecological methods in the other disciplines will also assist research to better understand the causes and effects of climate and climate change which are important gaps in knowledge needed to enable mitigation and adaptation of our society to future climate change.&lt;/p&gt;

&lt;h2&gt;Sociology&lt;/h2&gt;

&lt;p&gt;To clear up this confusion it is necessary to go back to the source of the use of the term 'ecological' by the urban sociologist Amos Hawley who used the term 'human ecology' in the 1950s to build on the theoretical traditions of &lt;a href=&quot;http://books.google.com.au/books?id=PCBWr5UySs8C&amp;amp;dq=Meade%20and%20Earickson%202000%20medical%20geography&amp;amp;source=gbs_book_other_versions&quot; title=&quot;Melinda S. Meade, Robert J. Earickson Medical Geography. Guilford Press 2005.&quot;&gt;Robert Parke and E. W. Burgess at the University of Chicago in the 1920s on the structure and development of cities&lt;/a&gt;. His influence in the Social Sciences led directly to the development a tradition of social human ecology, one largely without a bio-physical environment.  The usage of 'ecological' to mean multivariate studies of complex systems stems from the sociological tradition.&lt;/p&gt;

&lt;h2&gt;Ecology&lt;/h2&gt;

&lt;p&gt;The term 'ecology' used in the Ecology discipline &lt;a href=&quot;http://en.wikipedia.org/wiki/Ecology&quot; title=&quot;Wikipedia&quot;&gt;dates back to the 1870s&lt;/a&gt;, coined by German zoologist Ernst Haeckel (1834-1919) as Oekologie from Greek 'oikos' for house, dwelling place, habitation and 'logia' study of, then at the turn of the century the term became spelt 'oecology', which grew into 'ecology' in the early part of the 1900s.&lt;/p&gt;

&lt;h2&gt;Epidemiology&lt;/h2&gt;

&lt;p&gt;In epidemiology the term 'ecological study' is used to refer to studies where observations are taken at the level of a group (such as a country, school, or hospital) rather than at the individual &lt;a href=&quot;http://www.springer.com/medicine/book/978-1-4419-1004-2&quot; title=&quot;Marc Gellman, J. Rick Turner. Encyclopedia of Behavioral Medicine Springer, 2012.&quot;&gt;(such as patient) level&lt;/a&gt;.  It is well known though that when risk factors and outcomes are measured at an aggregate level, the relationship between the group-level variables may be different than the relationship between variables &lt;a href=&quot;http://en.wikipedia.org/wiki/Modifiable_areal_unit_problem&quot; title=&quot;Openshaw, S. (1984). The Modifiable Areal Unit Problem. Norwich: Geo Books. ISBN 0-86094-134-5.&quot;&gt;measured at the individual level&lt;/a&gt;. An often cited example used to illustrate the issue involved a 19th century study which found higher suicide rates within Prussian provinces that had &lt;a href=&quot;http://books.google.com.au/books?hl=en&amp;amp;lr=&amp;amp;id=v23YleX1UskC&amp;amp;oi=fnd&amp;amp;pg=PA9&amp;amp;dq=durkheim&amp;amp;ots=sXTEH3naVk&amp;amp;sig=ycS5kkxwSUA_gA84ts3iXrYNStI#v=onepage&amp;amp;q&amp;amp;f=false&quot; title=&quot;Durkheim, E. (1951). Suicide: A study in sociology (Spaulding &amp;amp; Simpson, trans.). Glencoe, IL: Free Press.(Original Work Published 1897).&quot;&gt;higher proportions of Protestant residents&lt;/a&gt;. The conclusion that Protestant individuals (rather than Catholic individuals) were more likely to commit suicide &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/7137430&quot; title=&quot;Morgenstern, H. (1982). Uses of ecologic analysis in epidemiologic research. American Journal of Public Health, 72(12)&quot;&gt;cannot be inferred based on the observed association among the provinces&lt;/a&gt;. One possible scenario is that Catholic residents within the largely Protestant provinces had the high suicide rates, resulting in a positive association between percent Protestant and suicide rate &lt;a href=&quot;http://ije.oxfordjournals.org/content/early/2011/05/24/ije.dyr081.full&quot; title=&quot;Robinson, W.S. (1950). Ecological Correlations and the Behavior of Individuals. American Sociological Review (American Sociological Review, Vol. 15, No. 3) 15 (3): 351???357.&quot;&gt;8&lt;/a&gt;. Extrapolation of aggregate results to individuals is a mistake in logic &lt;a href=&quot;http://ije.oxfordjournals.org/content/early/2011/05/24/ije.dyr081.full&quot; title=&quot;Robinson, W. S. (2009). Ecological correlations and the behavior of individuals. International Journal of Epidemiology, 38(2)&quot;&gt;9&lt;/a&gt; which can lead to a potentially misleading conclusion &lt;a href=&quot;http://ije.oxfordjournals.org/content/early/2011/05/24/ije.dyr081.full&quot; title=&quot;Grotenhuis, Manfred, Rob Eisinga, and SV Subramanian. Robinson's Ecological Correlations and the Behavior of Individuals: methodological corrections. International journal of epidemiology 40, no. 4 2011: 1123-5.&quot;&gt;10&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Because of this limitation 'ecologic studies' are often scorned in epidemiology as inferior and only useful for exploratory or hypothesis-generating studies rather than as confirmatory.  I argue to the contrary that there is the potential for a revolution in our ability to understand causal influences operating at multiple scales of space and time if we were to conduct truly 'ecological studies'.&lt;/p&gt;

&lt;h2&gt;Quantitative Geography&lt;/h2&gt;

&lt;p&gt;There is also a closely related concept that should be noted.  That of the Modifiable Areal Unit Problem &lt;a href=&quot;http://en.wikipedia.org/wiki/Modifiable_areal_unit_problem&quot; title=&quot;Openshaw, S. (1984). The Modifiable Areal Unit Problem. Norwich: Geo Books. ISBN 0-86094-134-5.&quot;&gt;(MAUP)&lt;/a&gt; as discussed in quantitative geography.  There is a large amount of geographical literature on the MAUP.  In this problem domain areal units (also called zones, regions, areas or polygons) inherently pose three problems to a researcher attempting to infer causal associations: scale, zonal and temporal:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scale; this issues is evident in the example above where phenomena investigated using data viewed at one scale may appear quite different (even opposite) using data aggregated at a different scale.&lt;/li&gt;
&lt;li&gt;Zonal; the zonal problem appears where phenomena investigated using data viewed using two sets of differing areas at a single scale can differ.&lt;/li&gt;
&lt;li&gt;Temporal; a third problem arises when analyzing data on modifiable areas when people keep modifying them by redrawing the boundaries over time.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;The term 'Ecological Fallacy' is itself a fallacy and non-Ecologists should be made aware of the existence of alternative ecologic methods from Ecology.  This would be a start towards better understanding between the disciplines and enhance our abilities to mitigate and adapt to climate change.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Centennial Scale Rainfall in Southeastern Australia</title>
   <link href="http://ivanhanigan.github.com/2012/06/centenial-scale-rainfall-in-southeastern-australia/"/>
   <updated>2012-06-05T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2012/06/centenial-scale-rainfall-in-southeastern-australia</id>
   <content type="html">&lt;h2&gt;Droughts are extreme rainfall events on centennial scale&lt;/h2&gt;

&lt;p&gt;In the &lt;a href=&quot;https://github.com/ivanhanigan/HutchinsonDroughtIndex&quot; title=&quot;Hutchinson Drought Index project&quot;&gt;Hutchinson Drought Index project&lt;/a&gt; we used the longest period of rainfall data available because the drought index is based on the ranking of each six-month average of the distribution in the entire record of observations. A period as long as this is required to calculate extreme rainfall deficits.  The original Hutchinson paper used a period 1920-1988 due to availability.  Another consideration is the longer term characteristics of the rainfall epoch you are considering.&lt;/p&gt;

&lt;p&gt;For example, in recent decades in Southeastern Australia annual total rainfal does not represent the longer term context very well.  Shown in the image below is the result of an exploratory data analysis using rainfall data from the Murray Darling Basin (the 'bread basket' of Australian agriculture).  I use the Classification And Regression Tree tool in the 'rpart' package to determine the optimal groupings.  I've dropped the last two years of the sequence because when I first ran this analysis two years ago I got the result shown, but thankfully the last two years have given us very good rainfalls.  This shows the difference between short-term and long-term rainfall patterns.&lt;/p&gt;

&lt;p&gt;Regardless of this data 'massaging', in the analysis presented the annual trend over the first half of the twentieth century was lower than the recent fifty years, by about 60 millimeters on average. The 1930-1946 period was particularly dry, as has been the 1998-2008 period.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/mdb_rain19002009.jpg&quot; alt=&quot;plot of seaust rainfall&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How to explain my current research interests?</title>
   <link href="http://ivanhanigan.github.com/2012/05/how-to-explain-my-current-research/"/>
   <updated>2012-05-02T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2012/05/how-to-explain-my-current-research</id>
   <content type="html">&lt;h2&gt;I'm having trouble explaining my current research interests.&lt;/h2&gt;

&lt;p&gt;I'm currently working on suicide and drought, heart disease and woodsmoke, violent deaths and heatwaves and a theoretical text on methods for rates, standardisation or adjustment in regression models.&lt;/p&gt;

&lt;p&gt;Why? It's complicated, but...&lt;/p&gt;

&lt;p&gt;I have been working on a range of interrelated projects for the last few years that have revolved around the influence of climate on human health and wellbeing.&lt;br/&gt;
That might sound clear enough on first glance, but when we got stuck in to it we found we struggled to find very many health outcomes with really potent causal influences of climatic variables from the literature.&lt;/p&gt;

&lt;h2&gt;HUMAN HEALTH AND CLIMATE CHANGE IN OCEANIA: A RISK ASSESSMENT 2002 &lt;/h2&gt;

&lt;p&gt;This all started with my involvement with the report &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CCYQFjAA&amp;amp;url=http%3A%2F%2Fwww.health.gov.au%2Finternet%2Fmain%2Fpublishing.nsf%2Fcontent%2F2D4037B384BC05F6CA256F1900042840%2F%24File%2Fenv_climate.pdf&amp;amp;ei=OhOhT8jeIuiviQfy6fHgBA&amp;amp;usg=AFQjCNH123UycdMu__7_cnN3BcqBu7_YYg&amp;amp;sig2=PUesyxJP74acSneaziy5PA&quot; title=&quot;DHA CC HIA 2002&quot;&gt;1&lt;/a&gt; by Tony McMichael and Rosalie Woodruff.  I was a research assistant and got my first taste of integrating data across population, health and environmental domains.&lt;/p&gt;

&lt;p&gt;We did a great job, but after we'd completed that work, Colin and I reflected on how difficult it was to find those 'low haning fruit' that might be most easily analysted in this new direction of environmental epidemioligy.  We then met Neville Nicholls from the BoM and found out that there was a strong suspicion of the increased risk of suicide during droughts amongst the meteorologists (to the point they were anxious about reporting unfavourable forecasting seasonal rainfall estimates based on the SOI and El Nino weather patterns).  We ended up publishing a simple paper about that topic &lt;a href=&quot;http://www.mendeley.com/research/interannual-rainfall-variations-suicide-new-south-wales-australia-1964/&quot; title=&quot;Nicholls et al 2005&quot;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Other health outcomes suggested themselves over the course of the following few years:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;will be bullet points&lt;/li&gt;
&lt;li&gt;includes Ross River Virus in WA&lt;/li&gt;
&lt;li&gt;heart disease and woodsmoke&lt;/li&gt;
&lt;li&gt;violent deaths and heatwaves&lt;/li&gt;
&lt;li&gt;a theoretical text on methods for rates, standardisation or adjustment in regression models&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So I am struggling to get a succint statement that reflects the current focus of my research interests.
Luckily my core reasearch interest is simpler: better understanding the dynamics of the many systems involved in human ecology.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Historical GIS evidence of Dengue in the far southeast of Australia?</title>
   <link href="http://ivanhanigan.github.com/2012/05/historical-evidence-of-dengue-in-southeastern-australia/"/>
   <updated>2012-05-02T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2012/05/historical-evidence-of-dengue-in-southeastern-australia</id>
   <content type="html">&lt;h2&gt;Dengue Fever (DF) and climate&lt;/h2&gt;

&lt;p&gt;DF is a mosquito borne virus that has high public health impact and is potentially strongly influenced by climate.&lt;/p&gt;

&lt;p&gt;There is an interesting story attached to this map from a paper by Russell et al 2009 &lt;a href=&quot;http://www.google.com&quot; title=&quot;Russell, Richard C, Bart J Currie, Michael D Lindsay, John S Mackenzie, Scott A Ritchie, and Peter I Whelan. Dengue and climate change in Australia: predictions for the future should incorporate knowledge from the past. Medical Journal of Australia 190, no. 5 (2009)&quot;&gt;1&lt;/a&gt; that draws together documentary references to historic Dengue Fever (DF) transmission, some as far south as Gosford and Bourke in NSW.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Russell2009MJA.jpg&quot; alt=&quot;Russel map of Dengue Virus&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This asserted southerly extent is much further south than that delineated in the Potential Climatic Niche model of Hales et al 2002 &lt;a href=&quot;http://www.google.com&quot; title=&quot;Hales, S., N. de Wet, J. Maindonald, and A. Woodward. Potential effect of population and climate changes on global distribution of dengue fever: an empirical model. The Lancet 360, no. 9336 (2002)&quot;&gt;2&lt;/a&gt; and is used as a basis to refute the veracity (and utility) of such a model.&lt;/p&gt;

&lt;p&gt;The Hales' model determined the potential transmission zone in Australia as being constrained much further north by levels of humidity (vapour pressure).  That predictive model was based on a regression of many climatic attributes of all locations of disease transmission in a global database.  There is an ongoing debate between the epidemiology and the entomology camps over this modelling.&lt;/p&gt;

&lt;p&gt;A key issue regarding this contentious map is a reference used by Russell et al &lt;a href=&quot;http://www.google.com&quot; title=&quot;Russell, Richard C, Bart J Currie, Michael D Lindsay, John S Mackenzie, Scott A Ritchie, and Peter I Whelan. Dengue and climate change in Australia: predictions for the future should incorporate knowledge from the past. Medical Journal of Australia 190, no. 5 (2009)&quot;&gt;1&lt;/a&gt; to dengue transmission having occurred inland at Bourke (30 S) and on the coast at Gosford  (33 S, 80 km north of Sydney) in the first half of the 1900s, (ref 13 Lee et al, 1987 &amp;amp; ref 14 Lumley and Taylor 1943 ).  However, a text search of Lee et al revealed only one reference to Gosford  and that refers to the presence of vector mosquito Aedes aegypti (AE), but not dengue transmission.&lt;/p&gt;

&lt;p&gt;The second reference is by the entomologist Frank H Taylor who mentions Brooklyn NSW, on page 158 (paragraph 2) .. as &quot;the most southerly discovered location of AE&quot; (the vector of DF in this area). Now, Brooklyn is on the railway on the Hawkesbury just south of Gosford. Taylor talks about railways spreading the vector. Maybe AE was found in NSW in conjunction with steam trains and railway water tanks for steam trains (the railway to Bourke was opened in 1885).  There is no clear discussion in Lumley and Taylor of DF transmission around Brooklyn.&lt;/p&gt;

&lt;p&gt;It is possible that the references cited for this map are really just evidence of the vector distribution, not actual virus transmission.  It is also likely that the southern-most border of the AE vector would not be the southern-most fringe of DF transmission. This therefore casts doubt on Russell's map which suggests the southern limit of DF transmission to have been as far south as Gosford and Bourke in NSW (Bourke is also asserted by Russell et al as a known transmission site using these same references).&lt;/p&gt;

&lt;p&gt;What Russell may be doing is just connecting the two points - Brooklyn and Bourke (who knows if there is a single data point in between) and asserting that that line of southernmost AE proof is the southerly boundary of DF transmission.  Perhaps if AE got to Brooklyn it might also have got to Bourke .. and Bourke being hotter than Brooklyn, DF transmission may have occurred there .. but we need more evidence than Russell et al provide.&lt;/p&gt;

&lt;p&gt;(Thanks to my epidemiological friends for scouring the historical references with a thoroughly incredulous eye).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Occam's Razor, Einstein's Razor and Chamberlin's Complex Thought</title>
   <link href="http://ivanhanigan.github.com/2012/04/Occams-Razor-Einsteins-Razor-Chamberlins-Complex-Thought/"/>
   <updated>2012-04-03T00:00:00+10:00</updated>
   <id>http://ivanhanigan.github.com/2012/04/Occams-Razor-Einsteins-Razor-Chamberlins-Complex-Thought</id>
   <content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I just read TC Chamberlin's paper on &lt;a href=&quot;http://www.ipicyt.edu.mx/storage-sipicyt/materialposgrado/Chamberlin_Science_1965_The_method_of_multiple_working_hypotheses1.pdf&quot; title=&quot;TC Chamberlin 1890&quot;&gt;the Method of Multiple Working Hypotheses&lt;/a&gt; and was very taken by the concept of Complex Thought:&lt;/p&gt;

&lt;p&gt;&quot;The use of the method leads to certain peculiar habits of mind which deserve passing notice, ... it develops a habit of thought analogous to the method itself, ... a habit of parallel or complex thought.
Instead of a simple succession of thoughts in linear order, the procedure is complex, and the mind appears to become possessed of the power of simultaneous vision from different standpoints.&quot;&lt;/p&gt;

&lt;p&gt;I was struck by the difference in this method to the KISS or Keep It Sensibly Simple approach I've been taught (also sometimes misrepresented as Keep It Simple Stupid... that only applies to Stupid theories, in my view).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Occam%27s_razor&quot; title=&quot;Occam's Razor&quot;&gt;Occam's Razor&lt;/a&gt; is a principle to Keep It Very Simple:&lt;/p&gt;

&lt;p&gt;&quot;to select among competing hypotheses that which makes the fewest assumptions and thereby offers the simplest explanation of the effect.&quot;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikiquote.org/wiki/Albert_Einstein&quot; title=&quot;Einstein's Razor&quot;&gt;Einstein's Razor&lt;/a&gt; is a  a warning against too much simplicity, with it's exhortation that we can make it as simple as possible:&lt;/p&gt;

&lt;p&gt;&quot;without having to surrender the adequate representation of a single datum of experience&quot;.&lt;/p&gt;

&lt;h2&gt;Complex Thought&lt;/h2&gt;

&lt;p&gt;I love the idea that I can train myself to acheive a kind of Science Zen that unveils all kinds of complex multifactorial causal mechanisms... but I fear the Danger of Vacillation Chamberlin speaks about:&lt;/p&gt;

&lt;p&gt;&quot;Like a pair of delicately poised scales, every added particle on the one side or the other produces its effect in oscillation. But such a pair of scales may be altogether too sensitive to be of practical value in the rough affairs of life&quot;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Shane-Weiss-Reich-White.worg approach to Code Management</title>
   <link href="http://ivanhanigan.github.com/2012/03/shane-weiss-reich-white-worg-approach-to-code-management/"/>
   <updated>2012-03-20T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/03/shane-weiss-reich-white-worg-approach-to-code-management</id>
   <content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I've been thinking alot about workflows recently.  I'm talking about the data, code, decisions etc bound up in the flow of material going through any project in the collective program of work we have going on at the Centre I work at.
The group are facing tough questions about how we do things; and why.  So in my reflections I've reviewed some links I'd saved and present below a unified summary version called the...&lt;/p&gt;

&lt;h2&gt;Shane-Weiss-Reich-White.worg approach &lt;/h2&gt;

&lt;p&gt;This a synthesis I've put together of approaches to managing code in complex data analysis projects. It's named after key exponents on various blogs, wikis and web Q-and-A sites.&lt;/p&gt;

&lt;p&gt;Stackoverflow user &lt;a href=&quot;http://stackoverflow.com/users/163053/shane&quot; title=&quot;Shane&quot;&gt;Shane&lt;/a&gt; posted &lt;a href=&quot;http://stackoverflow.com/a/2292913&quot; title=&quot;Shane's Post&quot;&gt;this excellent comment to stackoverflow&lt;/a&gt; to:&lt;/p&gt;

&lt;p&gt;&quot;start off with one R file as you start a project (or a set of files like in the &lt;a href=&quot;https://github.com/berndweiss&quot; title=&quot;Bernd Weiss&quot;&gt;Bernd Weiss&lt;/a&gt; and &lt;a href=&quot;http://stackoverflow.com/users/136862/josh-reich&quot; title=&quot;Josh Reich&quot;&gt;Josh Reich&lt;/a&gt; examples), and progressively add to it (so that it grows in size) as you make discoveries.&quot;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/berndweiss&quot; title=&quot;Bernd Weiss&quot;&gt;Bernd Weiss'&lt;/a&gt; projects have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;analysis,&lt;/li&gt;
&lt;li&gt;data and&lt;/li&gt;
&lt;li&gt;document directories and&lt;/li&gt;
&lt;li&gt;README.org (an Emacs org-mode file).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Bernd and Jeromy Anglim had an interesting discussion about this workflow in &lt;a href=&quot;http://stats.stackexchange.com/a/4506&quot; title=&quot;Bernd and Jeromy discuss the Weiss approach on stackexchange&quot;&gt;this post at stackexchange&lt;/a&gt;.  Especially note that Bernd recommends that every publication, presentation or semester/class etc. has its own git repository.  BUT that &quot;there is one real downside: using the same dataset in different publications means to maintain different versions of 'initialization code' (define missing values, generate new variables etc.). To overcome this problem, Bernd decided to maintain ONE study/dataset-related repository which contains the original init-file. For each publication, presentation etc. use a copy of the original data-file as well as of the init-file (in R via file.copy()). Of course, whenever you create a new variable you'll need to modify the original init-file and do a file.copy() (which is the most annoying part of the approach).&quot;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/users/136862/josh-reich&quot; title=&quot;Josh Reich&quot;&gt;Josh Reich&lt;/a&gt; breaks projects into 4 pieces:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;load.R,&lt;/li&gt;
&lt;li&gt;clean.R,&lt;/li&gt;
&lt;li&gt;func.R and&lt;/li&gt;
&lt;li&gt;do.R&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;a href=&quot;http://www.johnmyleswhite.com/about/&quot; title=&quot;John Myles White&quot;&gt;John Myles White's&lt;/a&gt; leads the &lt;a href=&quot;http://projecttemplate.net/architecture.html&quot; title=&quot;ProjectTemplate&quot;&gt;ProjectTemplate&lt;/a&gt; package that has 'create.project(minimal = TRUE)' which creates the layout:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cache,&lt;/li&gt;
&lt;li&gt;config,&lt;/li&gt;
&lt;li&gt;data,&lt;/li&gt;
&lt;li&gt;munge,&lt;/li&gt;
&lt;li&gt;src, and&lt;/li&gt;
&lt;li&gt;README&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I've just added reports.  If a project is a little bit bigger than minimal I'll add admin, metadata, versions etc etc. I contributed that idea to the ProjectTemplate discussion list... but those guys seem to mostly use the default minimal = FALSE which creates all the possible directories including reports.  I'll try to keep it simple and just bolt on whatever bits suit my needs as I go.&lt;/p&gt;

&lt;h2&gt;Which Code Editor is the Best?&lt;/h2&gt;

&lt;p&gt;And finally the meta work holding the project together is the code editor.  Despite the old joke which describes Emacs as &lt;a href=&quot;http://upsilon.cc/~zack/blog/posts/2008/10/from_Vim_to_Emacs_-_part_1/&quot; title=&quot;Why Emacs&quot;&gt;&quot;a great operating system, lacking only a decent editor&quot;&lt;/a&gt;, this editor has killer functions for managing code.  Check out &lt;a href=&quot;http://orgmode.org/worg/&quot; title=&quot;worg&quot;&gt;worg the Emacs Org-Mode Community&lt;/a&gt;. Recently proponents of worg wrote &lt;a href=&quot;http://www.jstatsoft.org/v46/i03/paper&quot; title=&quot;Orgmode takes over the data analysis world&quot;&gt;this article&lt;/a&gt;.  Previously I've REALLY enjoyed &lt;a href=&quot;http://sourceforge.net/projects/npptor/&quot; title=&quot;NPPtoR&quot;&gt;NPPtoR&lt;/a&gt; (only available under windoof).&lt;/p&gt;

&lt;p&gt;In the words of &lt;a href=&quot;http://stackoverflow.com/users/37751/jd-long&quot; title=&quot;JD Long&quot;&gt;JD Long&lt;/a&gt; in response to &lt;a href=&quot;http://stackoverflow.com/a/2292913&quot; title=&quot;Shane's Post&quot;&gt;Shane&lt;/a&gt; &quot;The choice of the specific tool is more idiosyncratic and not near as important as using SOMETHING.&quot;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Outlines. A way to organise, and to think</title>
   <link href="http://ivanhanigan.github.com/2012/03/outlines-way-to-organise-and-to-think/"/>
   <updated>2012-03-11T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/03/outlines-way-to-organise-and-to-think</id>
   <content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In a &lt;a href=&quot;http://ivanhanigan.github.com/2012/02/the-organisation-of-material/&quot; title=&quot;The Organisation of Material&quot;&gt;previous post&lt;/a&gt; I talked about the conceptual framework I draw on for organising material about complex systems. I quoted a passage by Koestler 1967 that talks about the writing process describing the use of 'outlines', albeit without using that term.  I find this quote particularly inspires me to think about how to select the material for inclusion (and identify the material ruled out of scope - those times we decide to &quot;chop off entire flowering branches from the tree and start growing them afresh&quot;).&lt;/p&gt;

&lt;h2&gt;A way to organise&lt;/h2&gt;

&lt;p&gt;Outlines are a method for writing, and also the basis of a toolbox called &lt;a href=&quot;http://en.wikipedia.org/wiki/Outliner&quot; title=&quot;Wikipedia Outliners&quot;&gt;'outliners'&lt;/a&gt;. I've been using the excellent free outliner &lt;a href=&quot;http://code.google.com/p/keynote-nf/&quot; title=&quot;Keynote-NF&quot;&gt;Keynote-NF&lt;/a&gt; for years and it has been great.  Recently I was inspired by this &lt;a href=&quot;http://www.jstatsoft.org/v46/i03/&quot; title=&quot;jstatsoft&quot;&gt;paper by Schulte et al&lt;/a&gt;, which very convincingly introduced me to Emacs Orgmode as a viable tool for someone without a computer science background (others had described Emacs as 'not an editor so much as a lifestyle choice' or 'a good operating system just lacking a decent editor').&lt;/p&gt;

&lt;h2&gt;A way to think&lt;/h2&gt;

&lt;p&gt;I recently had the privilege to attend a class with renowned epidemiologist Professor Nancy Krieger who gave us a valuable insight to her workflow.  She told us that for years she'd started every research project by writing out the outline and then filled out the branches much as Koestler's quote described.  Prof Krieger then said that she didn't explicitly write outlines anymore, she'd been doing it for so long it was deeply ingrained and natural part of her way of thinking.&lt;/p&gt;

&lt;p&gt;Prof Krieger went on to describe how she conducts her research workflow through the outline&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;first sketch the outline suggested by the hypotheses to be tested (data sourced,  methods applied)&lt;/li&gt;
&lt;li&gt;review the literature, there are very few areas of study that haven't been looked at by someone else&lt;/li&gt;
&lt;li&gt;create empty 'table shells' showing the form that the information will be presented in the final paper&lt;/li&gt;
&lt;li&gt;exploratory data analysis and graphics&lt;/li&gt;
&lt;li&gt;primary data analysis and interpretation of the results&lt;/li&gt;
&lt;li&gt;another literature review - identify the dominant story people are telling about this question&lt;/li&gt;
&lt;li&gt;write the discussion. Deal with any caveats head-on, how the bias may inflate or deflate estimates&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Writing workflow depends on forum&lt;/h2&gt;

&lt;p&gt;The flow of steps described above remind me &lt;a href=&quot;http://andrewgelman.com/2011/05/my_new_writing/&quot; title=&quot;Andrew Gelman's New Writing&quot;&gt;Andrew Gelman's point&lt;/a&gt; about how he writes science papers like this, but doesn't write blogs this way... he mentions thoughts on style, audience etc.  There may be a dichotomy worth exploring between clear structure vs evolving narrative.&lt;/p&gt;

&lt;h2&gt;Key features of outliner software&lt;/h2&gt;

&lt;p&gt;To conclude I'll just mention the most interesting features I've found in outliner software and will discuss these in a future post:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nodes: these are the building blocks&lt;/li&gt;
&lt;li&gt;Parents and children: there is a strict ordering of the nodes into a hierarchy&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.patternlanguage.com/archives/alexander1.htm&quot; title=&quot;A City is not a Tree&quot;&gt;Semilattice:&lt;/a&gt; this allows connections to be made across branches, not just through the hierarchy&lt;/li&gt;
&lt;li&gt;Folding can be used to hide entire sections of the tree, unfolding to 'drill down' to deep levels&lt;/li&gt;
&lt;li&gt;Hoisting: to promote and demote branches, including all children branches&lt;/li&gt;
&lt;li&gt;Lifting and grafting: easy reshuffling of the order&lt;/li&gt;
&lt;li&gt;Checked nodes: chosen nodes can be exported while the rest remain invisible&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Dr Tom Ford disentangles climate change writing</title>
   <link href="http://ivanhanigan.github.com/2012/02/dr-tom-ford-disentangles-climate-change-writing/"/>
   <updated>2012-02-18T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/02/dr-tom-ford-disentangles-climate-change-writing</id>
   <content type="html">&lt;h1&gt;Climate change and contemporary fiction&lt;/h1&gt;

&lt;p&gt;My friend Dr Tom Ford &lt;a href=&quot;http://climatechangefiction.blogspot.com.au&quot; title=&quot;Climate Change and Contemporary Fiction&quot;&gt;blogs&lt;/a&gt; about how climate and climate change are entangled in contemporary fiction. For a social scientist he does a really good job of disentangling esoteric and specialist environmental science knowledge with literary waffle and voodoo (sorry, that's an in-joke... I consider my self a waffly exponent of enviro voodoo too - but more than happy to cast aspersions and sling defamatory insults around).&lt;/p&gt;

&lt;p&gt;Tom's bag as far as I can tell is to reflect on the creation and development of literary constructs used by writers to talk about climate, and climate change. Is this a whole new category of post modern and existentialist literature?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Organisation of Material</title>
   <link href="http://ivanhanigan.github.com/2012/02/the-organisation-of-material/"/>
   <updated>2012-02-06T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/02/the-organisation-of-material</id>
   <content type="html">&lt;p&gt;During the course of my research I have repeatedly found the organisation of material to be a challenge (I'm talking about the code, data, text and everything else related to analyses).  One of the things I often struggle with is just keeping my thoughts clear and consistent between projects, through weeks and across years.  I'll try to blog about how I have decided to manage my work, and the tools I have tried and end up using.&lt;/p&gt;

&lt;p&gt;To start with, I'd like to share a passage taken from pages 55-57 of Arthur Koestler's &quot;The Ghost in the Machine&quot;, 1967, London, Pan Books, with some rephrasing of my own.&lt;/p&gt;

&lt;p&gt;&quot;The vexed problem of the 'organisation of material'; vexed because the different aspects of the problem, the welter of evidence and the welter of interpretations, are all interconnected like threads in a Persian carpet. The author is keenly aware of the pattern they form; but how can he convey that pattern if he has to unpick the threads in order to explain them one at a time? Here the problem of temporal order begins to intrude, although his mind may still be functioning in the partly or wholly non-verbal regions of images and intimations.&lt;/p&gt;

&lt;p&gt;At last he arrives at a tentative arrangement of his material, under a series of headings and sub-headings, which he shuffles about as if they were compact building blocks. They are probably each represented by a mere jotted key-word....&lt;/p&gt;

&lt;p&gt;...now the time has come for these intentional seeds to start growing into saplings which will branch out into sections, sub-sections, and so on: the selection of evidence to be quoted, of illustrations, comment and anecdotes, each of them necessitating further strategic choices. At each node - branching point - of the growing tree, more details are filled in, until at last the syntactic level is reached, the phrase generating machine takes over, the individual words are lined up - some effortlessly, some after a painful search, and are finally transformed into patterns of contractions of finger muscles guiding a pen: the logos has become incarnate.&lt;/p&gt;

&lt;p&gt;But of course the process is never quite as neat and orderly as that; trees do not grow in this rigidly symmetrical way. In our schematised account, the selection of the actual words occurs only at an advanced stage of the process, after the general plan and the ordering of the material have been decided on, and the buds of the tree are ready to burst open in their proper left-to-right order. In reality, however, one branch somewhere in the middle might blossom into words, while others have as yet hardly started to grow. And while it is true that the idea precedes the actual process of verbalisation, it is also true that ideas are often airy nothings until they crystallise into verbal concepts and acquire tangible shape....&lt;/p&gt;

&lt;p&gt;Thus our tree progresses with irregular growth and constant oscillations between levels. Transforming thought into language is not a one-way process; the sap flows in both directions, up and down the branches of the tree. The operation is further complicated and sometimes brought to the verge of a breakdown by the author's deplorable tendency to correct, erase, chop off entire flowering branches from the tree and start growing them afresh&quot;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>My Interests</title>
   <link href="http://ivanhanigan.github.com/2012/01/my-interests-2/"/>
   <updated>2012-01-24T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/01/my-interests-2</id>
   <content type="html">&lt;p&gt;My research interests revolve around making my data analyses easier and more reproducible. I'd like it if every step in the methodical exploration of (or prediction from) data is easily documented, reproduced, transformed, integrated and fun ... (if such a thing is possible).&lt;/p&gt;

&lt;p&gt;I conceptualise data analyses as grouped networks of the many choices and revisions an analyst makes through a complex workflow in a project, clusters of which make up the many projects, databases and codebases analysts use everyday.&lt;/p&gt;

&lt;p&gt;In my work as a data manager at a research school, I spend a lot of time linking together datasets to analyse population, health and environmental dimensions.  There are complex relationships to be found but because there are so many steps required for such an analysis workflow, and there are strong barriers to reproducibility.  A key barrier is due to the difficulties of tracking and documenting the numerous generations of derivative datasets and analyses.  I have developed an interest in software applications used during workflow actions and decision making to document a reproducible module of work; in desperately entwined and heroically integrated analyses.&lt;/p&gt;

&lt;h2&gt;My Topics&lt;/h2&gt;

&lt;p&gt;I am interested in systems analysis, especially of environmental health systems and&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;interventions&lt;/li&gt;
&lt;li&gt;catastrophes&lt;/li&gt;
&lt;li&gt;explanations&lt;/li&gt;
&lt;li&gt;predictions&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I also focus on&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sucide and Drought in Southern Australia&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;and Atmospherics, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pollution events from Bushfires, dust storms, aeroallergen peaks&lt;/li&gt;
&lt;li&gt;average exposures and extremes to temperature, humidity, rainfall... combinations of these&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I also dabble in other environmental health issues such as mosquito diseases and drinking water pathogens.&lt;/p&gt;

&lt;h2&gt;My Tools&lt;/h2&gt;

&lt;p&gt;I try do all my data integrations and analysis in R/Sweave.  I am currently planning to investigate the following tools&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;R/Sweave/make&lt;/li&gt;
&lt;li&gt;packages&lt;/li&gt;
&lt;li&gt;python&lt;/li&gt;
&lt;li&gt;version control (git)&lt;/li&gt;
&lt;li&gt;graphviz&lt;/li&gt;
&lt;li&gt;projectTemplate&lt;/li&gt;
&lt;li&gt;workflow apps: kepler\taverna\RanalyticFlow\Rgraphviz&lt;/li&gt;
&lt;li&gt;disentangleThings&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;My Intentions&lt;/h2&gt;

&lt;p&gt;In this blog I will write about my experiences as I dealve into these new areas and write a PhD about them (and my research topics, knotted problems to be disentangled as we go). I am not a very adept programmer, but hopefully these tools will help enable me to deal with the intergrated analyses I hope to do.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>About My Research</title>
   <link href="http://ivanhanigan.github.com/2012/01/about-my-research/"/>
   <updated>2012-01-19T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/01/about-my-research</id>
   <content type="html">&lt;p&gt;I am enticed by the work out there at the moment that challenges us to use the tools of open science (open software and open access publication) to make science more reproducible, transparent and awesome.  Science December 2 2011 Volume 334 (URL &lt;a href=&quot;http://www.sciencemag.org/content/334/6060.toc&quot;&gt;here&lt;/a&gt;) has a special section on replication and reproducibility.  Especially Roger Pengs perspective on:
limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.&lt;/p&gt;

&lt;p&gt;At the moment I am keenly aware of barriers to replicability due to the voluminous generation of data and the multitude of analysis workflow decisions that can affect the results of the kind of integrated analyses I am involved with ... choices include those relating to: which health outcomes are selected? which exposure estimates? of which environmental dataset?&lt;/p&gt;

&lt;p&gt;How data is linked together; population, health and environmental data is relevant to our ability to disentangle the complex relationships to be found.   I've had a difficult time due to multiple revisions on datasets and analysis plans that come from working with multidisciplinary teams of epidemiologists, environmental scientists and biostatisticians.  I studied geography and ecology in my undergraduate degree, so am able to link multiple layers of data together in a Geographical Information System (GIS), but what I think I need is an Integrative Information Systems (IIS has a nice ring to it).&lt;/p&gt;

&lt;p&gt;I try do all my weather/air pollution/health/demography data integrations and analysis using the Reproducible Research Reporting paradigm implemented in R/Sweave.  I do this so I can maintain tight control over modelling assumptions or data decisions at any point in the workflow, including multiple versions over the course of an evolving analysis plan.  This allows me to 'drill down' into parts of the data preparation and analysis many months after the bulk of the work has been done, change key portions that respond to the changed requirements of the project, and document the reason for the changes (in case of the inevitable change in requirements, see this xkcd comic strip &lt;a href=&quot;http://xkcd.com/844/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So in summary, I am interested in tools that enable analysts to deal with the issues of intergrated analysis networks, and tracking the many choices an analyst makes through a complex web of the analysis workflow between data, analysis, reporting and archiving activities.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>About Me</title>
   <link href="http://ivanhanigan.github.com/2012/01/about-me/"/>
   <updated>2012-01-17T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/01/about-me</id>
   <content type="html">&lt;p&gt;About Me&lt;/p&gt;

&lt;p&gt;I am a data manager and analyst at NCEPH &lt;a href=&quot;http://nceph.anu.edu.au/&quot;&gt;http://nceph.anu.edu.au/&lt;/a&gt; at the Australian National University. I am also undertaking a multi-disciplinary PhD, on a part time basis.&lt;/p&gt;

&lt;p&gt;My research focus is on linking population, health and environmental data for epidemiological analysis.&lt;/p&gt;

&lt;p&gt;I apply integrative techniques to analysis of environmental health issues such as:
1 Suicide and Drought
2 Air pollution, weather and mortality or morbidity
3 Aeroallergens
4 and more.&lt;/p&gt;

&lt;p&gt;This blog reflects my work on these various analyses, and the tools and techniques I apply there.&lt;br/&gt;
It is called Disentangle Things as both a description of what I try to do (disentangle is subtly different from untangle - more on this later) and also an exhortation.&lt;/p&gt;

&lt;p&gt;So let's Disentangle Things!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Progress Report</title>
   <link href="http://ivanhanigan.github.com/2012/01/progress-report/"/>
   <updated>2012-01-15T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/01/progress-report</id>
   <content type="html">&lt;p&gt;I have had another crack at this conversion. No chance yet to tweak the colour scheme (I want green [EDIT this was a change to stylesheets/screen.css at #header and #footer background-color: #238B45;]).&lt;/p&gt;

&lt;p&gt;Most of the obvious traces of Scott have gone, but it still looks like a rip-off.  Hoping to get a distinct look soon.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Started a blog with Jekyll</title>
   <link href="http://ivanhanigan.github.com/2012/01/started-a-blog-with-jeckyll/"/>
   <updated>2012-01-13T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2012/01/started-a-blog-with-jeckyll</id>
   <content type="html">&lt;p&gt;I have decided to start my website and blog on Jekyll on the basis that Scott Chamberlain has posted this blog on his move from Blogger and Wordpress to Jekyll.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://schamberlain.github.com/2012/01/moving-from-blogger-wordpress-to-jekyll/&quot;&gt;http://schamberlain.github.com/2012/01/moving-from-blogger-wordpress-to-jekyll/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am especially enticed by Scott's statements:&lt;/p&gt;

&lt;p&gt;&quot;What is the most important thing about science? That it is reproducible of course. Documenting your code and sharing with everyone on GitHub or SVN, etc. is great for science in facilitating collaboration and facilitating transparency. &quot;&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;&quot;it is nice for scientists to use this workflow all the time.&quot;&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;&quot;Look for your favorite, and git clone it. Edit the template you have cloned, and commit and push to GitHub&quot;&lt;/p&gt;

&lt;p&gt;So I have simply cloned Scott's website and started changing the content to my own.&lt;/p&gt;

&lt;p&gt;Hoping to find all the pieces of the puzzle in the next few weeks and have the conversion completed as soon as possible.  Then I'll try to resist the temptation to tweak the colour schemes endlessly and just keep my focus on the scientific research at hand!&lt;/p&gt;

&lt;p&gt;Disentangle Things!&lt;/p&gt;

&lt;p&gt;Thanks Scott!&lt;/p&gt;
</content>
 </entry>
 
 
</feed>