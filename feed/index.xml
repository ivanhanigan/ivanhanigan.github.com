<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>DisentangleThings</title>
 <link href="http://ivanhanigan.github.com/feed/" rel="self"/>
 <link href="http://ivanhanigan.github.com/"/>
 <updated>2015-12-22T17:48:55+11:00</updated>
 <id>http://ivanhanigan.github.com/</id>
 <author>
   <name>ivanhanigan</name>
   <email>ivan.hanigan@gmail.com</email>
 </author>

 
 <entry>
   <title>My framework of scientific workflow and integration software for holistic data analysis</title>
   <link href="http://ivanhanigan.github.com/2015/12/my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis/"/>
   <updated>2015-12-22T00:00:00+11:00</updated>
   <id>http://ivanhanigan.github.com/2015/12/my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis</id>
   <content type="html">&lt;p&gt;Scientific workflow and integration software for holistic data analysis (SWISH) is a
title I have given to describe the area of my research that focuses on the tools and techniques
of reproducible data analysis.&lt;/p&gt;

&lt;p&gt;Reproducibility is the ability to recompute the results of a data
analysis with the original data.  It is possible to have analyses that
are reproducible with varying degrees of difficulty. A data
analysis might be reproducible but require thousands of hours of work to
piece together the datasets, transformations, manipulations, calculations and interpretations of computational results.
A primary challenge to reproducible data analysis is to make analyses
that are &lt;em&gt;easy&lt;/em&gt; to reproduce.&lt;/p&gt;

&lt;p&gt;To achieve this, a guiding principle is that analysts should
effectively implement 'pipelines' of method steps and tools.  Data
analysts should employ standardised and evidence-based methods based
on conventions developed from many data analysts approaching the
problems in a similar way, rather than each analyst configuring
pipelines to suit particular individual or domain-specific
preferences.&lt;/p&gt;

&lt;h2&gt;Planning and implementing a pipeline&lt;/h2&gt;

&lt;p&gt;It can be much easier to conceptualise a complicated data analysis
method than to implement this as a reproducible research pipeline. The
most effective way to implement a pipeline is by methodically tracking
each of the steps taken, the data inputs needed and all the outputs of
the step.  If done in a disciplined way then the analyst or some other
person could 'audit' the procedure easily and access the details of
the pipeline they need to scrutinise.&lt;/p&gt;

&lt;h3&gt;Toward a standardised data analysis pipeline framework&lt;/h3&gt;

&lt;p&gt;In my own work I have tried a diverse variety of configurations based on
things I have read and discussions I have had.  Coming to the end of
my PhD project I have reflected on the framework that I have arrived at and
present this below as a schematic overview.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  *   /home/
  **    /overview.org 
           - summary data_inventory
           - DMP
  **    /worklog.org    
           - YYYY-MM-DD
  **   /projects/
  ***      /project1_data_analysis_project_health_research
               - index.org
               - git (local private, gitignore all subfolders)
               - workplan
               - worklog
               - workflow
                   - main.Rmd
  ****         /data1_provided
  ****         /data2_derived
  *****            - workflow script
  ****         /code
  ****         /results/  (this has all the pathways explored)
  *****            - README.md
                   - git (public Github)
                   /YYYY-MM-DD-shortname (i.e. EDA, prelim, model-selection, sensitivity)
                       /main.Rmd
                       /code/
                       /data/
  ****         /report/
                   /manuscript.Rmd
                       - main results recomputed in production/publication quality
                       - supporting_information (but also can refer to github/results)
                   /figures_and_tables/
                       - png
                       - csv
  *****           /journal_submission/
                       - cover letter
                       - approval signatures
                       - submitted manuscript
  *****           /journal_revision/
                       - response.org
  ***      /project2_data_analysis_project_exposure_assessment
               - index.org
               - git
               /data1_provided
               /data2_derived 
                   - stored here or
                   - web2py crud or
                   - geoserver
               /reports/
                   - manuscript.Rmd -&amp;gt; publish with the data somehow
               /tools (R package)
                   - git/master -&amp;gt; Github
           /methods_or_literature_review_project
  **   /tools/
           /web2py
               /applications
                   /data_inventory
                       - holdings
                       - prospective
                   /database_crud
            /disentangle (R package)
            /pipeline_templates
  **   /data/
           /postgis_hanigan
           /postgis_anu_gislibrary
           /geoserver_anu_gislibrary
  **   /references/
           - mendeley
           - bib
           - PDFs annotated
  **   /KeplerData/workflows/MyWorkflows/
  ***      /data_analysis_workflow_using_kepler (implemented as an R package)
  ****         /inst/doc/A01_load.R
  ***      /data_analysis_workflow_using_kepler (implemented as an R LCFD workflow)
               - main.Rmd (raw R version)
               - main.xml (this is kepler)
  ****         /data/
                   - file1.csv
                   - file2.csv
  ****         /code/
                   - load.R
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 
</feed>
