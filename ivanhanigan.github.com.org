
#+TITLE:ivan hanigan github website 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* Open Notebook
*** COMMENT research-code
#+name:research
#+begin_src R :session *R* :tangle research.markdown :exports none :eval no :padline no
---
name: research
layout: default
title: Open Notebook
---

#### Welcome to my Open Notebook
This is the public face of my Open Notebook, in which I keep all the details of the data, code and documents related to my research.  This is an Open Notebook with [Selected Content - Delayed](http://onsclaims.wikispaces.com/) and aligns with the principles of the Open Notebook Science (ONS) movement.  The private side of my Open Notebook (the closed bit) is private either because it includes unpublished work that I wish to keep embargoed until after publication, or because it is all the gory, messy details of the day-to-day business of writing and rewriting code and prose to analyse data and make sense of the data I am analysing.  These elements of the notebook do not look like standalone journal entries and I store my personal archive either hosted by [GitHub](https://github.com/ivanhanigan/) for the public parts (thanks to their superior integration with Jekyll websites thanks to gh-pages for each repository) or [BitBucket](https://bitbucket.org/ivanhanigan/) for the private bits (thanks to bitbucket's free unlimited private repositories).

#### Categories
The different categories can be thought of as seperate lab notebooks. My projects are connected by being placed into one of these categories. 

- [cloud building](http://ivanhanigan.github.io/categories/cloud%20building/): the experiments I am conducting in building Virtual Labs in the cloud
- [Data Documentation](http://ivanhanigan.github.io/categories/Data%20Documentation/): how metadata tools interact with data analysis tools
- [disentangle things](http://ivanhanigan.github.io/categories/disentangle%20things/): conceptual and methodological insights regarding complexity
- [e-collaboration](http://ivanhanigan.github.io/categories/e-collaboration/): my interactions with other scientists online
- [ecosocial tipping points](http://ivanhanigan.github.io/categories/ecosocial%20tipping%20points/): a major branch of my research 
- [extreme weather events](http://ivanhanigan.github.io/categories/extreme%20weather%20events/): another major branch of my research 
- [overview](http://ivanhanigan.github.io/categories/overview/): high level information about why I am doing this stuff
- [research methods](http://ivanhanigan.github.io/categories/research%20methods/): low level information about how I am doing this stuff
- [spatial](http://ivanhanigan.github.io/categories/spatial/): technical details about how to use spatial tools
- [spatial dependence](http://ivanhanigan.github.io/categories/spatial%20dependence/): theoretical details about spatial statistics

#### What is Open Notebook Science?  And Why am I doing it?
In [2005 Jean-Claude Bradley launched a web-based initiative called UsefulChem](http://www.infotoday.com/it/sep10/Poynder.shtml) and named his new technique Open Notebook Science (ONS).  He described it as a way of doing science in which you make all your research freely available to the public in real time. The proposed benefits include greater impact on the public good and enhanced ability to connect with like-minded collaborators. Proposed risks of ONS practice include being scooped by competitors or falling foul of Journal rules regarding prior publication and licencing of Intellectual Property. To mitigate the proposed risks the concept of ONS was broadened to allow research to be made public after a delay.

In 2010 [Carl Boettiger](http://carlboettiger.info/2012/09/28/Welcome-to-my-lab-notebook.html) initiated an experiment "to see if any of the purported benefits or supposed risks were well-founded."  After three years of his experiment Boettiger reported that his "evidence suggests that the practice of open notebook science can faciliate both the performance and dissemination of research while remaining compatible and even synergistic with academic publishing."

This promising result has inspired me to follow these practices in my own part-time PhD and my full-time work as Data Manager at a University (to the extent I am allowed to by the rules of the University and the willingness of my boss to share our results).

#+end_src

** 2014-04-19-what-is-this-open-notebook-and-why-am-i-doing-it
#+name:what-is-this-open-notebook-and-why-am-i-doing-it-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2014-04-19-what-is-this-open-notebook-and-why-am-i-doing-it.md :exports none :eval no :padline no
---
name: what-is-this-open-notebook-and-why-am-i-doing-it
layout: post
title: What is this Open Notebook? And Why Am I Doing It?
date: 2014-04-19
categories:
- overview
---

I just revised the content of the ["About My Notebook"](/research.html) page and thought it was also relevant to post as an entry.

#### Welcome to my Open Notebook
This is the public face of my Open Notebook, in which I keep all the details of the data, code and documents related to my research.  This is an Open Notebook with [Selected Content - Delayed](http://onsclaims.wikispaces.com/) and aligns with the principles of the Open Notebook Science (ONS) movement.  The private side of my Open Notebook (the closed bit) is private either because it includes unpublished work that I wish to keep embargoed until after publication, or because it is all the gory, messy details of the day-to-day business of writing and rewriting code and prose to analyse data and make sense of the data I am analysing.  These elements of the notebook do not look like standalone journal entries and I store my personal archive either hosted by [GitHub](https://github.com/ivanhanigan/) for the public parts (thanks to their superior integration with Jekyll websites thanks to gh-pages for each repository) or [BitBucket](https://bitbucket.org/ivanhanigan/) for the private bits (thanks to bitbucket's free unlimited private repositories).

#### Categories
The different categories can be thought of as seperate lab notebooks. My projects are connected by being placed into one of these categories. 

- [cloud building](http://ivanhanigan.github.io/categories/cloud%20building/): the experiments I am conducting in building Virtual Labs in the cloud
- [Data Documentation](http://ivanhanigan.github.io/categories/Data%20Documentation/): how metadata tools interact with data analysis tools
- [disentangle things](http://ivanhanigan.github.io/categories/disentangle%20things/): conceptual and methodological insights regarding complexity
- [e-collaboration](http://ivanhanigan.github.io/categories/e-collaboration/): my interactions with other scientists online
- [ecosocial tipping points](http://ivanhanigan.github.io/categories/ecosocial%20tipping%20points/): a major branch of my research 
- [extreme weather events](http://ivanhanigan.github.io/categories/extreme%20weather%20events/): another major branch of my research 
- [overview](http://ivanhanigan.github.io/categories/overview/): high level information about why I am doing this stuff
- [research methods](http://ivanhanigan.github.io/categories/research%20methods/): low level information about how I am doing this stuff
- [spatial](http://ivanhanigan.github.io/categories/spatial/): technical details about how to use spatial tools
- [spatial dependence](http://ivanhanigan.github.io/categories/spatial%20dependence/): theoretical details about spatial statistics

#### What is Open Notebook Science?  And Why am I doing it?
In [2005 Jean-Claude Bradley launched a web-based initiative called UsefulChem](http://www.infotoday.com/it/sep10/Poynder.shtml) and named his new technique Open Notebook Science (ONS).  He described it as a way of doing science in which you make all your research freely available to the public in real time. The proposed benefits include greater impact on the public good and enhanced ability to connect with like-minded collaborators. Proposed risks of ONS practice include being scooped by competitors or falling foul of Journal rules regarding prior publication and licencing of Intellectual Property. To mitigate the proposed risks the concept of ONS was broadened to allow research to be made public after a delay.

In 2010 [Carl Boettiger](http://carlboettiger.info/2012/09/28/Welcome-to-my-lab-notebook.html) initiated an experiment "to see if any of the purported benefits or supposed risks were well-founded."  After three years of his experiment Boettiger reported that his "evidence suggests that the practice of open notebook science can faciliate both the performance and dissemination of research while remaining compatible and even synergistic with academic publishing."

This promising result has inspired me to follow these practices in my own part-time PhD and my full-time work as Data Manager at a University (to the extent I am allowed to by the rules of the University and the willingness of my boss to share our results).

#+end_src


* COMMENT deprecated
** Open Notebook
*** research
**** research header
#+name:research-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
---
name: research
layout: default
title: Open Notebook
---


#* Data Documentation: [Notes](/data-doco.html)
#* [Farmer Suicide and Drought](http://www.pnas.org/content/early/2012/08/08/1112965109.full.pdf+html) 
#* Incidence Rates, Standardisation and Adjustment
#* [Open Notebook Science - Theory and Practice](/categories/ons)
#* [OpenSoftware-RestrictedData](http://opensoftware-restricteddata.github.io)
#* [pres](http://opensoftware-restricteddata.github.io/presentations/NCCARF-2013/presentation-hanigan-final.pdf)
#* PhD thesis: Disentangling the Health Impacts of Environmental Change from Social Factors      
#*  Pumilio-Bushfm: Bioacoustics Server: [Notes](/pumilio-bushfm-index.html),  [Report](http://ivanhanigan.github.io/pumilio-bushfm)
#* [Scientific Workflow Software](/categories/workflow)
#* [Spatio-temporal regression models](/categories/spatial dependence/)
#* TransformSurveyTools: Notes, Report, [Tools](https://github.com/ivanhanigan/TransformSurveyTools)
#* [Transformational-Adaptation-Energymark](/categories/energymark): transformations in Energy use
#* Transformational-Adaptation-Farmers: transformational adaptation in a sample of farmers

#+end_src  

** COMMENT categories
*** workflow
**** workflow header
#+name:workflow-header
#+begin_src markdown :tangle categories/workflow.md :exports none :eval no :padline no
---
name: workflow
layout: default
title: workflow
---

# [A workflow post](www.google.com)

    
#+end_src

** posts
*** 2014-01-14-Github-gh-pages-and-disqus-comments
#+name:datasharing-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2014-01-14-github-gh-pages-and-disqus-commentsdatasharing.md :exports none :eval no :padline no
  ---
  name:  github-gh-pages-and-disqus-comments
  layout: post
  title: Github, gh-pages and disqus comments 
  date: 2014-01-14
  categories:
  - research methods
  ---
  
  A while ago I posted about [sharing-and-extending-research-protocols](http://ivanhanigan.github.io/2013/11/sharing-and-extending-research-protocols/).

  I've started a new experiment for hosting a discussion around issues, suggesting new issues, agreeing on solutions, toward an agreement on methods that could become a protocol:
  [http://ivanhanigan.github.com/datasharing](http://ivanhanigan.github.com/datasharing).
  
  I forked the material from the original author Jeff Leek [https://github.com/jtleek/datasharing/network](https://github.com/jtleek/datasharing/network).
  
  The goal of my experiment is something along the lines of the Prometheus Wiki [http://prometheuswiki.publish.csiro.au](http://prometheuswiki.publish.csiro.au)​ which is a site for sharing research protocols. That idea is to give people a place to post research protocols since everyone develops them and then mentions them in papers but they rarely make it online in a usable format.
  
  But I was talking with an user of that and he complained it lacked a kind of "dynamic collaboration with a front-end markup system in place that was integrated with a good website-type backend".  This is what the github site might be able to do.
  
  I discussed with a colleague and he seemed to be receptive to experimenting with this, so long as it was not more cumbersome than:
  
  - shooting off an email with a list of points or
  - catching me in the tea room and saying "by the way - missing values should never be -9999"
  -  and then these being copied into a master document we all share.
  
  The system I'm using in the proposed experiment uses the hi-tech tools gh-pages with disqus comments.  This let's:
  
  - casual users chip in their two cents worth quickly via the  comments,
  - users can vote up or vote down other peoples comments,
  - track the discussion via their emails (if they choose that option),
  - but those wanting  deeper involvement can fork and edit the pages and then submit pull  requests to the lead author. 
  - Github's wiki and issues tracking functionality also could be used for serious development.
  
  
#+end_src

*** 2013-12-19-marco-fahmi-farewell-from-asn-ltern-data-portal-team
#+name:marco-fahmi-farewell-from-asn-ltern-data-portal-team-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-19-marco-fahmi-farewell-from-asn-ltern-data-portal-team.md :exports none :eval no :padline no
---
name: 2013-12-19-marco-fahmi-farewell-from-asn-ltern-data-portal-team
layout: post
title: Marco Fahmi Farewell From ASN-LTERN Data Portal Team
date: 2013-12-19
categories:
- e-collaboration
---

- This is Marco Fahmi's final week with the ASN-LTERN Data Portal Team and I'd like to take a moment to reflect on the contributions he has made.
- In this age of distributed teams across the cloud and e-commuting, the old style office whip around and card to sign is not possible so this is my attempt a farewell card using e-collaboration techniques.

#### Who is Marco Fahmi?

![marco.png](/images/marco.png)

- Marco is a great guy and a extremely good project manager.
- I got this picture of Marco from his Semaphore project team website [http://semaphoreblog.wordpress.com/team-bios/](http://semaphoreblog.wordpress.com/team-bios/)
- It is a little out of date, he has less hair than that now

#### Semaphore bio
    Marco sold his first piece of software in 1991; 
    a Microsoft Excel macro that was to be used in a masters research project.

    His promising career as a software developer came to an end a few months 
    before the Dotcom crash when he decided to leave the dungeon and see the world.

    Marco has been an academic Ronin since 2000. In his spare time, he plays Capoeira, 
    writes a PhD dissertation, spends time with his family and 
    contemplates the ideal work-life balance.

<p></p>

#### Also on twitter [https://twitter.com/fahmiger](https://twitter.com/fahmiger)
    I do lots of things -- sometimes upside down.    

<p></p>

#### Marco's achievements with ASN-LTERN

- A full list of Marco's achievements in relation to recent TERN and ANDS projects is formidable and I cannot do justice here.
- I particularly want to note that Marco was instrumental in setting up the first Data Portal and worked very hard getting the Metacat Service working for the Australian Supersite Network (ASN)
- Then Marco spread the joy of that to the Long Term Ecological Network (LTERN) project 
- Now the two facilities enjoy a solid platform to build our data portals on.
- Marco was also very influential in me starting to use this blog as my Open Notebook and has offered sage advice

#### THANKS MARCO
- Next year Marco is going to move on to other adventures.
- On behalf of myself and the rest of the team I'd like to say a big THANKYOU!

#### Quotable quotes by Marco

- Science Comes First... Well actually people come first, then Science.
- People Come First, then Science... Well actually money comes first, then People, then Science.     
- Follow the lying person to the door of their house, if they continue lying follow them inside to see where they will stop (my paraphrasing, sorry if I muddled it up.)

#### Please pop a note in the comments if you want to send Marco a message.

#+end_src

*** 2013-09-22-transformational-adaptation
#+name:transformational-adaptation-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
---
name: transformational-adaptation
layout: post
title: transformational-adaptation
date: 2013-09-22
categories:
- Energymark
---

Energymark is about transformational adaptations as opposed to incremental adaptation..
    
#+end_src

*** 2013-09-22-using-orgmode-and-jekyll-for-open-notebook
#+name:using-orgmode-and-jekyll-for-open-notebook-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-09-22-using-orgmode-and-jekyll-for-open-notebook.md :exports none :eval no :padline no
---
name: using-orgmode-and-jekyll-for-open-notebook
layout: post
title: using-orgmode-and-jekyll-for-open-notebook
date: 2013-09-22
categories:
- research methods
---

# Using Orgmode and Jekyll for Open Notebook
Orgmode is a great notebook tool because it allows the coding, evaluation and documentation all in one.  I also want to use it to send the documentation to my blog as an Open Notebook.

If starting again I'd look into this:

- [http://orgmode.org/worg/org-tutorials/org-jekyll.html]( http://orgmode.org/worg/org-tutorials/org-jekyll.html)

But as it is I already put a lot of work into configuring a jekyll blog I cloned from Scott Chamberlain over at ROpenSci and I will just use orgmode to publish the posts related to each project, tagged as 'categories'.

But here is a problem I just found out how to solve.  For a long time I thought that because github disabled ruby plugins that the automatic generate categories index pages was broken.  Luckily Charlie Park has written up the following solution and this seems to have worked for me today:    

- [http://charliepark.org/tags-in-jekyll/](http://charliepark.org/tags-in-jekyll/)
- [http://charliepark.org/jekyll-with-plugins/](http://charliepark.org/jekyll-with-plugins/)

Cheers!

#+end_src

*** software-ism
**** head
#+name:index
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no :padline no
--- 
name: software-ism
layout: post
title: software-ism
date: 2012-09-15
categories: 
- research methods
---
I am a huge fan of the R language for statistics and graphics.

I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).

In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.

I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.

For example a simple analysis in Rstudio using the 'R Markdown document' is below. 

You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#+end_src

**** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval yes
  ---
      summary(cars)
  --- 
  
  
  | speed | dist |
  |--------------|----------------
  | Min.   : 4.0 | Min.   :  2.00  
  | 1st Qu.:12.0 | 1st Qu.: 26.00  
  | Median :15.0 | Median : 36.00  
  | Mean   :15.4 | Mean   : 42.98  
  | 3rd Qu.:19.0 | 3rd Qu.: 56.00  
  | Max.   :25.0 | Max.   :120.00  
  ---  
#+end_src
**** and

#+name:and
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no
You can also embed plots, for example:
#+end_src
**** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval no
-----
    plot(cars)

-----
#+end_src
**** img
#+name:asdf
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports code :eval no
![plot of chunk unnamed-chunk-2](/images/unnamed-chunk-2.png)

I hope we can work toward a kind of 'tower of babel'.

#+end_src

**** deprecated
***** head
#+name:index
#+begin_src markdown :tangle no :exports none :eval no :padline no
--- 
name: software-ism
layout: post
title: software-ism
date: 2012-09-15
categories: 
- research methods
---
I am a huge fan of the R language for statistics and graphics.

I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).

In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.

I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.

For example a simple analysis in Rstudio using the 'R Markdown document' is below. 

You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#+end_src

***** code
#+name:asdf
#+begin_src markdown :session *R* :tangle no :exports code :eval yes
  ---
      summary(cars)
  --- 
  
  
  | speed | dist |
  |--------------|----------------
  | Min.   : 4.0 | Min.   :  2.00  
  | 1st Qu.:12.0 | 1st Qu.: 26.00  
  | Median :15.0 | Median : 36.00  
  | Mean   :15.4 | Mean   : 42.98  
  | 3rd Qu.:19.0 | 3rd Qu.: 56.00  
  | Max.   :25.0 | Max.   :120.00  
  ---  
#+end_src
*** pioz-et-al-2012-model-selection
#+begin_src markdown :tangle _posts/2013-04-18-pioz-et-al-2012-model-selection.md :exports none :eval no :padline no
---
name: pioz-et-al-2012-model-selection
layout: post
title: Pioz et al 2012 model selection
categories:
- spatial
---

In the [GIS forum SPDEP study group](http://gis-forum.github.io/study.html) we've been discussing the Bluetongue paper [http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did](http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did-environmental-factors-influencing-the-velocity-of-blueton)

I'd like to know more about the the Lagrange Multiplier tests and Francis
raised the [seminal Anselin 1988 paper for that](http://ivanhanigan.github.io/2013/04/reflections-bob-haining/#comment-864167749)


But in this post I just wanted to summarise their model selection procedure in a flow diagram


![pioz_modelling.png](/images/pioz_modelling.png)


#+end_src
**** COMMENT pioz_modelling-code
#+name:pioz_modelling
#+begin_src R :session *R* :tangle no :exports none :eval yes
    ################################################################
    # name:pioz_modelling
    require(disentangle)
    nodes <- newnode("data", "variable selection/transformation", newgraph = T)
    nodes  <- newnode("model building dataset (75%)",
                      inputs = "data"
                      )
    nodes  <- newnode("validation dataset (25%)", "data")
    nodes  <- newnode("OLS","model building dataset (75%)")
  #  nodes  <- newnode("diagnostics", "OLS")
    nodes  <- newnode("semi-variogram of the OLS residuals", "OLS", c("200km radius"))
    nodes <- newnode("inverse distance weighting", "assumption")
    nodes  <- newnode("spatial lag model", c("200km radius", "inverse distance weighting"))
    nodes  <- newnode("spatial error model",  c("200km radius", "inverse distance weighting", "robust Lagrange Multiplier"))
    nodes <- newnode("robust Lagrange Multiplier", c("spatial lag model", "spatial error model"))
    nodes <- newnode("three thematic sets of variables", "variable selection/transformation")
    nodes <- newnode("AIC to select variables", c("spatial error model", "three thematic sets of variables"), "minimal model")
  
    nodes <- newnode("compare dir, magnt and sig", c("OLS", "minimal model"))
    nodes <- newnode("coefficient of determination","literature several pseudo-R2 have")
    nodes <- newnode("assess fit", c("minimal model","RMSE", "coefficient of determination"))
    nodes <- newnode("assess fit with validation dataset", c("validation dataset (25%)", "RMSE", "coefficient of determination"))
    nodes <- newnode("assess each covariate",  "minimal model", "LR tests, loop drop-one-test-repeat")
    nodes <- newnode("compare the OLS and spatial error results for variables", c("OLS", "LR tests, loop drop-one-test-repeat"))
    dev.copy(png,"images/pioz_modelling.png", height = 1000, width = 700, res = 105)
    dev.off(); dev.off()
#+end_src

#+RESULTS: pioz_modelling
: 1

    
*** animated-maps

#+name:animated-maps-header
#+begin_src markdown :tangle _posts/2013-07-30-animated-maps.md :exports none :eval no :padline no
--- 
name: animated-maps
layout: post
title: animated-maps 
date: 2013-07-30
categories: 
- spatial 
---

# Animated maps to allow exploration of alternate levels of 'jitter'
In a [previous project](http://www.ncbi.nlm.nih.gov/pubmed/22672028) we published a map of point locations that had been 'jittered', ie adding random noise to the latitude and longitude.  We did this by testing out a few maps and deciding on one that we thought protected privacy adequately whilst not destroying the spatial pattern we wished to display (evocatively).

![Figure 2_FINAL.jpg](/jitter/Figure 2_FINAL.jpg)

I always wondered about a way to interactively do this and I think the animation package might do the trick, with the ability to step thru levels of jittering with the pause, fwd and back buttons.

[Clink here for the same data shown in a new animation](/jitter/index.html).

# Reference
Vally, H., Peel, M., Dowse, G. K., Cameron, S., Codde, J. P., Hanigan, I., & Lindsay, M. D. a. (2012). Geographic Information Systems used to describe the link between the risk of Ross River virus infection and proximity to the Leschenault estuary, WA. Australian and New Zealand Journal of Public Health, 36(3), 229–235. doi:10.1111/j.1753-6405.2012.00869.x
    
#+end_src

*** Worflow flowcharts
see the Dropbox/projects/swish project
**** Worflow flowcharts header
#+name:Worflow flowcharts-header
#+begin_src markdown :tangle _posts/2013-07-31-worflow-flowcharts.md :exports none :eval no :padline no
  ---
  name: worflow-flowcharts
  layout: post
  title: Worflow flowcharts
  date: 2013-07-31
  categories: 
  - research methods
  ---
  
  ## What is the issue  
  Most people seem to collect multiple datasets together in a single spot that can be split into 2 or more separate data packages.  I think this is a natural set up from an analysts perspective, where the results of multiple steps accumulate as 'stepping stones' toward the file they end up analysing.  
  
  I was first taught GIS by Isabelle Balzer at Ecowise Environmental Services in Canberra.  She showed me the method of keeping a table (sticky-taped to the desk!) of all the files and transformations that were going on. This was a method that didn't allow any multitasking!  I call this the 'Balzerian Method' (I am sure others used it before Isabelle, but I think Balzerian is a great word).

  I think the data wharehouse at my work is an example, and probably we'll find the key challenge for big data will be for analysts to disentangle their own filing systems.
  
  In my experience the way people store research data is often one (or a couple, or all) of these three types:

  - a database with heaps of tables and views
  - a directory (and sub-directories) with heaps of files 
  - a spreadsheet workbook with heaps of sheets (and links to other workbooks)
  
  I am developing a tool based on the open source graphviz softawre. The tool I am developing addresses the challenge of graphing the links between these sequential steps.  

  #### Code:introducing newnode
      # NB this only works easily on linux
      require(devtools)
      install_github("disentangle", "ivanhanigan")
      require(disentangle)
      # the core of the tool is Rgraphviz, I just built a wrapper function
      # to add newnodes to a graph of nodes
      # always start with (newgraph = T) because the newnode function ADDS
      # nodes to a graph, unless told otherwise, and fails if no 'nodes'
      # object exists
      nodes  <- newnode(name="NAME",inputs="INPUT",outputs="OUTPUT", newgraph = T)

  ![images/newnode1.png](/images/newnode1.png)

  #### Code:adding nodes
      # now we can add nodes, and we can pass multiple inputs or outputs
      nodes  <- newnode(name="OUTPUT",inputs=c("NAME","ANOTHER THING"))
      # outputs are optional

  ![images/newnode2.png](/images/newnode2.png)  

  It can be used in two or three ways.  

  ## Example one, the composite view:
  So if there is a Balzerian filelist table available, convert it to a spreadsheet.  This is als similar to a labbook from Chemistry but follows a very rigid structure: NAME,        INPUTS,           OUTPUTS,         DESCRIPTION.  The first method I'll show will take one of these tables and map out the steps in the workflow.
  
  #### Code: Composite Worflow Files List
      #    so if there is a Balzerian filelist table available,
      # either make a spreadsheet with names, inputs and outputs 
      # fileslist <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
      # or 
      filesList <- read.csv(textConnection(
      'NAME,        INPUTS,           OUTPUTS,         DESCRIPTION
      FileA,        TableXYZ,         Input1,          Transformed variable
      FileB,        TableABC,         Input2,          Collapsed dimensions
      analysisFile, "Input1,Input2",  analysisResults, Merged inputs and analysed
      '), stringsAsFactors = F, strip.white = T)
      filesList

      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,"NAME"],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
        )
      }
  
  ## shows this result
  ![fileRelationships.png](/images/fileRelationships.png)
  
  ## Example two, tracking the steps while analysing data:
  Structure a script into sections and document each section before evaluating the code to execute the step.  This works well with orgmode/ESS, Sweave or knitr style workflows.
  For example:
  
  #### Code: Ad Hoc Files Lists Flowcharts
      #### step one ####
      nodes <- newnode(name="FileA", inputs="TableXYZ", outputs="Input1",
                       newgraph =T) # this is required to tell newnode to
                                    # start a new graph, rather than add to
                                    # the nodes
      FileA  <- read.table("TableXYZ.txt")
      Input1 <- log(FileA$columnZ)
       
      #### step two ####
      nodes <- newnode(name="FileB", inputs="TableABC", outputs="Input2")
      FileB  <- read.table("TableABC.txt")
      Input2 <- ddply(FileB, "id", summarise,
                      duration = max(year) - min(year),
                      nteams = length(unique(team)))
       
      #### step three ####
      nodes <- newnode(name="analysisFile", inputs=c("Input1","Input2"),
                       outputs="analysisResults")
      analysisFile  <- merge(Input1, Input2, by="id")
      analysisResults  <- lm(y ~ duration + nteams, data = analysisFile)
  
  
  ## Example three: visualising relationships
  It is not aimed at visualising the linked structure of a tree or semi-lattice but can be used in such a way but changing the nodename and inputs concept to parent/child relationships.
  
  As an example I'll describe how a list of database tables might be displayed as a tree. I am a great fan of Josh Reich due to his [LCFD workflow](http://stackoverflow.com/a/1434424), and I also like his work on the [Simple Bank](https://www.simple.com/) so when I stumbled on this [blog post](http://blog.i2pi.com/post/52812976752/joshs-postgresql-database-conventions) in which he says:
  
  "Show me your flowchart and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won’t usually need your flowchart; it’ll be obvious."
  
  I was switched on and I started thinking about how the graphVis tool could be used to describe a list of tables and views from a database.
  
  Say that two groups studied the same file TableXYZ with different inputs.  One of these groups wrote a seminal paper in the field, while their rivals wrote an inferior paper with a different result.  Imagine now a subsequent group who gathered the data from the previous work into the following database tables and conducted a replication study, with a new sensitivity analysis to explain why the original two papers produced different results.  

  Let's assume this database has all the data from all the groups in it and we want to get a pictorial view so we can disentangle which files belong to which study.  First get the following list of tables as INPUTS, grouping them by 'NAME' will give the tree structure and showing their results as OUTPUTS allows the subsequent replication study to use them as inputs and assume the position at the bottom of the flowchart.

  #### Code: database tables and different studies       
      filesList <- read.csv(textConnection(
      'NAME                 ,             INPUTS         , OUTPUTS
      The Seminal Study     ,              FileA         , 
      The Seminal Study     ,              FileB         , 
      The Seminal Study     ,       analysisFile         , 
      The Seminal Study     ,           TableXYZ         , 
      The Seminal Study     ,           TableABC         , 
      The Seminal Study     ,      Input1,Input2         ,
      The Seminal Study     ,             Input1         , 
      The Seminal Study     ,             Input2         , 
      The Seminal Study     ,      The Seminal Study     , analysisResults 
      The Inferior Rivals   ,                FileC       , 
      The Inferior Rivals   ,        analysisFileX       , 
      The Inferior Rivals   ,             TableXYZ       , 
      The Inferior Rivals   ,               InputX       , 
      The Inferior Rivals   ,    The Inferior Rivals     , analysisResultsX       
      The Replication Study ,    "Input1,Input2,TableXYZ",  analysisResultsR     
      The Replication Study ,    "Input1,InputX,TableXYZ",  sensitivityResult 
      '), stringsAsFactors = F, strip.white = T)

      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,"NAME"],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
        )
      }


      
  
  
  ## the result
  ![filesRelationships2.png](/images/filesRelationships2.png)  
#+end_src
**** man-newnode-workflow-code
#+name:man-newnode-workflow
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:man-newnode-workflow
  # NB this only works easily on linux
  require(devtools)
  install_github("disentangle", "ivanhanigan")
  require(disentangle)
  # the core of the tool is Rgraphviz, I just built a wrapper function
  # to add newnodes to a graph of nodes
  # always start with (newgraph = T) because the newnode function ADDS
  # nodes to a graph, unless told otherwise, and fails if no 'nodes'
  # object exists
  nodes  <- newnode(name="NAME",inputs="INPUT",outputs="OUTPUT", newgraph = T)
  dev.copy(png,"images/newnode1.png")
  dev.off()
  # now we can add nodes, and we can pass multiple inputs or outputs
  nodes  <- newnode(name="OUTPUT",inputs=c("NAME","ANOTHER THING"))
  dev.copy(png,"images/newnode2.png")
  dev.off()
  # outputs are optional
  
  #    so if there is a Balzerian filelist table available,
      # either make a spreadsheet with names, inputs and outputs 
      # fileslist <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
      # or 
      filesList <- read.csv(textConnection(
      'NAME,        INPUTS,           OUTPUTS,         DESCRIPTION
      FileA,        TableXYZ,         Input1,          Transformed variable
      FileB,        TableABC,         Input2,          Collapsed dimensions
      analysisFile, "Input1,Input2",  analysisResults, Merged inputs and analysed
      '), stringsAsFactors = F, strip.white = T)
      filesList
  
      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,1],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
                         )
      }
      dev.copy(png,'images/fileRelationships.png')
      dev.off();
  
  # but it was really something I designed to be used in a script like this
  #### step one ####
  nodes <- newnode(name="FileA", inputs="TableXYZ", outputs="Input1",
                   newgraph =T) # this is required to tell newnode to
                                # start a new graph, rather than add to
                                # the nodes
  FileA  <- read.table("TableXYZ.txt")
  Input1 <- log(FileA$columnZ)
  
  #### step two ####
  nodes <- newnode(name="FileB", inputs="TableABC", outputs="Input2")
  FileB  <- read.table("TableABC.txt")
  Input2 <- ddply(FileB, "id", summarise,
                  duration = max(year) - min(year),
                  nteams = length(unique(team)))
  
  #### step three ####
  nodes <- newnode(name="analysisFile", inputs=c("Input1","Input2"),
                   outputs="analysisResults")
  analysisFile  <- merge(Input1, Input2, by="id")
  analysisResults  <- lm(y ~ duration + nteams, data = analysisFile)
  
  # now generate a messy database full of tables
  require(reshape)
  require(sqldf)
  filesList$STUDY <- "The Seminal Study"
  filesList2  <- melt(filesList, id.vars = "STUDY")
  
  # now there was a second study, by rivals with only one dataset
  filesList_rivals <- read.csv(textConnection(
  'FILE,        INPUTS,           OUTPUTS,         DESCRIPTION
  FileC,        TableIJK,         InputX,          Transformed variable
  analysisFileX, InputX,  analysisResultsX,          analysed
  '), stringsAsFactors = F, strip.white = T)
  filesList_rivals$STUDY <- "The Inferior Rivals"
  filesList2  <- rbind(filesList2,
                       melt(filesList_rivals, id.vars = "STUDY")
                       )
  
  # and sometime later there is a third study that replicated the first and added a
  # sensitivity test
  filesList_replication <- read.csv(textConnection(
  'FILE,        INPUTS,           OUTPUTS,            DESCRIPTION
  analysisFileR, "Input1,Input2",  analysisResultsR, Merged inputs and analysed
  sensitivityAnalysisFile, InputX, sensitivityResult, SupportForSeminalStudy'), stringsAsFactors = F, strip.white = T)
  filesList_replication$STUDY <- "The Replication Study"
  filesList_replication
  filesList2  <- rbind(filesList2,
                       melt(filesList_replication, id.vars = "STUDY")
                       )
  filesList2
  filesList3  <- sqldf("SELECT DISTINCT STUDY, value
  FROM filesList2
  where variable != 'DESCRIPTION'")
  filesList3
  # somehow we've converted FILE to factor
  filesList3$FILE <- as.character(filesList3$FILE)
  
  filesList <- read.csv(textConnection(
  'NAME                 ,             INPUTS         , OUTPUTS
  The Seminal Study     ,              FileA         , 
  The Seminal Study     ,              FileB         , 
  The Seminal Study     ,       analysisFile         , 
  The Seminal Study     ,           TableXYZ         , 
  The Seminal Study     ,           TableABC         , 
  The Seminal Study     ,      Input1,Input2         ,
  The Seminal Study     ,             Input1         , 
  The Seminal Study     ,             Input2         , 
  The Seminal Study     ,      The Seminal Study     , analysisResults 
  The Inferior Rivals   ,                FileC       , 
  The Inferior Rivals   ,        analysisFileX       , 
  The Inferior Rivals   ,             TableXYZ       , 
  The Inferior Rivals   ,               InputX       , 
  The Inferior Rivals   ,    The Inferior Rivals     , analysisResultsX       
  The Replication Study ,   "Input1,Input2,TableXYZ" ,  analysisResultsR     
  The Replication Study ,   "Input1,InputX,TableXYZ" ,  sensitivityResult 
  '), stringsAsFactors = F, strip.white = T)
  
  for(i in 1:nrow(filesList))
  {
    nodes <- newnode(name = filesList[i,"NAME"],
                     inputs = strsplit(filesList$INPUTS, ",")[[i]],
                     outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                     newgraph = (i == 1)
    )
  }
  
  
  dev.copy(png, "images/filesRelationships2.png")
  dev.off()
  
#+end_src
*** Open Notebook System
*** ons
**** COMMENT ons header
#+name:ons-header
#+begin_src markdown :tangle _posts/2013-09-13-ons.md :exports none :eval no :padline no
  ---
  name: ons
  layout: post
  title: Starting my Open Notebook Science Blog
  date: 2013-09-13
  categories: 
  - research methods
  ---
  
  Many examples are emerging of scientists who are transitioning to a
  much more open model of research.  This is in part externally driven
  by funding bodies (such as the Aussie Research Council asking for deposit of funded data and papers) and journals
  ([ie. Nature journals removing length restrictions on Methods sections.](http://www.nature.com/ng/journal/v45/n5/full/ng.2621.html)). Also the increased value being placed on transparency of reproducible analysis to safeguard against error and fraud is becoming an internal driver within science communities.
  
  [Open Notebook Science](http://en.wikipedia.org/wiki/Open_Notebook_Science)
  (ONS) style is an extreme of transparent approaches to research.
  According to the wikipedia page it is the "practice of making the
  entire primary record of a research project publicly available online
  as it is recorded".  
  
  That's pretty extreme!  In my view a lot of stuff in the research project should probably be archived quickly and left to rot.
  
  I like the range of options available.  I think I'll go for [SCD or "Seclected Content / Delayed"](http://onsclaims.wikispaces.com/) and show their image below.  In this model a portion of the open notebook and associated supporting raw data are available after some delay. I'll try to use this blog for weekly updates on progress for each project, and provide links off my 'Open Notebook' and 'Software' Tabs.
  
  ![ONS-SCD.png](/images/ONS-SCD.png)
      
#+end_src
** projects
*** OpenSoftware-RestrictedData
*** Bioacoustics and Pumilio
**** COMMENT pumilio-code
#+name:pumilio
#+begin_src sh :session *shell* :tangle no :exports none :eval yes
################################################################
# name:pumilio
cp ~/Dropbox/projects/JCU/pumilio/pumilio.html pumilio.html
#+end_src

#+RESULTS: pumilio

*** Farmer Suicide and Drought
*** Energymark: transformations in Energy use
*** Farmer Transformations: transformational adaptation in a sample of farmers
*** PhD thesis: Disentangling the Health Impacts of Environmental Change from Social Factors      
*** Rates, Standardisation and Adjustment
*** Spatio-temporal regression models
[[~/Dropbox/projects/spatiotemporal-regression-models/spatiotemporal.org
]]
*** Workflow
[[~/Dropbox/projects/swish/swish-overview.org]]
** go
#+name:go
#+begin_src sh :session *shell2* :tangle no :exports none :eval yes
################################################################
# name:go
jekyll serve
#+end_src

  

