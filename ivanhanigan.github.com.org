#+TITLE:ivan hanigan github website 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* posts
** software-ism
*** head
#+name:index
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no :padline no
--- 
name: software-ism
layout: post
title: software-ism
date: 2012-09-15
categories: 
- software
---
I am a huge fan of the R language for statistics and graphics.

I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).

In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.

I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.

For example a simple analysis in Rstudio using the 'R Markdown document' is below. 

You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#+end_src

*** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval yes
  ---
      summary(cars)
  --- 
  
  
  | speed | dist |
  |--------------|----------------
  | Min.   : 4.0 | Min.   :  2.00  
  | 1st Qu.:12.0 | 1st Qu.: 26.00  
  | Median :15.0 | Median : 36.00  
  | Mean   :15.4 | Mean   : 42.98  
  | 3rd Qu.:19.0 | 3rd Qu.: 56.00  
  | Max.   :25.0 | Max.   :120.00  
  ---  
#+end_src
*** and

#+name:and
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no
You can also embed plots, for example:
#+end_src
*** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval no
-----
    plot(cars)

-----
#+end_src
*** img
#+name:asdf
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports code :eval no
![plot of chunk unnamed-chunk-2](/images/unnamed-chunk-2.png)

I hope we can work toward a kind of 'tower of babel'.

#+end_src

** Bob Haining
*** head
#+name:index
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no :padline no
--- 
name: software-ism
layout: post
title: software-ism
date: 2012-09-15
categories: 
- software
---
I am a huge fan of the R language for statistics and graphics.

I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).

In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.

I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.

For example a simple analysis in Rstudio using the 'R Markdown document' is below. 

You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#+end_src

*** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval yes
  ---
      summary(cars)
  --- 
  
  
  | speed | dist |
  |--------------|----------------
  | Min.   : 4.0 | Min.   :  2.00  
  | 1st Qu.:12.0 | 1st Qu.: 26.00  
  | Median :15.0 | Median : 36.00  
  | Mean   :15.4 | Mean   : 42.98  
  | 3rd Qu.:19.0 | 3rd Qu.: 56.00  
  | Max.   :25.0 | Max.   :120.00  
  ---  
#+end_src
** pioz-et-al-2012-model-selection
#+begin_src markdown :tangle _posts/2013-04-18-pioz-et-al-2012-model-selection.md :exports none :eval no :padline no
---
name: pioz-et-al-2012-model-selection
layout: post
title: Pioz et al 2012 model selection
categories:
- Spatial Dependence
- Modelling
- Disentangle
---

In the [GIS forum SPDEP study group](http://gis-forum.github.io/study.html) we've been discussing the Bluetongue paper [http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did](http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did-environmental-factors-influencing-the-velocity-of-blueton)

I'd like to know more about the the Lagrange Multiplier tests and Francis
raised the [seminal Anselin 1988 paper for that](http://ivanhanigan.github.io/2013/04/reflections-bob-haining/#comment-864167749)


But in this post I just wanted to summarise their model selection procedure in a flow diagram


![pioz_modelling.png](/images/pioz_modelling.png)


#+end_src
*** COMMENT pioz_modelling-code
#+name:pioz_modelling
#+begin_src R :session *R* :tangle no :exports none :eval yes
    ################################################################
    # name:pioz_modelling
    require(disentangle)
    nodes <- newnode("data", "variable selection/transformation", newgraph = T)
    nodes  <- newnode("model building dataset (75%)",
                      inputs = "data"
                      )
    nodes  <- newnode("validation dataset (25%)", "data")
    nodes  <- newnode("OLS","model building dataset (75%)")
  #  nodes  <- newnode("diagnostics", "OLS")
    nodes  <- newnode("semi-variogram of the OLS residuals", "OLS", c("200km radius"))
    nodes <- newnode("inverse distance weighting", "assumption")
    nodes  <- newnode("spatial lag model", c("200km radius", "inverse distance weighting"))
    nodes  <- newnode("spatial error model",  c("200km radius", "inverse distance weighting", "robust Lagrange Multiplier"))
    nodes <- newnode("robust Lagrange Multiplier", c("spatial lag model", "spatial error model"))
    nodes <- newnode("three thematic sets of variables", "variable selection/transformation")
    nodes <- newnode("AIC to select variables", c("spatial error model", "three thematic sets of variables"), "minimal model")
  
    nodes <- newnode("compare dir, magnt and sig", c("OLS", "minimal model"))
    nodes <- newnode("coefficient of determination","literature several pseudo-R2 have")
    nodes <- newnode("assess fit", c("minimal model","RMSE", "coefficient of determination"))
    nodes <- newnode("assess fit with validation dataset", c("validation dataset (25%)", "RMSE", "coefficient of determination"))
    nodes <- newnode("assess each covariate",  "minimal model", "LR tests, loop drop-one-test-repeat")
    nodes <- newnode("compare the OLS and spatial error results for variables", c("OLS", "LR tests, loop drop-one-test-repeat"))
    dev.copy(png,"images/pioz_modelling.png", height = 1000, width = 700, res = 105)
    dev.off(); dev.off()
#+end_src

#+RESULTS: pioz_modelling
: 1

    
** animated-maps

#+name:animated-maps-header
#+begin_src markdown :tangle _posts/2013-07-30-animated-maps.md :exports none :eval no :padline no
--- 
name: animated-maps
layout: post
title: animated-maps 
date: 2013-07-30
categories: 
- spatial 
- animation
---

# Animated maps to allow exploration of alternate levels of 'jitter'
In a [previous project](http://www.ncbi.nlm.nih.gov/pubmed/22672028) we published a map of point locations that had been 'jittered', ie adding random noise to the latitude and longitude.  We did this by testing out a few maps and deciding on one that we thought protected privacy adequately whilst not destroying the spatial pattern we wished to display (evocatively).

I always wondered about a way to interactively do this and I think the animation package might do the trick, with the ability to step thru levels of jittering with the pause, fwd and back buttons.

[Clink here for the same data shown in a new animation](/jitter/index.html).

# Reference
Vally, H., Peel, M., Dowse, G. K., Cameron, S., Codde, J. P., Hanigan, I., & Lindsay, M. D. a. (2012). Geographic Information Systems used to describe the link between the risk of Ross River virus infection and proximity to the Leschenault estuary, WA. Australian and New Zealand Journal of Public Health, 36(3), 229–235. doi:10.1111/j.1753-6405.2012.00869.x
    
#+end_src

** Worflow flowcharts
*** Worflow flowcharts header
#+name:Worflow flowcharts-header
#+begin_src markdown :tangle _posts/2013-07-31-worflow-flowcharts.md :exports none :eval no :padline no
  ---
  name: worflow-flowcharts
  layout: post
  title: Worflow flowcharts
  date: 2013-07-31
  categories: 
  - workflow
  - disentangle
  ---
  
  ## What is the issue  
  Most people seem to collect multiple datasets together in a single spot that can be split into 2 or more separate data packages.  I think this is a natural set up from an analysts perspective, where the results of multiple steps accumulate as 'stepping stones' toward the file they end up analysing.  
  
  I was first taught GIS by Isabelle Balzer at Ecowise Environmental Services in Canberra.  She showed me the method of keeping a table (sticky-taped to the desk!) of all the files and transformations that were going on. This was a method that didn't allow any multitasking!  I call this the 'Balzerian Method' (I am sure others used it before Isabelle, but I think Balzerian is a great word).

  I think the data wharehouse at my work is an example, and probably we'll find the key challenge for big data will be for analysts to disentangle their own filing systems.
  
  In my experience the way people store research data is often one (or a couple, or all) of these three types:

  - a database with heaps of tables and views
  - a directory (and sub-directories) with heaps of files 
  - a spreadsheet workbook with heaps of sheets (and links to other workbooks)
  
  I am developing a tool based on the open source graphviz softawre. The tool I am developing addresses the challenge of graphing the links between these sequential steps.  

  #### Code:introducing newnode
      # NB this only works easily on linux
      require(devtools)
      install_github("disentangle", "ivanhanigan")
      require(disentangle)
      # the core of the tool is Rgraphviz, I just built a wrapper function
      # to add newnodes to a graph of nodes
      # always start with (newgraph = T) because the newnode function ADDS
      # nodes to a graph, unless told otherwise, and fails if no 'nodes'
      # object exists
      nodes  <- newnode(name="NAME",inputs="INPUT",outputs="OUTPUT", newgraph = T)

  ![images/newnode1.png](/images/newnode1.png)

  #### Code:adding nodes
      # now we can add nodes, and we can pass multiple inputs or outputs
      nodes  <- newnode(name="OUTPUT",inputs=c("NAME","ANOTHER THING"))
      # outputs are optional

  ![images/newnode2.png](/images/newnode2.png)  

  It can be used in two or three ways.  

  ## Example one, the composite view:
  So if there is a Balzerian filelist table available, convert it to a spreadsheet.  This is als similar to a labbook from Chemistry but follows a very rigid structure: NAME,        INPUTS,           OUTPUTS,         DESCRIPTION.  The first method I'll show will take one of these tables and map out the steps in the workflow.
  
  #### Code: Composite Worflow Files List
      #    so if there is a Balzerian filelist table available,
      # either make a spreadsheet with names, inputs and outputs 
      # fileslist <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
      # or 
      filesList <- read.csv(textConnection(
      'NAME,        INPUTS,           OUTPUTS,         DESCRIPTION
      FileA,        TableXYZ,         Input1,          Transformed variable
      FileB,        TableABC,         Input2,          Collapsed dimensions
      analysisFile, "Input1,Input2",  analysisResults, Merged inputs and analysed
      '), stringsAsFactors = F, strip.white = T)
      filesList

      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,"NAME"],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
        )
      }
  
  ## shows this result
  ![fileRelationships.png](/images/fileRelationships.png)
  
  ## Example two, tracking the steps while analysing data:
  Structure a script into sections and document each section before evaluating the code to execute the step.  This works well with orgmode/ESS, Sweave or knitr style workflows.
  For example:
  
  #### Code: Ad Hoc Files Lists Flowcharts
      #### step one ####
      nodes <- newnode(name="FileA", inputs="TableXYZ", outputs="Input1",
                       newgraph =T) # this is required to tell newnode to
                                    # start a new graph, rather than add to
                                    # the nodes
      FileA  <- read.table("TableXYZ.txt")
      Input1 <- log(FileA$columnZ)
       
      #### step two ####
      nodes <- newnode(name="FileB", inputs="TableABC", outputs="Input2")
      FileB  <- read.table("TableABC.txt")
      Input2 <- ddply(FileB, "id", summarise,
                      duration = max(year) - min(year),
                      nteams = length(unique(team)))
       
      #### step three ####
      nodes <- newnode(name="analysisFile", inputs=c("Input1","Input2"),
                       outputs="analysisResults")
      analysisFile  <- merge(Input1, Input2, by="id")
      analysisResults  <- lm(y ~ duration + nteams, data = analysisFile)
  
  
  ## Example three: visualising relationships
  It is not aimed at visualising the linked structure of a tree or semi-lattice but can be used in such a way but changing the nodename and inputs concept to parent/child relationships.
  
  As an example I'll describe how a list of database tables might be displayed as a tree. I am a great fan of Josh Reich due to his [LCFD workflow](http://stackoverflow.com/a/1434424), and I also like his work on the [Simple Bank](https://www.simple.com/) so when I stumbled on this [blog post](http://blog.i2pi.com/post/52812976752/joshs-postgresql-database-conventions) in which he says:
  
  "Show me your flowchart and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won’t usually need your flowchart; it’ll be obvious."
  
  I was switched on and I started thinking about how the graphVis tool could be used to describe a list of tables and views from a database.
  
  Say that two groups studied the same file TableXYZ with different inputs.  One of these groups wrote a seminal paper in the field, while their rivals wrote an inferior paper with a different result.  Imagine now a subsequent group who gathered the data from the previous work into the following database tables and conducted a replication study, with a new sensitivity analysis to explain why the original two papers produced different results.  

  Let's assume this database has all the data from all the groups in it and we want to get a pictorial view so we can disentangle which files belong to which study.  First get the following list of tables as INPUTS, grouping them by 'NAME' will give the tree structure and showing their results as OUTPUTS allows the subsequent replication study to use them as inputs and assume the position at the bottom of the flowchart.

  #### Code: database tables and different studies       
      filesList <- read.csv(textConnection(
      'NAME                 ,             INPUTS         , OUTPUTS
      The Seminal Study     ,              FileA         , 
      The Seminal Study     ,              FileB         , 
      The Seminal Study     ,       analysisFile         , 
      The Seminal Study     ,           TableXYZ         , 
      The Seminal Study     ,           TableABC         , 
      The Seminal Study     ,      Input1,Input2         ,
      The Seminal Study     ,             Input1         , 
      The Seminal Study     ,             Input2         , 
      The Seminal Study     ,      The Seminal Study     , analysisResults 
      The Inferior Rivals   ,                FileC       , 
      The Inferior Rivals   ,        analysisFileX       , 
      The Inferior Rivals   ,             TableXYZ       , 
      The Inferior Rivals   ,               InputX       , 
      The Inferior Rivals   ,    The Inferior Rivals     , analysisResultsX       
      The Replication Study ,    "Input1,Input2,TableXYZ",  analysisResultsR     
      The Replication Study ,    "Input1,InputX,TableXYZ",  sensitivityResult 
      '), stringsAsFactors = F, strip.white = T)

      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,"NAME"],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
        )
      }


      
  
  
  ## the result
  ![filesRelationships2.png](/images/filesRelationships2.png)  
#+end_src
*** man-newnode-workflow-code
#+name:man-newnode-workflow
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:man-newnode-workflow
  # NB this only works easily on linux
  require(devtools)
  install_github("disentangle", "ivanhanigan")
  require(disentangle)
  # the core of the tool is Rgraphviz, I just built a wrapper function
  # to add newnodes to a graph of nodes
  # always start with (newgraph = T) because the newnode function ADDS
  # nodes to a graph, unless told otherwise, and fails if no 'nodes'
  # object exists
  nodes  <- newnode(name="NAME",inputs="INPUT",outputs="OUTPUT", newgraph = T)
  dev.copy(png,"images/newnode1.png")
  dev.off()
  # now we can add nodes, and we can pass multiple inputs or outputs
  nodes  <- newnode(name="OUTPUT",inputs=c("NAME","ANOTHER THING"))
  dev.copy(png,"images/newnode2.png")
  dev.off()
  # outputs are optional
  
  #    so if there is a Balzerian filelist table available,
      # either make a spreadsheet with names, inputs and outputs 
      # fileslist <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
      # or 
      filesList <- read.csv(textConnection(
      'NAME,        INPUTS,           OUTPUTS,         DESCRIPTION
      FileA,        TableXYZ,         Input1,          Transformed variable
      FileB,        TableABC,         Input2,          Collapsed dimensions
      analysisFile, "Input1,Input2",  analysisResults, Merged inputs and analysed
      '), stringsAsFactors = F, strip.white = T)
      filesList
  
      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,1],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
                         )
      }
      dev.copy(png,'images/fileRelationships.png')
      dev.off();
  
  # but it was really something I designed to be used in a script like this
  #### step one ####
  nodes <- newnode(name="FileA", inputs="TableXYZ", outputs="Input1",
                   newgraph =T) # this is required to tell newnode to
                                # start a new graph, rather than add to
                                # the nodes
  FileA  <- read.table("TableXYZ.txt")
  Input1 <- log(FileA$columnZ)
  
  #### step two ####
  nodes <- newnode(name="FileB", inputs="TableABC", outputs="Input2")
  FileB  <- read.table("TableABC.txt")
  Input2 <- ddply(FileB, "id", summarise,
                  duration = max(year) - min(year),
                  nteams = length(unique(team)))
  
  #### step three ####
  nodes <- newnode(name="analysisFile", inputs=c("Input1","Input2"),
                   outputs="analysisResults")
  analysisFile  <- merge(Input1, Input2, by="id")
  analysisResults  <- lm(y ~ duration + nteams, data = analysisFile)
  
  # now generate a messy database full of tables
  require(reshape)
  require(sqldf)
  filesList$STUDY <- "The Seminal Study"
  filesList2  <- melt(filesList, id.vars = "STUDY")
  
  # now there was a second study, by rivals with only one dataset
  filesList_rivals <- read.csv(textConnection(
  'FILE,        INPUTS,           OUTPUTS,         DESCRIPTION
  FileC,        TableIJK,         InputX,          Transformed variable
  analysisFileX, InputX,  analysisResultsX,          analysed
  '), stringsAsFactors = F, strip.white = T)
  filesList_rivals$STUDY <- "The Inferior Rivals"
  filesList2  <- rbind(filesList2,
                       melt(filesList_rivals, id.vars = "STUDY")
                       )
  
  # and sometime later there is a third study that replicated the first and added a
  # sensitivity test
  filesList_replication <- read.csv(textConnection(
  'FILE,        INPUTS,           OUTPUTS,            DESCRIPTION
  analysisFileR, "Input1,Input2",  analysisResultsR, Merged inputs and analysed
  sensitivityAnalysisFile, InputX, sensitivityResult, SupportForSeminalStudy'), stringsAsFactors = F, strip.white = T)
  filesList_replication$STUDY <- "The Replication Study"
  filesList_replication
  filesList2  <- rbind(filesList2,
                       melt(filesList_replication, id.vars = "STUDY")
                       )
  filesList2
  filesList3  <- sqldf("SELECT DISTINCT STUDY, value
  FROM filesList2
  where variable != 'DESCRIPTION'")
  filesList3
  # somehow we've converted FILE to factor
  filesList3$FILE <- as.character(filesList3$FILE)
  
  filesList <- read.csv(textConnection(
  'NAME                 ,             INPUTS         , OUTPUTS
  The Seminal Study     ,              FileA         , 
  The Seminal Study     ,              FileB         , 
  The Seminal Study     ,       analysisFile         , 
  The Seminal Study     ,           TableXYZ         , 
  The Seminal Study     ,           TableABC         , 
  The Seminal Study     ,      Input1,Input2         ,
  The Seminal Study     ,             Input1         , 
  The Seminal Study     ,             Input2         , 
  The Seminal Study     ,      The Seminal Study     , analysisResults 
  The Inferior Rivals   ,                FileC       , 
  The Inferior Rivals   ,        analysisFileX       , 
  The Inferior Rivals   ,             TableXYZ       , 
  The Inferior Rivals   ,               InputX       , 
  The Inferior Rivals   ,    The Inferior Rivals     , analysisResultsX       
  The Replication Study ,   "Input1,Input2,TableXYZ" ,  analysisResultsR     
  The Replication Study ,   "Input1,InputX,TableXYZ" ,  sensitivityResult 
  '), stringsAsFactors = F, strip.white = T)
  
  for(i in 1:nrow(filesList))
  {
    nodes <- newnode(name = filesList[i,"NAME"],
                     inputs = strsplit(filesList$INPUTS, ",")[[i]],
                     outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                     newgraph = (i == 1)
    )
  }
  
  
  dev.copy(png, "images/filesRelationships2.png")
  dev.off()
  
#+end_src
