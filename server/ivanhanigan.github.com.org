#+TITLE:ivan hanigan github website 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* Open Notebook
** research
*** research header
#+name:research-header
#+begin_src markdown :tangle research.markdown :exports none :eval no :padline no
---
name: research
layout: default
title: Open Notebook
---


* Data Documentation: [Notes](/data-doco.html)
* [Farmer Suicide and Drought](http://www.pnas.org/content/early/2012/08/08/1112965109.full.pdf+html) 
* Incidence Rates, Standardisation and Adjustment
* [Open Notebook Science - Theory and Practice](/categories/ons)
* [OpenSoftware-RestrictedData](http://opensoftware-restricteddata.github.io)
* [pres](http://opensoftware-restricteddata.github.io/presentations/NCCARF-2013/presentation-hanigan-final.pdf)
* PhD thesis: Disentangling the Health Impacts of Environmental Change from Social Factors      
*  Pumilio-Bushfm: Bioacoustics Server: [Notes](/pumilio-bushfm-index.html),  [Report](http://ivanhanigan.github.io/pumilio-bushfm)
* [Scientific Workflow Software](/categories/workflow)
* [Spatio-temporal regression models](/categories/spatial dependence/)
* TransformSurveyTools: Notes, Report, [Tools](https://github.com/ivanhanigan/TransformSurveyTools)
* [Transformational-Adaptation-Energymark](/categories/energymark): transformations in Energy use
* Transformational-Adaptation-Farmers: transformational adaptation in a sample of farmers

#+end_src  

* COMMENT categories
** workflow
*** workflow header
#+name:workflow-header
#+begin_src markdown :tangle categories/workflow.md :exports none :eval no :padline no
---
name: workflow
layout: default
title: workflow
---

# [A workflow post](www.google.com)

    
#+end_src

* posts
** 2013-09-22-transformational-adaptation
#+name:transformational-adaptation-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-09-22-transformational-adaptation.md :exports none :eval no :padline no
---
name: transformational-adaptation
layout: post
title: transformational-adaptation
date: 2013-09-22
categories:
- Energymark
---

Energymark is about transformational adaptations as opposed to incremental adaptation..
    
#+end_src

** 2013-09-22-using-orgmode-and-jekyll-for-open-notebook
#+name:using-orgmode-and-jekyll-for-open-notebook-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-09-22-using-orgmode-and-jekyll-for-open-notebook.md :exports none :eval no :padline no
---
name: using-orgmode-and-jekyll-for-open-notebook
layout: post
title: using-orgmode-and-jekyll-for-open-notebook
date: 2013-09-22
categories:
- orgmode
---

# Using Orgmode and Jekyll for Open Notebook
Orgmode is a great notebook tool because it allows the coding, evaluation and documentation all in one.  I also want to use it to send the documentation to my blog as an Open Notebook.

If starting again I'd look into this:

- [http://orgmode.org/worg/org-tutorials/org-jekyll.html]( http://orgmode.org/worg/org-tutorials/org-jekyll.html)

But as it is I already put a lot of work into configuring a jekyll blog I cloned from Scott Chamberlain over at ROpenSci and I will just use orgmode to publish the posts related to each project, tagged as 'categories'.

But here is a problem I just found out how to solve.  For a long time I thought that because github disabled ruby plugins that the automatic generate categories index pages was broken.  Luckily Charlie Park has written up the following solution and this seems to have worked for me today:    

- [http://charliepark.org/tags-in-jekyll/](http://charliepark.org/tags-in-jekyll/)
- [http://charliepark.org/jekyll-with-plugins/](http://charliepark.org/jekyll-with-plugins/)

Cheers!

#+end_src

** software-ism
*** head
#+name:index
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no :padline no
--- 
name: software-ism
layout: post
title: software-ism
date: 2012-09-15
categories: 
- workflow
---
I am a huge fan of the R language for statistics and graphics.

I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).

In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.

I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.

For example a simple analysis in Rstudio using the 'R Markdown document' is below. 

You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#+end_src

*** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval yes
  ---
      summary(cars)
  --- 
  
  
  | speed | dist |
  |--------------|----------------
  | Min.   : 4.0 | Min.   :  2.00  
  | 1st Qu.:12.0 | 1st Qu.: 26.00  
  | Median :15.0 | Median : 36.00  
  | Mean   :15.4 | Mean   : 42.98  
  | 3rd Qu.:19.0 | 3rd Qu.: 56.00  
  | Max.   :25.0 | Max.   :120.00  
  ---  
#+end_src
*** and

#+name:and
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no
You can also embed plots, for example:
#+end_src
*** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval no
-----
    plot(cars)

-----
#+end_src
*** img
#+name:asdf
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports code :eval no
![plot of chunk unnamed-chunk-2](/images/unnamed-chunk-2.png)

I hope we can work toward a kind of 'tower of babel'.

#+end_src

** Bob Haining
*** head
#+name:index
#+begin_src markdown :tangle _posts/2012-09-15-software-ism.md :exports none :eval no :padline no
--- 
name: software-ism
layout: post
title: software-ism
date: 2012-09-15
categories: 
- software
---
I am a huge fan of the R language for statistics and graphics.

I sometimes hear people say they don't like R but then admit that they have never tried to use it, or if they have it was close to ten years ago (and a lot has changed).

In recent discussions at work I got the impression some people have got a bit predjudiced against R and other software that they don't actually use, primarily because of the added difficulty of software that requires a bit of programming.

I think that multi-disciplinary work will inevitably mean we find a mix of software in use, and they'll all have strengths and weaknesses.  A major strength of R is that one can weave together a report that includes the data, code, graphs and interpretations for an analysis, rather than copy-and-pasting these elements together as is required with other software toolboxes.

For example a simple analysis in Rstudio using the 'R Markdown document' is below. 

You can load and explore data in the document by placing 'Code Chunks' in the document, then when you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#+end_src

*** code
#+name:asdf
#+begin_src markdown :session *R* :tangle _posts/2012-09-15-software-ism.md :exports code :eval yes
  ---
      summary(cars)
  --- 
  
  
  | speed | dist |
  |--------------|----------------
  | Min.   : 4.0 | Min.   :  2.00  
  | 1st Qu.:12.0 | 1st Qu.: 26.00  
  | Median :15.0 | Median : 36.00  
  | Mean   :15.4 | Mean   : 42.98  
  | 3rd Qu.:19.0 | 3rd Qu.: 56.00  
  | Max.   :25.0 | Max.   :120.00  
  ---  
#+end_src
** pioz-et-al-2012-model-selection
#+begin_src markdown :tangle _posts/2013-04-18-pioz-et-al-2012-model-selection.md :exports none :eval no :padline no
---
name: pioz-et-al-2012-model-selection
layout: post
title: Pioz et al 2012 model selection
categories:
- Spatial Dependence
- Modelling
- Disentangle
---

In the [GIS forum SPDEP study group](http://gis-forum.github.io/study.html) we've been discussing the Bluetongue paper [http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did](http://www.mendeley.com/research/why-did-bluetongue-spread-the-way-it-did-environmental-factors-influencing-the-velocity-of-blueton)

I'd like to know more about the the Lagrange Multiplier tests and Francis
raised the [seminal Anselin 1988 paper for that](http://ivanhanigan.github.io/2013/04/reflections-bob-haining/#comment-864167749)


But in this post I just wanted to summarise their model selection procedure in a flow diagram


![pioz_modelling.png](/images/pioz_modelling.png)


#+end_src
*** COMMENT pioz_modelling-code
#+name:pioz_modelling
#+begin_src R :session *R* :tangle no :exports none :eval yes
    ################################################################
    # name:pioz_modelling
    require(disentangle)
    nodes <- newnode("data", "variable selection/transformation", newgraph = T)
    nodes  <- newnode("model building dataset (75%)",
                      inputs = "data"
                      )
    nodes  <- newnode("validation dataset (25%)", "data")
    nodes  <- newnode("OLS","model building dataset (75%)")
  #  nodes  <- newnode("diagnostics", "OLS")
    nodes  <- newnode("semi-variogram of the OLS residuals", "OLS", c("200km radius"))
    nodes <- newnode("inverse distance weighting", "assumption")
    nodes  <- newnode("spatial lag model", c("200km radius", "inverse distance weighting"))
    nodes  <- newnode("spatial error model",  c("200km radius", "inverse distance weighting", "robust Lagrange Multiplier"))
    nodes <- newnode("robust Lagrange Multiplier", c("spatial lag model", "spatial error model"))
    nodes <- newnode("three thematic sets of variables", "variable selection/transformation")
    nodes <- newnode("AIC to select variables", c("spatial error model", "three thematic sets of variables"), "minimal model")
  
    nodes <- newnode("compare dir, magnt and sig", c("OLS", "minimal model"))
    nodes <- newnode("coefficient of determination","literature several pseudo-R2 have")
    nodes <- newnode("assess fit", c("minimal model","RMSE", "coefficient of determination"))
    nodes <- newnode("assess fit with validation dataset", c("validation dataset (25%)", "RMSE", "coefficient of determination"))
    nodes <- newnode("assess each covariate",  "minimal model", "LR tests, loop drop-one-test-repeat")
    nodes <- newnode("compare the OLS and spatial error results for variables", c("OLS", "LR tests, loop drop-one-test-repeat"))
    dev.copy(png,"images/pioz_modelling.png", height = 1000, width = 700, res = 105)
    dev.off(); dev.off()
#+end_src

#+RESULTS: pioz_modelling
: 1

    
** animated-maps

#+name:animated-maps-header
#+begin_src markdown :tangle _posts/2013-07-30-animated-maps.md :exports none :eval no :padline no
--- 
name: animated-maps
layout: post
title: animated-maps 
date: 2013-07-30
categories: 
- spatial 
- animation
---

# Animated maps to allow exploration of alternate levels of 'jitter'
In a [previous project](http://www.ncbi.nlm.nih.gov/pubmed/22672028) we published a map of point locations that had been 'jittered', ie adding random noise to the latitude and longitude.  We did this by testing out a few maps and deciding on one that we thought protected privacy adequately whilst not destroying the spatial pattern we wished to display (evocatively).

I always wondered about a way to interactively do this and I think the animation package might do the trick, with the ability to step thru levels of jittering with the pause, fwd and back buttons.

[Clink here for the same data shown in a new animation](/jitter/index.html).

# Reference
Vally, H., Peel, M., Dowse, G. K., Cameron, S., Codde, J. P., Hanigan, I., & Lindsay, M. D. a. (2012). Geographic Information Systems used to describe the link between the risk of Ross River virus infection and proximity to the Leschenault estuary, WA. Australian and New Zealand Journal of Public Health, 36(3), 229–235. doi:10.1111/j.1753-6405.2012.00869.x
    
#+end_src

** Worflow flowcharts
see the Dropbox/projects/swish project
*** Worflow flowcharts header
#+name:Worflow flowcharts-header
#+begin_src markdown :tangle _posts/2013-07-31-worflow-flowcharts.md :exports none :eval no :padline no
  ---
  name: worflow-flowcharts
  layout: post
  title: Worflow flowcharts
  date: 2013-07-31
  categories: 
  - workflow
  - disentangle
  ---
  
  ## What is the issue  
  Most people seem to collect multiple datasets together in a single spot that can be split into 2 or more separate data packages.  I think this is a natural set up from an analysts perspective, where the results of multiple steps accumulate as 'stepping stones' toward the file they end up analysing.  
  
  I was first taught GIS by Isabelle Balzer at Ecowise Environmental Services in Canberra.  She showed me the method of keeping a table (sticky-taped to the desk!) of all the files and transformations that were going on. This was a method that didn't allow any multitasking!  I call this the 'Balzerian Method' (I am sure others used it before Isabelle, but I think Balzerian is a great word).

  I think the data wharehouse at my work is an example, and probably we'll find the key challenge for big data will be for analysts to disentangle their own filing systems.
  
  In my experience the way people store research data is often one (or a couple, or all) of these three types:

  - a database with heaps of tables and views
  - a directory (and sub-directories) with heaps of files 
  - a spreadsheet workbook with heaps of sheets (and links to other workbooks)
  
  I am developing a tool based on the open source graphviz softawre. The tool I am developing addresses the challenge of graphing the links between these sequential steps.  

  #### Code:introducing newnode
      # NB this only works easily on linux
      require(devtools)
      install_github("disentangle", "ivanhanigan")
      require(disentangle)
      # the core of the tool is Rgraphviz, I just built a wrapper function
      # to add newnodes to a graph of nodes
      # always start with (newgraph = T) because the newnode function ADDS
      # nodes to a graph, unless told otherwise, and fails if no 'nodes'
      # object exists
      nodes  <- newnode(name="NAME",inputs="INPUT",outputs="OUTPUT", newgraph = T)

  ![images/newnode1.png](/images/newnode1.png)

  #### Code:adding nodes
      # now we can add nodes, and we can pass multiple inputs or outputs
      nodes  <- newnode(name="OUTPUT",inputs=c("NAME","ANOTHER THING"))
      # outputs are optional

  ![images/newnode2.png](/images/newnode2.png)  

  It can be used in two or three ways.  

  ## Example one, the composite view:
  So if there is a Balzerian filelist table available, convert it to a spreadsheet.  This is als similar to a labbook from Chemistry but follows a very rigid structure: NAME,        INPUTS,           OUTPUTS,         DESCRIPTION.  The first method I'll show will take one of these tables and map out the steps in the workflow.
  
  #### Code: Composite Worflow Files List
      #    so if there is a Balzerian filelist table available,
      # either make a spreadsheet with names, inputs and outputs 
      # fileslist <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
      # or 
      filesList <- read.csv(textConnection(
      'NAME,        INPUTS,           OUTPUTS,         DESCRIPTION
      FileA,        TableXYZ,         Input1,          Transformed variable
      FileB,        TableABC,         Input2,          Collapsed dimensions
      analysisFile, "Input1,Input2",  analysisResults, Merged inputs and analysed
      '), stringsAsFactors = F, strip.white = T)
      filesList

      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,"NAME"],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
        )
      }
  
  ## shows this result
  ![fileRelationships.png](/images/fileRelationships.png)
  
  ## Example two, tracking the steps while analysing data:
  Structure a script into sections and document each section before evaluating the code to execute the step.  This works well with orgmode/ESS, Sweave or knitr style workflows.
  For example:
  
  #### Code: Ad Hoc Files Lists Flowcharts
      #### step one ####
      nodes <- newnode(name="FileA", inputs="TableXYZ", outputs="Input1",
                       newgraph =T) # this is required to tell newnode to
                                    # start a new graph, rather than add to
                                    # the nodes
      FileA  <- read.table("TableXYZ.txt")
      Input1 <- log(FileA$columnZ)
       
      #### step two ####
      nodes <- newnode(name="FileB", inputs="TableABC", outputs="Input2")
      FileB  <- read.table("TableABC.txt")
      Input2 <- ddply(FileB, "id", summarise,
                      duration = max(year) - min(year),
                      nteams = length(unique(team)))
       
      #### step three ####
      nodes <- newnode(name="analysisFile", inputs=c("Input1","Input2"),
                       outputs="analysisResults")
      analysisFile  <- merge(Input1, Input2, by="id")
      analysisResults  <- lm(y ~ duration + nteams, data = analysisFile)
  
  
  ## Example three: visualising relationships
  It is not aimed at visualising the linked structure of a tree or semi-lattice but can be used in such a way but changing the nodename and inputs concept to parent/child relationships.
  
  As an example I'll describe how a list of database tables might be displayed as a tree. I am a great fan of Josh Reich due to his [LCFD workflow](http://stackoverflow.com/a/1434424), and I also like his work on the [Simple Bank](https://www.simple.com/) so when I stumbled on this [blog post](http://blog.i2pi.com/post/52812976752/joshs-postgresql-database-conventions) in which he says:
  
  "Show me your flowchart and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won’t usually need your flowchart; it’ll be obvious."
  
  I was switched on and I started thinking about how the graphVis tool could be used to describe a list of tables and views from a database.
  
  Say that two groups studied the same file TableXYZ with different inputs.  One of these groups wrote a seminal paper in the field, while their rivals wrote an inferior paper with a different result.  Imagine now a subsequent group who gathered the data from the previous work into the following database tables and conducted a replication study, with a new sensitivity analysis to explain why the original two papers produced different results.  

  Let's assume this database has all the data from all the groups in it and we want to get a pictorial view so we can disentangle which files belong to which study.  First get the following list of tables as INPUTS, grouping them by 'NAME' will give the tree structure and showing their results as OUTPUTS allows the subsequent replication study to use them as inputs and assume the position at the bottom of the flowchart.

  #### Code: database tables and different studies       
      filesList <- read.csv(textConnection(
      'NAME                 ,             INPUTS         , OUTPUTS
      The Seminal Study     ,              FileA         , 
      The Seminal Study     ,              FileB         , 
      The Seminal Study     ,       analysisFile         , 
      The Seminal Study     ,           TableXYZ         , 
      The Seminal Study     ,           TableABC         , 
      The Seminal Study     ,      Input1,Input2         ,
      The Seminal Study     ,             Input1         , 
      The Seminal Study     ,             Input2         , 
      The Seminal Study     ,      The Seminal Study     , analysisResults 
      The Inferior Rivals   ,                FileC       , 
      The Inferior Rivals   ,        analysisFileX       , 
      The Inferior Rivals   ,             TableXYZ       , 
      The Inferior Rivals   ,               InputX       , 
      The Inferior Rivals   ,    The Inferior Rivals     , analysisResultsX       
      The Replication Study ,    "Input1,Input2,TableXYZ",  analysisResultsR     
      The Replication Study ,    "Input1,InputX,TableXYZ",  sensitivityResult 
      '), stringsAsFactors = F, strip.white = T)

      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,"NAME"],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
        )
      }


      
  
  
  ## the result
  ![filesRelationships2.png](/images/filesRelationships2.png)  
#+end_src
*** man-newnode-workflow-code
#+name:man-newnode-workflow
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:man-newnode-workflow
  # NB this only works easily on linux
  require(devtools)
  install_github("disentangle", "ivanhanigan")
  require(disentangle)
  # the core of the tool is Rgraphviz, I just built a wrapper function
  # to add newnodes to a graph of nodes
  # always start with (newgraph = T) because the newnode function ADDS
  # nodes to a graph, unless told otherwise, and fails if no 'nodes'
  # object exists
  nodes  <- newnode(name="NAME",inputs="INPUT",outputs="OUTPUT", newgraph = T)
  dev.copy(png,"images/newnode1.png")
  dev.off()
  # now we can add nodes, and we can pass multiple inputs or outputs
  nodes  <- newnode(name="OUTPUT",inputs=c("NAME","ANOTHER THING"))
  dev.copy(png,"images/newnode2.png")
  dev.off()
  # outputs are optional
  
  #    so if there is a Balzerian filelist table available,
      # either make a spreadsheet with names, inputs and outputs 
      # fileslist <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
      # or 
      filesList <- read.csv(textConnection(
      'NAME,        INPUTS,           OUTPUTS,         DESCRIPTION
      FileA,        TableXYZ,         Input1,          Transformed variable
      FileB,        TableABC,         Input2,          Collapsed dimensions
      analysisFile, "Input1,Input2",  analysisResults, Merged inputs and analysed
      '), stringsAsFactors = F, strip.white = T)
      filesList
  
      for(i in 1:nrow(filesList))
      {
        nodes <- newnode(name = filesList[i,1],
                         inputs = strsplit(filesList$INPUTS, ",")[[i]],
                         outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                         newgraph = (i == 1)
                         )
      }
      dev.copy(png,'images/fileRelationships.png')
      dev.off();
  
  # but it was really something I designed to be used in a script like this
  #### step one ####
  nodes <- newnode(name="FileA", inputs="TableXYZ", outputs="Input1",
                   newgraph =T) # this is required to tell newnode to
                                # start a new graph, rather than add to
                                # the nodes
  FileA  <- read.table("TableXYZ.txt")
  Input1 <- log(FileA$columnZ)
  
  #### step two ####
  nodes <- newnode(name="FileB", inputs="TableABC", outputs="Input2")
  FileB  <- read.table("TableABC.txt")
  Input2 <- ddply(FileB, "id", summarise,
                  duration = max(year) - min(year),
                  nteams = length(unique(team)))
  
  #### step three ####
  nodes <- newnode(name="analysisFile", inputs=c("Input1","Input2"),
                   outputs="analysisResults")
  analysisFile  <- merge(Input1, Input2, by="id")
  analysisResults  <- lm(y ~ duration + nteams, data = analysisFile)
  
  # now generate a messy database full of tables
  require(reshape)
  require(sqldf)
  filesList$STUDY <- "The Seminal Study"
  filesList2  <- melt(filesList, id.vars = "STUDY")
  
  # now there was a second study, by rivals with only one dataset
  filesList_rivals <- read.csv(textConnection(
  'FILE,        INPUTS,           OUTPUTS,         DESCRIPTION
  FileC,        TableIJK,         InputX,          Transformed variable
  analysisFileX, InputX,  analysisResultsX,          analysed
  '), stringsAsFactors = F, strip.white = T)
  filesList_rivals$STUDY <- "The Inferior Rivals"
  filesList2  <- rbind(filesList2,
                       melt(filesList_rivals, id.vars = "STUDY")
                       )
  
  # and sometime later there is a third study that replicated the first and added a
  # sensitivity test
  filesList_replication <- read.csv(textConnection(
  'FILE,        INPUTS,           OUTPUTS,            DESCRIPTION
  analysisFileR, "Input1,Input2",  analysisResultsR, Merged inputs and analysed
  sensitivityAnalysisFile, InputX, sensitivityResult, SupportForSeminalStudy'), stringsAsFactors = F, strip.white = T)
  filesList_replication$STUDY <- "The Replication Study"
  filesList_replication
  filesList2  <- rbind(filesList2,
                       melt(filesList_replication, id.vars = "STUDY")
                       )
  filesList2
  filesList3  <- sqldf("SELECT DISTINCT STUDY, value
  FROM filesList2
  where variable != 'DESCRIPTION'")
  filesList3
  # somehow we've converted FILE to factor
  filesList3$FILE <- as.character(filesList3$FILE)
  
  filesList <- read.csv(textConnection(
  'NAME                 ,             INPUTS         , OUTPUTS
  The Seminal Study     ,              FileA         , 
  The Seminal Study     ,              FileB         , 
  The Seminal Study     ,       analysisFile         , 
  The Seminal Study     ,           TableXYZ         , 
  The Seminal Study     ,           TableABC         , 
  The Seminal Study     ,      Input1,Input2         ,
  The Seminal Study     ,             Input1         , 
  The Seminal Study     ,             Input2         , 
  The Seminal Study     ,      The Seminal Study     , analysisResults 
  The Inferior Rivals   ,                FileC       , 
  The Inferior Rivals   ,        analysisFileX       , 
  The Inferior Rivals   ,             TableXYZ       , 
  The Inferior Rivals   ,               InputX       , 
  The Inferior Rivals   ,    The Inferior Rivals     , analysisResultsX       
  The Replication Study ,   "Input1,Input2,TableXYZ" ,  analysisResultsR     
  The Replication Study ,   "Input1,InputX,TableXYZ" ,  sensitivityResult 
  '), stringsAsFactors = F, strip.white = T)
  
  for(i in 1:nrow(filesList))
  {
    nodes <- newnode(name = filesList[i,"NAME"],
                     inputs = strsplit(filesList$INPUTS, ",")[[i]],
                     outputs = strsplit(filesList$OUTPUTS, ",")[[i]],
                     newgraph = (i == 1)
    )
  }
  
  
  dev.copy(png, "images/filesRelationships2.png")
  dev.off()
  
#+end_src
** Open Notebook System
** ons
*** COMMENT ons header
#+name:ons-header
#+begin_src markdown :tangle _posts/2013-09-13-ons.md :exports none :eval no :padline no
  ---
  name: ons
  layout: post
  title: Starting my Open Notebook Science Blog
  date: 2013-09-13
  categories: 
  - notebook
  - data science
  - reproducibility
  - replication
  ---
  
  Many examples are emerging of scientists who are transitioning to a
  much more open model of research.  This is in part externally driven
  by funding bodies (such as the Aussie Research Council asking for deposit of funded data and papers) and journals
  ([ie. Nature journals removing length restrictions on Methods sections.](http://www.nature.com/ng/journal/v45/n5/full/ng.2621.html)). Also the increased value being placed on transparency of reproducible analysis to safeguard against error and fraud is becoming an internal driver within science communities.
  
  [Open Notebook Science](http://en.wikipedia.org/wiki/Open_Notebook_Science)
  (ONS) style is an extreme of transparent approaches to research.
  According to the wikipedia page it is the "practice of making the
  entire primary record of a research project publicly available online
  as it is recorded".  
  
  That's pretty extreme!  In my view a lot of stuff in the research project should probably be archived quickly and left to rot.
  
  I like the range of options available.  I think I'll go for [SCD or "Seclected Content / Delayed"](http://onsclaims.wikispaces.com/) and show their image below.  In this model a portion of the open notebook and associated supporting raw data are available after some delay. I'll try to use this blog for weekly updates on progress for each project, and provide links off my 'Open Notebook' and 'Software' Tabs.
  
  ![ONS-SCD.png](/images/ONS-SCD.png)
      
#+end_src
* projects
** OpenSoftware-RestrictedData
** Bioacoustics and Pumilio
*** COMMENT pumilio-code
#+name:pumilio
#+begin_src sh :session *shell* :tangle no :exports none :eval yes
################################################################
# name:pumilio
cp ~/Dropbox/projects/JCU/pumilio/pumilio.html pumilio.html
#+end_src

#+RESULTS: pumilio

** Farmer Suicide and Drought
** Energymark: transformations in Energy use
** Farmer Transformations: transformational adaptation in a sample of farmers
** PhD thesis: Disentangling the Health Impacts of Environmental Change from Social Factors      
** Rates, Standardisation and Adjustment
** Spatio-temporal regression models
[[~/Dropbox/projects/spatiotemporal-regression-models/spatiotemporal.org
]]
** Workflow
[[~/Dropbox/projects/swish/swish-overview.org]]
* go
#+name:go
#+begin_src sh :session *shell2* :tangle no :exports none :eval yes
################################################################
# name:go
jekyll serve
#+end_src

  

